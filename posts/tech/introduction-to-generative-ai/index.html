<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Introduction to Generative AI | GopherDing&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="å…³äºCS5494â€”â€”Generative AIçš„è¯¾ç¨‹æè¿°">
<meta name="author" content="GopherDing">
<link rel="canonical" href="https://gopherding.github.io/posts/tech/introduction-to-generative-ai/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://gopherding.github.io/img/GopherSpaceCommunity.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://gopherding.github.io/img/GopherSpaceCommunity.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://gopherding.github.io/img/GopherSpaceCommunity.png">
<link rel="apple-touch-icon" href="https://gopherding.github.io/img/GopherSpaceCommunity.png">
<link rel="mask-icon" href="https://gopherding.github.io/img/GopherSpaceCommunity.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="google-site-verification" content="7XaJy5jt6ZoEXT-jlbI3sDg79M-BnIkGtU7WZLCQkzg" />
<link rel="alternate" hreflang="zh" href="https://gopherding.github.io/posts/tech/introduction-to-generative-ai/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<script src="/js/busuanzi.pure.mini.js"></script>
<link rel="stylesheet" href="/css/font-awesome.min.css">

<script src="/js/jquery-3.7.1.min.js"></script>


<script>// ç§»åŠ¨ç«¯å¯¼èˆªä¼˜åŒ–è„šæœ¬
(function() {
    'use strict';
    
    function optimizeNavigation() {
        const menu = document.getElementById('menu');
        if (!menu) return;
        
        const menuItems = menu.querySelectorAll('li a span');
        
        // ç§»é™¤å¯¼èˆªæ–‡å­—éšè—åŠŸèƒ½ï¼Œä¿æŒæ‰€æœ‰è®¾å¤‡éƒ½æ˜¾ç¤ºå®Œæ•´æ–‡æœ¬
        menuItems.forEach(item => {
            const originalText = item.textContent || item.innerText;
            
            if (!item.dataset.originalText) {
                item.dataset.originalText = originalText;
            }
            
            // å§‹ç»ˆæ˜¾ç¤ºå®Œæ•´æ–‡æœ¬ï¼Œä¸å†éšè—
            item.textContent = item.dataset.originalText || originalText;
            item.closest('a').removeAttribute('title');
        });
    }
    
    // çª—å£å¤§å°æ”¹å˜æ—¶é‡æ–°ä¼˜åŒ–
    let resizeTimer;
    window.addEventListener('resize', function() {
        clearTimeout(resizeTimer);
        resizeTimer = setTimeout(optimizeNavigation, 150);
    });
    
    // æ·»åŠ è§¦æ‘¸åé¦ˆ
    function addTouchFeedback() {
        const menuItems = document.querySelectorAll('#menu li a');
        
        menuItems.forEach(item => {
            // è§¦æ‘¸å¼€å§‹æ—¶æ·»åŠ æŒ‰ä¸‹æ•ˆæœ
            item.addEventListener('touchstart', function(e) {
                this.style.transform = 'scale(0.95)';
                this.style.transition = 'transform 0.1s ease';
            });
            
            // è§¦æ‘¸ç»“æŸæ—¶æ¢å¤
            item.addEventListener('touchend', function(e) {
                setTimeout(() => {
                    this.style.transform = '';
                    this.style.transition = '';
                }, 100);
            });
        });
    }
    
    // é¡µé¢åŠ è½½å®Œæˆåæ·»åŠ è§¦æ‘¸åé¦ˆ
    if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', function() {
            optimizeNavigation();
            addTouchFeedback();
        });
    } else {
        optimizeNavigation();
        addTouchFeedback();
    }
    
})();</script>


<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<meta property="og:title" content="Introduction to Generative AI" />
<meta property="og:description" content="å…³äºCS5494â€”â€”Generative AIçš„è¯¾ç¨‹æè¿°" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gopherding.github.io/posts/tech/introduction-to-generative-ai/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-12-15T15:55:23+08:00" />
<meta property="article:modified_time" content="2025-12-15T15:55:23+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to Generative AI"/>
<meta name="twitter:description" content="å…³äºCS5494â€”â€”Generative AIçš„è¯¾ç¨‹æè¿°"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  1 ,
          "name": "ğŸ“šæ–‡ç« ",
          "item": "https://gopherding.github.io/posts/"
        },

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "ğŸ‘¨ğŸ»â€ğŸ’» æŠ€æœ¯",
          "item": "https://gopherding.github.io/posts/tech/"
        }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Introduction to Generative AI",
      "item": "https://gopherding.github.io/posts/tech/introduction-to-generative-ai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Introduction to Generative AI",
  "name": "Introduction to Generative AI",
  "description": "å…³äºCS5494â€”â€”Generative AIçš„è¯¾ç¨‹æè¿°",
  "keywords": [
    ""
  ],
  "articleBody": "CS5494 Week 1: Introduction to Generative AI \u0026 Perception Models ç¬¬ä¸€å‘¨ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸æ„ŸçŸ¥æ¨¡å‹å¯¼è®º 1. æ ¸å¿ƒæ¦‚å¿µï¼šç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹ Core Concept: Generative vs. Discriminative Models è¿™æ˜¯æœ¬èŠ‚è¯¾æœ€é‡è¦çš„ç†è®ºåŸºç¡€ï¼Œç†è§£å®ƒä»¬çš„åŒºåˆ«æ˜¯å…¥é—¨çš„å…³é”®ã€‚\n1. åˆ¤åˆ«æ¨¡å‹ (Discriminative Models)\nåŠŸèƒ½ (Function): å­¦ä¹ å¦‚ä½•åŒºåˆ†æˆ–é¢„æµ‹ã€‚å°±åƒç»™ä½œä¸šæ‰“åˆ†çš„è€å¸ˆï¼Œçœ‹åˆ°ä¸€ä¸ªè¾“å…¥ï¼ˆè¯•å·ï¼‰ï¼Œç»™å‡ºä¸€ä¸ªè¾“å‡ºï¼ˆåˆ†æ•°ï¼‰ã€‚ Analogy: Like a teacher grading an exam. Given an input (exam paper), it produces an output (score). æ•°å­¦è¡¨è¾¾ (Mathematical Form): å»ºæ¨¡æ¡ä»¶æ¦‚ç‡ $P(y|x)$ã€‚å³ç»™å®šè¾“å…¥ $x$ï¼ˆå¦‚ä¸€å¼ ç…§ç‰‡ï¼‰ï¼Œé¢„æµ‹æ ‡ç­¾ $y$ï¼ˆå¦‚â€œçŒ«â€æˆ–â€œç‹—â€ï¼‰çš„æ¦‚ç‡ã€‚ Math: Models the conditional probability $P(y|x)$. Given input $x$, predict the probability of label $y$. å±€é™ (Limitation): å®ƒä»¬æ— æ³•åˆ›é€ æ–°æ•°æ®ï¼Œåªèƒ½å¯¹ç°æœ‰æ•°æ®è¿›è¡Œåˆ†ç±»æˆ–å›å½’ã€‚ 2. ç”Ÿæˆæ¨¡å‹ (Generative Models)\nåŠŸèƒ½ (Function): å­¦ä¹ æ•°æ®çš„åº•å±‚åˆ†å¸ƒ (Underlying Distribution)ï¼Œä»è€Œèƒ½å¤Ÿåˆ›é€ å‡ºä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼ä½†å…¨æ–°çš„æ•°æ®ã€‚å°±åƒä¸€ä¸ªç”»å®¶ï¼Œçœ‹è¿‡å¾ˆå¤šçŒ«ä¹‹åï¼Œåœ¨ä¸€å¼ ç™½çº¸ä¸Šç”»å‡ºä¸€åªä»æœªå­˜åœ¨è¿‡çš„çŒ«ã€‚ Analogy: Like an artist who, after seeing many cats, draws a new cat that never existed before on a blank piece of paper. æ•°å­¦è¡¨è¾¾ (Mathematical Form): å»ºæ¨¡è”åˆæ¦‚ç‡åˆ†å¸ƒ $P(x,y)$ æˆ–æ•°æ®æœ¬èº«çš„åˆ†å¸ƒ $P(x)$ã€‚ Math: Models the joint probability distribution $P(x,y)$ or the data distribution $P(x)$ itself. æ ¸å¿ƒç›®æ ‡ (Core Goal): èƒ½å¤Ÿä»å­¦ä¹ åˆ°çš„åˆ†å¸ƒä¸­é‡‡æ · (Sample)ï¼Œç”Ÿæˆé€¼çœŸçš„æ–°æ ·æœ¬ï¼ˆå›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ï¼‰ã€‚ 2. æ„ŸçŸ¥æ¨¡å‹ä¸è¡¨ç¤ºå­¦ä¹  Perception Models \u0026 Representation Learning è¯¾ä»¶ä¸­å¼ºè°ƒï¼Œä¸ºäº†åšå¥½ç”Ÿæˆï¼ˆGenerationï¼‰ï¼Œé¦–å…ˆè¦è§£å†³æ„ŸçŸ¥ï¼ˆPerceptionï¼‰çš„é—®é¢˜ã€‚æ„ŸçŸ¥æ¨¡å‹çš„æ ¸å¿ƒå°±æ˜¯è¡¨ç¤ºå­¦ä¹ ã€‚\nä»€ä¹ˆæ˜¯è¡¨ç¤ºå­¦ä¹ ï¼Ÿ (What is Representation Learning?)\nå®šä¹‰: å°†åŸå§‹æ•°æ®ï¼ˆRaw Dataï¼‰è½¬åŒ–ä¸ºæœºå™¨æ›´å®¹æ˜“ç†è§£å’Œå¤„ç†çš„å½¢å¼ï¼ˆFeature/Embeddingï¼‰ã€‚ Definition: Converting raw data into a form (features/embeddings) that is easier for machines to understand and process. è¿‡ç¨‹: ä»åŸå§‹åƒç´ ï¼ˆPixelsï¼‰$\\rightarrow$ æŠ½è±¡ç‰¹å¾ï¼ˆAbstractionï¼‰$\\rightarrow$ æ¦‚å¿µï¼ˆConceptsï¼‰ã€‚ Process: From raw pixels $\\rightarrow$ Abstract features $\\rightarrow$ High-level concepts. é‡è¦æ€§: å¥½çš„è¡¨ç¤ºï¼ˆRepresentationï¼‰èƒ½è®©æ¨¡å‹â€œç†è§£â€æ•°æ®çš„æœ¬è´¨ï¼Œè€Œä¸ä»…ä»…æ˜¯æ­»è®°ç¡¬èƒŒã€‚AlphaGo çš„æˆåŠŸå°±å½’åŠŸäºå®ƒèƒ½æ¯”äººç±»æ›´å¥½åœ°è¡¨ç¤ºæ£‹ç›˜å±€åŠ¿ã€‚ 3. æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¼”è¿› (Deep Neural Networks Evolution) From LeNet to Transformers è¿™éƒ¨åˆ†å›é¡¾äº†æ·±åº¦å­¦ä¹ è¿‡å»åå¹´çš„å…³é”®çªç ´ï¼Œæ­£æ˜¯è¿™äº›æŠ€æœ¯è®©ç°åœ¨çš„ GenAI æˆä¸ºå¯èƒ½ã€‚\nA. å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) çš„å´›èµ· LeNet (1989/1998): å¼•å…¥äº†å·ç§¯ (Convolution) å’Œ æ± åŒ– (Pooling) çš„æ¦‚å¿µã€‚åˆ©ç”¨æƒå€¼å…±äº«ï¼ˆWeight Sharingï¼‰å¤§å¤§å‡å°‘äº†å‚æ•°é‡ï¼Œé€‚åˆå¤„ç†å›¾åƒã€‚ AlexNet (2012): æ·±åº¦å­¦ä¹ çš„çˆ†å‘ç‚¹ã€‚å¼•å…¥äº† ReLU æ¿€æ´»å‡½æ•°ï¼ˆè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼‰å’Œ Dropoutï¼Œå¹¶ä½¿ç”¨äº† GPU åŠ é€Ÿè®­ç»ƒã€‚ VGG (2014): è¯æ˜äº†**â€œè¶Šæ·±è¶Šå¥½â€ (Deeper is better)**ã€‚å®ƒç”¨è¿ç»­çš„ 3x3 å°å·ç§¯æ ¸ä»£æ›¿äº†å¤§å·ç§¯æ ¸ï¼ŒåŠ æ·±äº†ç½‘ç»œç»“æ„ã€‚ B. çªç ´ç“¶é¢ˆï¼šåˆå§‹åŒ–ä¸å½’ä¸€åŒ– (Initialization \u0026 Normalization) éšç€ç½‘ç»œå˜æ·±ï¼Œè®­ç»ƒå˜å¾—æå…¶å›°éš¾ï¼ˆæ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±ï¼‰ã€‚\nåˆå§‹åŒ– (Initialization): Xavier å’Œ Kaiming Initialization æä¾›äº†ç§‘å­¦çš„å‚æ•°åˆå§‹å€¼è®¾å®šæ–¹æ³•ï¼Œè®©ä¿¡å·èƒ½æ›´ç¨³å®šåœ°åœ¨ç½‘ç»œä¸­ä¼ æ’­ã€‚ æ‰¹å½’ä¸€åŒ– (Batch Normalization, BN): å¼ºåˆ¶å°†æ¯ä¸€å±‚çš„è¾“å…¥æ‹‰å›åˆ°æ ‡å‡†çš„åˆ†å¸ƒã€‚è¿™è¢«è®¤ä¸ºæ˜¯è®­ç»ƒæ·±å±‚ç½‘ç»œçš„â€œç¥æŠ€â€ï¼Œå¤§å¤§åŠ é€Ÿäº†æ”¶æ•›ã€‚ C. æ®‹å·®ç½‘ç»œ (ResNet, 2015) é—®é¢˜: å½“ç½‘ç»œéå¸¸æ·±æ—¶ï¼ˆå¦‚ 100 å±‚ï¼‰ï¼Œç®€å•å †å å±‚æ•°åè€Œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼ˆDegradation problemï¼‰ã€‚ è§£å†³æ–¹æ¡ˆ: å¼•å…¥ Shortcut Connection (è·³è·ƒè¿æ¥)ï¼Œè®©æ•°æ®å¯ä»¥ç›´æ¥è·¨å±‚ä¼ é€’ã€‚ Mechanism: The network learns the residual (difference) $F(x)$ instead of the original mapping. Mathematically: $y = F(x) + x$. æ„ä¹‰: ä½¿å¾—è®­ç»ƒæˆç™¾ä¸Šåƒå±‚çš„ç½‘ç»œæˆä¸ºå¯èƒ½ï¼Œæ˜¯ç°ä»£å¤§æ¨¡å‹ï¼ˆåŒ…æ‹¬ GPT ç³»åˆ—ï¼‰çš„åŸºçŸ³ç»“æ„ã€‚ D. Transformer (2017) \u0026 ViT (2020) Transformer: æŠ›å¼ƒäº†å¾ªç¯ï¼ˆRNNï¼‰å’Œå·ç§¯ï¼ˆCNNï¼‰ï¼Œå®Œå…¨ä¾èµ– Attention Mechanism (æ³¨æ„åŠ›æœºåˆ¶)ã€‚æ¯ä¸€ä¸ª token éƒ½èƒ½çœ‹åˆ°æ‰€æœ‰å…¶ä»– tokenï¼Œæ‹¥æœ‰å…¨å±€ä¸Šä¸‹æ–‡ (Global Context)ã€‚ Vision Transformer (ViT): å°†å›¾åƒåˆ‡æˆå°å—ï¼ˆPatchesï¼‰ï¼Œåƒå¤„ç†æ–‡å­—ä¸€æ ·å¤„ç†å›¾åƒã€‚è¿™ç»Ÿä¸€äº†è§†è§‰å’Œè¯­è¨€çš„æ¨¡å‹æ¶æ„ã€‚ 4. ç”Ÿæˆæ¨¡å‹çš„æ¡†æ¶ Framework of Generative Models è¯¾ä»¶æœ€åæ€»ç»“äº†æ„å»ºä¸€ä¸ªç”Ÿæˆæ¨¡å‹çš„äº”ä¸ªå…³é”®è¦ç´ ï¼š\nå½¢å¼åŒ– (Formulation): å°†é—®é¢˜å®šä¹‰ä¸ºæ¦‚ç‡å»ºæ¨¡é—®é¢˜ï¼ˆå¦‚ä½•æè¿° $P(x)$ï¼Ÿï¼‰ã€‚ è¡¨ç¤º (Representation): ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå¦‚ ResNet, Transformerï¼‰æ¥æ‹Ÿåˆå¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚ ç›®æ ‡å‡½æ•° (Objective Function): å®šä¹‰â€œç”Ÿæˆå¾—å¥½ä¸å¥½â€çš„æ ‡å‡†ï¼ˆLoss Functionï¼‰ï¼Œè¡¡é‡é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„å·®å¼‚ã€‚ ä¼˜åŒ– (Optimization): è°ƒæ•´ç½‘ç»œå‚æ•°ä»¥æœ€å°åŒ–ç›®æ ‡å‡½æ•°ï¼ˆé€šå¸¸ä½¿ç”¨åå‘ä¼ æ’­ï¼‰ã€‚ æ¨æ–­ (Inference): è®­ç»ƒå¥½åï¼Œå¦‚ä½•é‡‡æ ·ï¼ˆSamplerï¼‰ç”Ÿæˆæ–°æ•°æ®ï¼Ÿ 5. ä¸“æœ‰åè¯è¡¨ (Glossary) ä¸­æ–‡æœ¯è¯­ English Term è¯¦ç»†è§£é‡Š / Detailed Explanation ç”Ÿæˆæ¨¡å‹ Generative Model å­¦ä¹ æ•°æ®åˆ†å¸ƒ $P(x)$ ä»¥ç”Ÿæˆæ–°æ ·æœ¬çš„æ¨¡å‹ã€‚å¦‚ GPT, Stable Diffusionã€‚ åˆ¤åˆ«æ¨¡å‹ Discriminative Model å­¦ä¹ æ¡ä»¶æ¦‚ç‡ $P(y|x)$ ä»¥åˆ†ç±»æˆ–é¢„æµ‹æ ‡ç­¾çš„æ¨¡å‹ã€‚å¦‚åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ã€‚ è¡¨ç¤ºå­¦ä¹  Representation Learning è‡ªåŠ¨ä»åŸå§‹æ•°æ®ä¸­æå–æœ‰æ•ˆç‰¹å¾çš„è¿‡ç¨‹ï¼Œå°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°ä½ç»´ã€æŠ½è±¡çš„ç‰¹å¾ç©ºé—´ã€‚ å·ç§¯ç¥ç»ç½‘ç»œ CNN (Convolutional Neural Network) ä¸“é—¨å¤„ç†ç½‘æ ¼æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰çš„ç¥ç»ç½‘ç»œï¼Œåˆ©ç”¨å·ç§¯å±‚æå–å±€éƒ¨ç‰¹å¾ã€‚ æ®‹å·®å­¦ä¹  Residual Learning ResNet çš„æ ¸å¿ƒã€‚é€šè¿‡å¼•å…¥â€œè·³è·ƒè¿æ¥â€ï¼Œè®©ç½‘ç»œå­¦ä¹ æ®‹å·®ï¼ˆå·®å¼‚ï¼‰è€Œä¸æ˜¯å®Œæ•´çš„æ˜ å°„ï¼Œè§£å†³äº†æ·±å±‚ç½‘ç»œçš„é€€åŒ–é—®é¢˜ã€‚ æ‰¹å½’ä¸€åŒ– Batch Normalization (BN) åœ¨æ¯ä¸€å±‚ç½‘ç»œçš„æ¿€æ´»å‰å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œé˜²æ­¢åˆ†å¸ƒåç§»ï¼ŒåŠ é€Ÿè®­ç»ƒã€‚ æ³¨æ„åŠ›æœºåˆ¶ Attention Mechanism Transformer çš„æ ¸å¿ƒã€‚å…è®¸æ¨¡å‹åœ¨å¤„ç†ä¸€ä¸ªå…ƒç´ æ—¶ï¼ŒåŠ¨æ€å…³æ³¨åºåˆ—ä¸­çš„å…¶ä»–ç›¸å…³å…ƒç´ ï¼ˆæ— è®ºè·ç¦»å¤šè¿œï¼‰ã€‚ åå‘ä¼ æ’­ Backpropagation è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ã€‚æ ¹æ®è¾“å‡ºè¯¯å·®ï¼Œåå‘è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç½‘ç»œå‚æ•°ã€‚ æ¦‚ç‡åˆ†å¸ƒ Probability Distribution æè¿°éšæœºå˜é‡å–å€¼å¯èƒ½æ€§çš„æ•°å­¦å‡½æ•°ã€‚ç”Ÿæˆæ¨¡å‹æœ¬è´¨ä¸Šå°±æ˜¯åœ¨æ‹Ÿåˆè¿™ä¸ªå¤æ‚çš„å‡½æ•°ã€‚ æ¨æ–­ Inference æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œåˆ©ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹æˆ–ç”Ÿæˆæ–°æ•°æ®çš„è¿‡ç¨‹ã€‚ æ€»ç»“ (Summary) è¿™ä»½è¯¾ä»¶çš„æ ¸å¿ƒé€»è¾‘æ˜¯ï¼šGenerative AI çš„æœ¬è´¨æ˜¯æ¦‚ç‡åˆ†å¸ƒçš„å»ºæ¨¡ï¼Œè€Œä¸ºäº†é€šè¿‡æœºå™¨å­¦ä¹ å¥½è¿™ä¸ªåˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦å¼ºå¤§çš„ Deep Learning æ¨¡å‹ï¼ˆå¦‚ ResNet, Transformerï¼‰ä½œä¸ºæ”¯æ’‘ã€‚ æ‰€ä»¥ç¬¬ä¸€å‘¨èŠ±äº†å¾ˆå¤šæ—¶é—´å¤ä¹ æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¶æ„ã€‚\nCS5494 Week 2: Basics of Probability Distributions ç¬¬äºŒå‘¨ï¼šæ¦‚ç‡åˆ†å¸ƒåŸºç¡€ 1. ç”Ÿæˆæ¨¡å‹ vs. åˆ¤åˆ«æ¨¡å‹ (è¿›é˜¶ç‰ˆ) Generative vs. Discriminative Models (Revisited) ä¸Šå‘¨è®²äº†æ¦‚å¿µï¼Œè¿™å‘¨ä»æ•°å­¦è§’åº¦æ·±å…¥å¯¹æ¯”ã€‚\nåˆ¤åˆ«æ¨¡å‹ (Discriminative Models):\nç›®æ ‡ (Goal): ç›´æ¥åŒºåˆ† $y$ï¼ˆæ ‡ç­¾ï¼‰ã€‚å»ºç«‹ $x$ åˆ° $y$ çš„æ˜ å°„ã€‚ æ•°å­¦ (Math): å»ºæ¨¡æ¡ä»¶æ¦‚ç‡ $P(y|x)$ã€‚ å†³ç­–è¾¹ç•Œ (Decision Boundary): å®ƒåªå…³å¿ƒæ€ä¹ˆæŠŠä¸¤ç±»æ•°æ®åˆ†å¼€ï¼Œä¸å…³å¿ƒæ•°æ®é•¿ä»€ä¹ˆæ ·ã€‚ Analogy: Like learning a rule to distinguish cats from dogs without knowing how to draw them. ç”Ÿæˆæ¨¡å‹ (Generative Models):\nç›®æ ‡ (Goal): æè¿°æ•°æ® $x$ æ˜¯å¦‚ä½•ç”Ÿæˆçš„ã€‚ æ•°å­¦ (Math): å»ºæ¨¡è”åˆæ¦‚ç‡ $P(x, y)$ æˆ–è¾¹ç¼˜æ¦‚ç‡ $P(x)$ã€‚ è´å¶æ–¯å…¬å¼ (Bayesâ€™ Rule): ç”Ÿæˆæ¨¡å‹å¯ä»¥é€šè¿‡è´å¶æ–¯å…¬å¼è½¬åŒ–ä¸ºåˆ¤åˆ«æ¨¡å‹ï¼š $$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$$ ä¼˜åŠ¿ (Advantage): è¯¾ä»¶ç‰¹åˆ«æåˆ°ï¼Œç”Ÿæˆæ¨¡å‹åœ¨ç¼ºå¤±æ•°æ® (Missing Data) çš„æƒ…å†µä¸‹ä¾ç„¶æœ‰æ•ˆã€‚å› ä¸ºå®ƒå¯ä»¥å¯¹æœªè§‚å¯Ÿåˆ°çš„å˜é‡è¿›è¡Œè¾¹ç¼˜åŒ– (Marginalize)ï¼Œè€Œåˆ¤åˆ«æ¨¡å‹å¿…é¡»ä¾èµ–å®Œæ•´çš„ $x$ã€‚ 2. æ ¸å¿ƒæŒ‘æˆ˜ï¼šç»´æ•°ç¾éš¾ The Core Challenge: Curse of Dimensionality è¿™æ˜¯æœ¬èŠ‚è¯¾æå‡ºçš„æœ€æ ¹æœ¬é—®é¢˜ï¼šä¸ºä»€ä¹ˆç”Ÿæˆæ¨¡å‹è¿™ä¹ˆéš¾åšï¼Ÿ\né—®é¢˜æè¿° (Problem): å‡è®¾æˆ‘ä»¬è¦ä¸ºä¸€ä¸ªç®€å•çš„ $28 \\times 28$ é»‘ç™½åƒç´ å›¾åƒï¼ˆå¦‚ MNIST æ•°å­—ï¼‰å»ºæ¨¡è”åˆåˆ†å¸ƒã€‚ æ¯ä¸ªåƒç´ æœ‰ 2 ç§çŠ¶æ€ï¼ˆ0 æˆ– 1ï¼‰ã€‚ æ€»å…±æœ‰ $784$ ä¸ªåƒç´ ã€‚ é‚£ä¹ˆè¿™å¹…å›¾å¯èƒ½çš„çŠ¶æ€æ€»æ•°æ˜¯ $2^{784}$ã€‚ Concept: The number of possible configurations grows exponentially with the number of variables (pixels). ç»“è®º (Conclusion): æˆ‘ä»¬ä¸å¯èƒ½åˆ—å‡ºä¸€å¼ è¡¨æ¥è®°å½•æ¯ä¸€ä¸ªå¯èƒ½å›¾åƒçš„æ¦‚ç‡ã€‚æ‰€éœ€çš„å‚æ•°é‡è¿œè¿œè¶…è¿‡äº†å®‡å®™ä¸­åŸå­çš„æ•°é‡ã€‚ è§£å†³æ–¹æ¡ˆ (Solution): æˆ‘ä»¬å¿…é¡»å¼•å…¥å‡è®¾ (Assumptions) å’Œ ç»“æ„ (Structure) æ¥å‡å°‘å‚æ•°é‡ã€‚ 3. ç»“æ„åŒ–æ¨¡å‹ï¼šè´å¶æ–¯ç½‘ç»œ Structured Models: Bayesian Networks ä¸ºäº†è§£å†³ç»´æ•°ç¾éš¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œæ¡ä»¶ç‹¬ç«‹æ€§â€å‡è®¾ã€‚\nA. é“¾å¼æ³•åˆ™ (The Chain Rule) ä»»ä½•å¤æ‚çš„è”åˆåˆ†å¸ƒéƒ½å¯ä»¥åˆ†è§£ä¸ºæ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ï¼š $$P(x_1, x_2, x_3, x_4) = P(x_1) P(x_2|x_1) P(x_3|x_1, x_2) P(x_4|x_1, x_2, x_3)$$\nè§£é‡Š: å°±åƒè®²æ•…äº‹ï¼Œåé¢çš„æƒ…èŠ‚å–å†³äºå‰é¢çš„é“ºå«ã€‚ é—®é¢˜: å³ä½¿åˆ†è§£äº†ï¼Œå‚æ•°é‡å¹¶æ²¡æœ‰å‡å°‘ã€‚æœ€åå‡ é¡¹ä¾ç„¶éå¸¸å¤æ‚ã€‚ B. æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ (Conditional Independence Assumption) è¿™æ˜¯è´å¶æ–¯ç½‘ç»œçš„æ ¸å¿ƒã€‚æˆ‘ä»¬å‡è®¾ï¼šæ¯ä¸ªå˜é‡åªä¾èµ–äºå®ƒçš„å°‘æ•°å‡ ä¸ªâ€œçˆ¶èŠ‚ç‚¹â€ï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„æ‰€æœ‰å˜é‡ã€‚\nKey Idea: Variable $x_i$ is independent of its non-descendants given its parents. C. è´å¶æ–¯ç½‘ç»œ (Bayesian Networks) å®šä¹‰: ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ (DAG)ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å˜é‡ï¼Œè¾¹ä»£è¡¨ä¾èµ–å…³ç³»ã€‚ å…¬å¼: $P(x_1, â€¦, x_n) = \\prod_{i=1}^{n} P(x_i | \\text{Parents}(x_i))$ æ•ˆæœ: æå¤§åœ°å‡å°‘äº†å‚æ•°é‡ã€‚ å…¨è¿æ¥ (Fully Connected): æ¯ä¸ªå˜é‡éƒ½ä¾èµ–æ‰€æœ‰å‰åºå˜é‡ $\\rightarrow$ å‚æ•°çˆ†ç‚¸ã€‚ ç¨€ç–è¿æ¥ (Sparse): æ¯ä¸ªå˜é‡åªä¾èµ– 1-2 ä¸ªçˆ¶èŠ‚ç‚¹ $\\rightarrow$ å‚æ•°å¯æ§ã€‚ D. ç»å…¸æ¡ˆä¾‹ï¼šæœ´ç´ è´å¶æ–¯ (Naive Bayes) è¿™æ˜¯è´å¶æ–¯ç½‘ç»œçš„ä¸€ä¸ªæç«¯ç‰¹ä¾‹ã€‚ å‡è®¾: ç»™å®šç±»åˆ« $y$ åï¼Œæ‰€æœ‰çš„ç‰¹å¾ $x_i$ éƒ½æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚ Assumption: All features are independent given the class label. å±€é™: è¿™ä¸ªå‡è®¾å¤ªå¼ºäº†ï¼ˆç°å®ä¸­åƒç´ ä¹‹é—´è‚¯å®šæœ‰å…³è”ï¼‰ï¼Œæ‰€ä»¥å®ƒç”Ÿæˆçš„å›¾ç‰‡é€šå¸¸å…¨æ˜¯å™ªç‚¹ï¼Œæ•ˆæœä¸å¥½ã€‚ä½†å®ƒä½œä¸ºåˆ†ç±»å™¨æ•ˆæœè¿˜ä¸é”™ã€‚ 4. ç¥ç»æ¨¡å‹ï¼šå¼•å…¥æ·±åº¦å­¦ä¹  Neural Models: Merging Probability with Deep Learning ä¼ ç»Ÿçš„å›¾æ¨¡å‹ï¼ˆå¦‚è´å¶æ–¯ç½‘ç»œï¼‰éœ€è¦ä¸“å®¶æ‰‹å·¥è®¾è®¡ä¾èµ–å…³ç³»å›¾ï¼Œè¿™å¾ˆéš¾ã€‚ç°åœ¨çš„è¶‹åŠ¿æ˜¯ç»“åˆç¥ç»ç½‘ç»œã€‚\nå‚æ•°åŒ– (Parameterization): åœ¨è´å¶æ–¯ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”¨è¡¨æ ¼æˆ–ç®€å•å‡½æ•°æ¥è¡¨ç¤º $P(x_i | \\text{Parents}(x_i))$ã€‚ åœ¨ç¥ç»æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆè¿™ä¸ªæ¡ä»¶æ¦‚ç‡å‡½æ•°ã€‚ Mechanism: Use a neural network to output the probability distribution parameters. éçº¿æ€§ä¾èµ– (Non-linear Dependence): çº¿æ€§æ¨¡å‹ï¼ˆå¦‚é€»è¾‘å›å½’ï¼‰å‡è®¾å˜é‡é—´æ˜¯çº¿æ€§å…³ç³»ã€‚è€Œç¥ç»ç½‘ç»œé€šè¿‡æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ Sigmoid, ReLUï¼‰å¼•å…¥äº†éçº¿æ€§ï¼Œèƒ½å¤Ÿæ•æ‰æ›´å¤æ‚çš„æ•°æ®å…³ç³»ã€‚ $$y = \\sigma(Wx + b)$$ ä»å›¾åˆ°ç½‘ (From Graphs to Nets): æˆ‘ä»¬å¯ä»¥é‡å¤å †å ç¥ç»å±‚ï¼Œæ„å»ºæ·±å±‚ç½‘ç»œã€‚è¿™å®é™…ä¸Šæ˜¯åœ¨å­¦ä¹ æ›´å¤æ‚çš„ã€éšå«çš„ä¾èµ–ç»“æ„ï¼Œè€Œä¸éœ€è¦äººå·¥æ˜¾å¼åœ°ç”»å‡ºæ¯ä¸€æ¡è¾¹ã€‚ 5. ä¸“æœ‰åè¯è¡¨ (Glossary) ä¸­æ–‡æœ¯è¯­ English Term è¯¦ç»†è§£é‡Š / Detailed Explanation è”åˆåˆ†å¸ƒ Joint Distribution $P(x, y)$ æˆ– $P(x_1, â€¦, x_n)$ã€‚æè¿°æ‰€æœ‰å˜é‡åŒæ—¶å–ç‰¹å®šå€¼çš„æ¦‚ç‡ã€‚è¿™æ˜¯ç”Ÿæˆæ¨¡å‹çš„æ ¸å¿ƒã€‚ æ¡ä»¶æ¦‚ç‡ Conditional Probability $P(A|B)$ã€‚åœ¨äº‹ä»¶ B å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼Œäº‹ä»¶ A å‘ç”Ÿçš„æ¦‚ç‡ã€‚ è¾¹ç¼˜åŒ– Marginalization é€šè¿‡å¯¹æŸäº›å˜é‡æ±‚å’Œï¼ˆæˆ–ç§¯åˆ†ï¼‰ï¼Œä»è”åˆåˆ†å¸ƒä¸­å¾—åˆ°å­é›†å˜é‡åˆ†å¸ƒçš„è¿‡ç¨‹ã€‚å¸¸ç”¨äºå¤„ç†ç¼ºå¤±æ•°æ®ã€‚ ç»´æ•°ç¾éš¾ Curse of Dimensionality éšç€æ•°æ®ç»´åº¦ï¼ˆç‰¹å¾æ•°é‡ï¼‰å¢åŠ ï¼Œæ•°æ®ç©ºé—´å‘ˆæŒ‡æ•°çº§çˆ†ç‚¸ï¼Œå¯¼è‡´æ•°æ®å˜å¾—æå…¶ç¨€ç–ï¼Œéš¾ä»¥å»ºæ¨¡ã€‚ é“¾å¼æ³•åˆ™ Chain Rule (Probability) æ¦‚ç‡è®ºåŸºæœ¬å®šç†ï¼Œå…è®¸å°†è”åˆæ¦‚ç‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ã€‚ è´å¶æ–¯ç½‘ç»œ Bayesian Network ä¸€ç§æ¦‚ç‡å›¾æ¨¡å‹ï¼Œä½¿ç”¨æœ‰å‘æ— ç¯å›¾ (DAG) æ¥è¡¨ç¤ºå˜é‡é—´çš„æ¡ä»¶ä¾èµ–å…³ç³»ã€‚ æ¡ä»¶ç‹¬ç«‹ Conditional Independence å¦‚æœå·²çŸ¥å˜é‡ Zï¼Œå˜é‡ X å’Œ Y äº’ä¸å½±å“ï¼Œåˆ™ç§° X å’Œ Y å…³äº Z æ¡ä»¶ç‹¬ç«‹ã€‚è¿™æ˜¯ç®€åŒ–æ¨¡å‹çš„å…³é”®ã€‚ æœ´ç´ è´å¶æ–¯ Naive Bayes ä¸€ç§ç®€å•çš„ç”Ÿæˆæ¨¡å‹ï¼Œå‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚å¸¸ä½œä¸ºåŸºå‡†æ¨¡å‹ (Baseline)ã€‚ æœ‰å‘æ— ç¯å›¾ DAG (Directed Acyclic Graph) è´å¶æ–¯ç½‘ç»œçš„ç»“æ„åŸºç¡€ï¼Œå›¾ä¸­çš„è¾¹æ˜¯æœ‰æ–¹å‘çš„ï¼Œä¸”ä¸å­˜åœ¨é—­ç¯ã€‚ é€»è¾‘å‡½æ•° Logistic Function (Sigmoid) $\\sigma(z) = \\frac{1}{1+e^{-z}}$ã€‚å¸¸ç”¨äºç¥ç»ç½‘ç»œä¸­å°†è¾“å‡ºå‹ç¼©åˆ° (0,1) ä¹‹é—´ï¼Œè¡¨ç¤ºæ¦‚ç‡ã€‚ æ€»ç»“ (Summary) Week 2 å‘Šè¯‰ä½ ï¼šå› ä¸ºä¸–ç•Œå¤ªå¤æ‚ï¼ˆç»´æ•°ç¾éš¾ï¼‰ï¼Œæˆ‘ä»¬ä¸èƒ½è›®åŠ›è®°å½•æ‰€æœ‰å¯èƒ½æ€§ã€‚ æˆ‘ä»¬å¿…é¡»å·æ‡’â€”â€”è¦ä¹ˆå‡è®¾å˜é‡ä¹‹é—´æ²¡é‚£ä¹ˆå¤šå…³ç³»ï¼ˆè´å¶æ–¯ç½‘ç»œï¼‰ï¼Œè¦ä¹ˆç”¨ä¸€ä¸ªå¼ºå¤§çš„é»‘ç›’ï¼ˆç¥ç»ç½‘ç»œï¼‰å»æ‹Ÿåˆè¿™äº›å…³ç³»ã€‚ç°ä»£ GenAI æ­£æ˜¯é€‰æ‹©äº†åè€…ã€‚\nCS5494 Week 3: Autoregressive Models ç¬¬ä¸‰å‘¨ï¼šè‡ªå›å½’æ¨¡å‹ 1. æ ¸å¿ƒå®šä¹‰ï¼šä»€ä¹ˆæ˜¯è‡ªå›å½’æ¨¡å‹ï¼Ÿ Core Definition: What are Autoregressive Models? è‡ªå›å½’æ¨¡å‹æ˜¯ç”Ÿæˆå¼ AI ä¸­æœ€ä¸»æµçš„ä¸€æ´¾ï¼ˆGPT ä¸­çš„ â€œGâ€ å°±æ˜¯ Generativeï¼Œå®é™…ä¸Šæ˜¯ Autoregressive çš„ï¼‰ã€‚\nåŸºæœ¬æ€æƒ³ (Basic Idea): å°†ç”Ÿæˆé«˜ç»´æ•°æ®ï¼ˆå¦‚ä¸€å¼ å›¾æˆ–ä¸€æ®µè¯ï¼‰çš„ä»»åŠ¡ï¼Œæ‹†è§£ä¸ºåºåˆ—ç”Ÿæˆ (Sequential Generation) ä»»åŠ¡ã€‚å³ï¼šæ ¹æ®å‰é¢æ‰€æœ‰çš„å†…å®¹ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå†…å®¹ã€‚\nConcept: Decompose the task of generating high-dimensional data into a sequential generation task. Predict the next token based on all previous tokens. æ•°å­¦åŸºç¡€ï¼šé“¾å¼æ³•åˆ™ (Mathematical Foundation: Chain Rule) æˆ‘ä»¬åˆ©ç”¨æ¦‚ç‡é“¾å¼æ³•åˆ™ï¼Œå°†è”åˆåˆ†å¸ƒåˆ†è§£ä¸ºæ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ï¼Œä¸å¼•å…¥ä»»ä½•ç‹¬ç«‹æ€§å‡è®¾ï¼š $$P(x_1, â€¦, x_n) = \\prod_{i=1}^{n} P(x_i | x_1, â€¦, x_{i-1})$$\nExplanation: $x_i$ depends on all previous variables $x_{",
  "wordCount" : "19120",
  "inLanguage": "zh",
  "datePublished": "2025-12-15T15:55:23+08:00",
  "dateModified": "2025-12-15T15:55:23+08:00",
  "author":[{
    "@type": "Person",
    "name": "GopherDing"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://gopherding.github.io/posts/tech/introduction-to-generative-ai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "GopherDing's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://gopherding.github.io/img/GopherSpaceCommunity.png"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://gopherding.github.io/" accesskey="h" title="GopherDing&#39;s Blog (Alt + H)">
            <img src="https://gopherding.github.io/img/GopherSpaceCommunity.png" alt="logo" aria-label="logo"
                 height="50">GopherDing&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://gopherding.github.io/" title="ğŸ  ä¸»é¡µ">
                <span>ğŸ  ä¸»é¡µ</span>
                </a>
            </li>
            <li>
                <a href="https://gopherding.github.io/search" title="ğŸ” æœç´¢ (Alt &#43; /)" accesskey=/>
                <span>ğŸ” æœç´¢</span>
                </a>
            </li>
            <li>
                <a href="https://gopherding.github.io/tags" title="ğŸ§© æ ‡ç­¾">
                <span>ğŸ§© æ ‡ç­¾</span>
                </a>
            </li>
            <li>
                <a href="https://gopherding.github.io/archives/" title="â±ï¸ æ—¶é—´è½´">
                <span>â±ï¸ æ—¶é—´è½´</span>
                </a>
            </li>
            <li>
                <a href="https://gopherding.github.io/about" title="ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº">
                <span>ğŸ™‹ğŸ»â€â™‚ï¸ å…³äº</span>
                </a>
            </li>
            <li>
                <a href="https://gopherding.github.io/links" title="ğŸ¤ å‹é“¾">
                <span>ğŸ¤ å‹é“¾</span>
                </a>
            </li>
            <li>
                <a href="https://www.travellings.cn/go.html" title="ğŸš‡ å¼€å¾€">
                <span>ğŸš‡ å¼€å¾€</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://gopherding.github.io/">ğŸ  ä¸»é¡µ</a>&nbsp;Â»&nbsp;<a href="https://gopherding.github.io/posts/">ğŸ“šæ–‡ç« </a>&nbsp;Â»&nbsp;<a href="https://gopherding.github.io/posts/tech/">ğŸ‘¨ğŸ»â€ğŸ’» æŠ€æœ¯</a></div>
            <h1 class="post-title">
                Introduction to Generative AI
            </h1>
            <div class="post-description">
                å…³äºCS5494â€”â€”Generative AIçš„è¯¾ç¨‹æè¿°
            </div>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2025-12-15
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>19120å­—
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>39åˆ†é’Ÿ
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>GopherDing
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://gopherding.github.io/tags/generative-ai/" style="color: var(--secondary)!important;">Generative AI</a>
            </span>
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_7">
                        &nbsp;&nbsp;
                        <span class="fa fa-eye" ></span>
                        <span>
                            <span id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv"></span></span>
                            &nbsp;&nbsp;
                        </span>
                    </span>
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo//twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://gopherding.github.io/"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId:  null , 
                                region:  null , 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">ç›®å½•</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#cs5494-week-1-introduction-to-generative-ai--perception-models" aria-label="CS5494 Week 1: Introduction to Generative AI &amp; Perception Models">CS5494 Week 1: Introduction to Generative AI &amp; Perception Models</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%b8%80%e5%91%a8%e7%94%9f%e6%88%90%e5%bc%8f%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e4%b8%8e%e6%84%9f%e7%9f%a5%e6%a8%a1%e5%9e%8b%e5%af%bc%e8%ae%ba" aria-label="ç¬¬ä¸€å‘¨ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸æ„ŸçŸ¥æ¨¡å‹å¯¼è®º">ç¬¬ä¸€å‘¨ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸æ„ŸçŸ¥æ¨¡å‹å¯¼è®º</a><ul>
                        
                <li>
                    <a href="#1-%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8b%e4%b8%8e%e5%88%a4%e5%88%ab%e6%a8%a1%e5%9e%8b" aria-label="1. æ ¸å¿ƒæ¦‚å¿µï¼šç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹">1. æ ¸å¿ƒæ¦‚å¿µï¼šç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹</a><ul>
                        
                <li>
                    <a href="#core-concept-generative-vs-discriminative-models" aria-label="Core Concept: Generative vs. Discriminative Models">Core Concept: Generative vs. Discriminative Models</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e6%84%9f%e7%9f%a5%e6%a8%a1%e5%9e%8b%e4%b8%8e%e8%a1%a8%e7%a4%ba%e5%ad%a6%e4%b9%a0" aria-label="2. æ„ŸçŸ¥æ¨¡å‹ä¸è¡¨ç¤ºå­¦ä¹ ">2. æ„ŸçŸ¥æ¨¡å‹ä¸è¡¨ç¤ºå­¦ä¹ </a><ul>
                        
                <li>
                    <a href="#perception-models--representation-learning" aria-label="Perception Models &amp; Representation Learning">Perception Models &amp; Representation Learning</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e6%b7%b1%e5%ba%a6%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%bc%94%e8%bf%9b-deep-neural-networks-evolution" aria-label="3. æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¼”è¿› (Deep Neural Networks Evolution)">3. æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¼”è¿› (Deep Neural Networks Evolution)</a><ul>
                        
                <li>
                    <a href="#from-lenet-to-transformers" aria-label="From LeNet to Transformers">From LeNet to Transformers</a><ul>
                        
                <li>
                    <a href="#a-%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c-cnns-%e7%9a%84%e5%b4%9b%e8%b5%b7" aria-label="A. å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) çš„å´›èµ·">A. å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) çš„å´›èµ·</a></li>
                <li>
                    <a href="#b-%e7%aa%81%e7%a0%b4%e7%93%b6%e9%a2%88%e5%88%9d%e5%a7%8b%e5%8c%96%e4%b8%8e%e5%bd%92%e4%b8%80%e5%8c%96-initialization--normalization" aria-label="B. çªç ´ç“¶é¢ˆï¼šåˆå§‹åŒ–ä¸å½’ä¸€åŒ– (Initialization &amp; Normalization)">B. çªç ´ç“¶é¢ˆï¼šåˆå§‹åŒ–ä¸å½’ä¸€åŒ– (Initialization &amp; Normalization)</a></li>
                <li>
                    <a href="#c-%e6%ae%8b%e5%b7%ae%e7%bd%91%e7%bb%9c-resnet-2015" aria-label="C. æ®‹å·®ç½‘ç»œ (ResNet, 2015)">C. æ®‹å·®ç½‘ç»œ (ResNet, 2015)</a></li>
                <li>
                    <a href="#d-transformer-2017--vit-2020" aria-label="D. Transformer (2017) &amp; ViT (2020)">D. Transformer (2017) &amp; ViT (2020)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8b%e7%9a%84%e6%a1%86%e6%9e%b6" aria-label="4. ç”Ÿæˆæ¨¡å‹çš„æ¡†æ¶">4. ç”Ÿæˆæ¨¡å‹çš„æ¡†æ¶</a><ul>
                        
                <li>
                    <a href="#framework-of-generative-models" aria-label="Framework of Generative Models">Framework of Generative Models</a></li></ul>
                </li>
                <li>
                    <a href="#5-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary" aria-label="5. ä¸“æœ‰åè¯è¡¨ (Glossary)">5. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#cs5494-week-2-basics-of-probability-distributions" aria-label="CS5494 Week 2: Basics of Probability Distributions">CS5494 Week 2: Basics of Probability Distributions</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%ba%8c%e5%91%a8%e6%a6%82%e7%8e%87%e5%88%86%e5%b8%83%e5%9f%ba%e7%a1%80" aria-label="ç¬¬äºŒå‘¨ï¼šæ¦‚ç‡åˆ†å¸ƒåŸºç¡€">ç¬¬äºŒå‘¨ï¼šæ¦‚ç‡åˆ†å¸ƒåŸºç¡€</a><ul>
                        
                <li>
                    <a href="#1-%e7%94%9f%e6%88%90%e6%a8%a1%e5%9e%8b-vs-%e5%88%a4%e5%88%ab%e6%a8%a1%e5%9e%8b-%e8%bf%9b%e9%98%b6%e7%89%88" aria-label="1. ç”Ÿæˆæ¨¡å‹ vs. åˆ¤åˆ«æ¨¡å‹ (è¿›é˜¶ç‰ˆ)">1. ç”Ÿæˆæ¨¡å‹ vs. åˆ¤åˆ«æ¨¡å‹ (è¿›é˜¶ç‰ˆ)</a><ul>
                        
                <li>
                    <a href="#generative-vs-discriminative-models-revisited" aria-label="Generative vs. Discriminative Models (Revisited)">Generative vs. Discriminative Models (Revisited)</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e6%a0%b8%e5%bf%83%e6%8c%91%e6%88%98%e7%bb%b4%e6%95%b0%e7%81%be%e9%9a%be" aria-label="2. æ ¸å¿ƒæŒ‘æˆ˜ï¼šç»´æ•°ç¾éš¾">2. æ ¸å¿ƒæŒ‘æˆ˜ï¼šç»´æ•°ç¾éš¾</a><ul>
                        
                <li>
                    <a href="#the-core-challenge-curse-of-dimensionality" aria-label="The Core Challenge: Curse of Dimensionality">The Core Challenge: Curse of Dimensionality</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e7%bb%93%e6%9e%84%e5%8c%96%e6%a8%a1%e5%9e%8b%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c" aria-label="3. ç»“æ„åŒ–æ¨¡å‹ï¼šè´å¶æ–¯ç½‘ç»œ">3. ç»“æ„åŒ–æ¨¡å‹ï¼šè´å¶æ–¯ç½‘ç»œ</a><ul>
                        
                <li>
                    <a href="#structured-models-bayesian-networks" aria-label="Structured Models: Bayesian Networks">Structured Models: Bayesian Networks</a><ul>
                        
                <li>
                    <a href="#a-%e9%93%be%e5%bc%8f%e6%b3%95%e5%88%99-the-chain-rule" aria-label="A. é“¾å¼æ³•åˆ™ (The Chain Rule)">A. é“¾å¼æ³•åˆ™ (The Chain Rule)</a></li>
                <li>
                    <a href="#b-%e6%9d%a1%e4%bb%b6%e7%8b%ac%e7%ab%8b%e6%80%a7%e5%81%87%e8%ae%be-conditional-independence-assumption" aria-label="B. æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ (Conditional Independence Assumption)">B. æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ (Conditional Independence Assumption)</a></li>
                <li>
                    <a href="#c-%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bd%91%e7%bb%9c-bayesian-networks" aria-label="C. è´å¶æ–¯ç½‘ç»œ (Bayesian Networks)">C. è´å¶æ–¯ç½‘ç»œ (Bayesian Networks)</a></li>
                <li>
                    <a href="#d-%e7%bb%8f%e5%85%b8%e6%a1%88%e4%be%8b%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af-naive-bayes" aria-label="D. ç»å…¸æ¡ˆä¾‹ï¼šæœ´ç´ è´å¶æ–¯ (Naive Bayes)">D. ç»å…¸æ¡ˆä¾‹ï¼šæœ´ç´ è´å¶æ–¯ (Naive Bayes)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-%e7%a5%9e%e7%bb%8f%e6%a8%a1%e5%9e%8b%e5%bc%95%e5%85%a5%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0" aria-label="4. ç¥ç»æ¨¡å‹ï¼šå¼•å…¥æ·±åº¦å­¦ä¹ ">4. ç¥ç»æ¨¡å‹ï¼šå¼•å…¥æ·±åº¦å­¦ä¹ </a><ul>
                        
                <li>
                    <a href="#neural-models-merging-probability-with-deep-learning" aria-label="Neural Models: Merging Probability with Deep Learning">Neural Models: Merging Probability with Deep Learning</a></li></ul>
                </li>
                <li>
                    <a href="#5-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary-1" aria-label="5. ä¸“æœ‰åè¯è¡¨ (Glossary)">5. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary-1" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#cs5494-week-3-autoregressive-models" aria-label="CS5494 Week 3: Autoregressive Models">CS5494 Week 3: Autoregressive Models</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%b8%89%e5%91%a8%e8%87%aa%e5%9b%9e%e5%bd%92%e6%a8%a1%e5%9e%8b" aria-label="ç¬¬ä¸‰å‘¨ï¼šè‡ªå›å½’æ¨¡å‹">ç¬¬ä¸‰å‘¨ï¼šè‡ªå›å½’æ¨¡å‹</a><ul>
                        
                <li>
                    <a href="#1-%e6%a0%b8%e5%bf%83%e5%ae%9a%e4%b9%89%e4%bb%80%e4%b9%88%e6%98%af%e8%87%aa%e5%9b%9e%e5%bd%92%e6%a8%a1%e5%9e%8b" aria-label="1. æ ¸å¿ƒå®šä¹‰ï¼šä»€ä¹ˆæ˜¯è‡ªå›å½’æ¨¡å‹ï¼Ÿ">1. æ ¸å¿ƒå®šä¹‰ï¼šä»€ä¹ˆæ˜¯è‡ªå›å½’æ¨¡å‹ï¼Ÿ</a><ul>
                        
                <li>
                    <a href="#core-definition-what-are-autoregressive-models" aria-label="Core Definition: What are Autoregressive Models?">Core Definition: What are Autoregressive Models?</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e5%85%a8%e5%8f%af%e8%a7%81%e4%bf%a1%e5%bf%b5%e7%bd%91%e7%bb%9c-fvbn" aria-label="2. å…¨å¯è§ä¿¡å¿µç½‘ç»œ (FVBN)">2. å…¨å¯è§ä¿¡å¿µç½‘ç»œ (FVBN)</a><ul>
                        
                <li>
                    <a href="#fully-visible-belief-networks-fvbn" aria-label="Fully Visible Belief Networks (FVBN)">Fully Visible Belief Networks (FVBN)</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c-rnn" aria-label="3. å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)">3. å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)</a><ul>
                        
                <li>
                    <a href="#recurrent-neural-networks-rnn" aria-label="Recurrent Neural Networks (RNN)">Recurrent Neural Networks (RNN)</a></li></ul>
                </li>
                <li>
                    <a href="#4-%e9%9d%a9%e5%91%bd%e6%80%a7%e7%aa%81%e7%a0%b4attention-%e4%b8%8e-transformer" aria-label="4. é©å‘½æ€§çªç ´ï¼šAttention ä¸ Transformer">4. é©å‘½æ€§çªç ´ï¼šAttention ä¸ Transformer</a><ul>
                        
                <li>
                    <a href="#the-revolution-attention--transformer" aria-label="The Revolution: Attention &amp; Transformer">The Revolution: Attention &amp; Transformer</a><ul>
                        
                <li>
                    <a href="#a-%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6-attention-mechanism" aria-label="A. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)">A. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)</a></li>
                <li>
                    <a href="#b-transformer-%e6%9e%b6%e6%9e%84" aria-label="B. Transformer æ¶æ„">B. Transformer æ¶æ„</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#5-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary-2" aria-label="5. ä¸“æœ‰åè¯è¡¨ (Glossary)">5. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary-2" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#cs5494-week-4-large-language-models-decoder-only-architecture" aria-label="CS5494 Week 4: Large Language Models (Decoder-only Architecture)">CS5494 Week 4: Large Language Models (Decoder-only Architecture)</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%9b%9b%e5%91%a8%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e4%bb%85%e8%a7%a3%e7%a0%81%e5%99%a8%e6%9e%b6%e6%9e%84" aria-label="ç¬¬å››å‘¨ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆä»…è§£ç å™¨æ¶æ„ï¼‰">ç¬¬å››å‘¨ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆä»…è§£ç å™¨æ¶æ„ï¼‰</a><ul>
                        
                <li>
                    <a href="#1-%e5%ae%8f%e8%a7%82%e6%9e%b6%e6%9e%84decoder-only-%e6%a8%a1%e5%9e%8b" aria-label="1. å®è§‚æ¶æ„ï¼šDecoder-only æ¨¡å‹">1. å®è§‚æ¶æ„ï¼šDecoder-only æ¨¡å‹</a><ul>
                        
                <li>
                    <a href="#macro-architecture-decoder-only-models" aria-label="Macro Architecture: Decoder-only Models">Macro Architecture: Decoder-only Models</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e9%a2%84%e5%a4%84%e7%90%86%e7%ac%ac%e4%b8%80%e6%ad%a5%e5%88%86%e8%af%8d%e5%99%a8-tokenizer" aria-label="2. é¢„å¤„ç†ç¬¬ä¸€æ­¥ï¼šåˆ†è¯å™¨ (Tokenizer)">2. é¢„å¤„ç†ç¬¬ä¸€æ­¥ï¼šåˆ†è¯å™¨ (Tokenizer)</a><ul>
                        
                <li>
                    <a href="#preprocessing-step-1-tokenizer" aria-label="Preprocessing Step 1: Tokenizer">Preprocessing Step 1: Tokenizer</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e9%a2%84%e5%a4%84%e7%90%86%e7%ac%ac%e4%ba%8c%e6%ad%a5%e5%b5%8c%e5%85%a5%e5%b1%82-embedding" aria-label="3. é¢„å¤„ç†ç¬¬äºŒæ­¥ï¼šåµŒå…¥å±‚ (Embedding)">3. é¢„å¤„ç†ç¬¬äºŒæ­¥ï¼šåµŒå…¥å±‚ (Embedding)</a><ul>
                        
                <li>
                    <a href="#preprocessing-step-2-embedding" aria-label="Preprocessing Step 2: Embedding">Preprocessing Step 2: Embedding</a></li></ul>
                </li>
                <li>
                    <a href="#4-%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af%e4%bd%8d%e7%bd%ae%e7%bc%96%e7%a0%81-positional-embeddingencoding" aria-label="4. å…³é”®æŠ€æœ¯ï¼šä½ç½®ç¼–ç  (Positional Embedding/Encoding)">4. å…³é”®æŠ€æœ¯ï¼šä½ç½®ç¼–ç  (Positional Embedding/Encoding)</a><ul>
                        
                <li>
                    <a href="#key-technique-positional-embedding" aria-label="Key Technique: Positional Embedding">Key Technique: Positional Embedding</a></li></ul>
                </li>
                <li>
                    <a href="#5-%e6%a0%b8%e5%bf%83%e6%9c%ba%e5%88%b6%e5%9b%a0%e6%9e%9c%e6%b3%a8%e6%84%8f%e5%8a%9b-causal-attention" aria-label="5. æ ¸å¿ƒæœºåˆ¶ï¼šå› æœæ³¨æ„åŠ› (Causal Attention)">5. æ ¸å¿ƒæœºåˆ¶ï¼šå› æœæ³¨æ„åŠ› (Causal Attention)</a><ul>
                        
                <li>
                    <a href="#core-mechanism-causal-attention-masked-attention" aria-label="Core Mechanism: Causal Attention (Masked Attention)">Core Mechanism: Causal Attention (Masked Attention)</a></li></ul>
                </li>
                <li>
                    <a href="#6-%e8%ae%ad%e7%bb%83%e4%b8%8e%e6%8e%a8%e7%90%86-training-vs-inference" aria-label="6. è®­ç»ƒä¸æ¨ç† (Training vs. Inference)">6. è®­ç»ƒä¸æ¨ç† (Training vs. Inference)</a><ul>
                        
                <li>
                    <a href="#training-phase-vs-generation-phase" aria-label="Training Phase vs. Generation Phase">Training Phase vs. Generation Phase</a></li></ul>
                </li>
                <li>
                    <a href="#7-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary" aria-label="7. ä¸“æœ‰åè¯è¡¨ (Glossary)">7. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary-3" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#cs5494-week-5-generative-adversarial-networks-gan" aria-label="CS5494 Week 5: Generative Adversarial Networks (GAN)">CS5494 Week 5: Generative Adversarial Networks (GAN)</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%ba%94%e5%91%a8%e7%94%9f%e6%88%90%e5%af%b9%e6%8a%97%e7%bd%91%e7%bb%9c" aria-label="ç¬¬äº”å‘¨ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œ">ç¬¬äº”å‘¨ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</a><ul>
                        
                <li>
                    <a href="#1-%e6%a0%b8%e5%bf%83%e7%9b%b4%e8%a7%89%e5%af%b9%e6%8a%97%e5%8d%9a%e5%bc%88" aria-label="1. æ ¸å¿ƒç›´è§‰ï¼šå¯¹æŠ—åšå¼ˆ">1. æ ¸å¿ƒç›´è§‰ï¼šå¯¹æŠ—åšå¼ˆ</a><ul>
                        
                <li>
                    <a href="#core-intuition-the-adversarial-game" aria-label="Core Intuition: The Adversarial Game">Core Intuition: The Adversarial Game</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e6%95%b0%e5%ad%a6%e6%9e%b6%e6%9e%84%e6%9e%81%e5%b0%8f%e6%9e%81%e5%a4%a7%e5%8d%9a%e5%bc%88" aria-label="2. æ•°å­¦æ¶æ„ï¼šæå°æå¤§åšå¼ˆ">2. æ•°å­¦æ¶æ„ï¼šæå°æå¤§åšå¼ˆ</a><ul>
                        
                <li>
                    <a href="#mathematical-architecture-minimax-game" aria-label="Mathematical Architecture: Minimax Game">Mathematical Architecture: Minimax Game</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e7%90%86%e8%ae%ba%e7%97%9b%e7%82%b9%e4%b8%ba%e4%bb%80%e4%b9%88%e5%8e%9f%e5%a7%8b-gan-%e5%be%88%e9%9a%be%e8%ae%ad%e7%bb%83" aria-label="3. ç†è®ºç—›ç‚¹ï¼šä¸ºä»€ä¹ˆåŸå§‹ GAN å¾ˆéš¾è®­ç»ƒï¼Ÿ">3. ç†è®ºç—›ç‚¹ï¼šä¸ºä»€ä¹ˆåŸå§‹ GAN å¾ˆéš¾è®­ç»ƒï¼Ÿ</a><ul>
                        
                <li>
                    <a href="#theoretical-pain-point-why-is-vanilla-gan-hard-to-train" aria-label="Theoretical Pain Point: Why is Vanilla GAN Hard to Train?">Theoretical Pain Point: Why is Vanilla GAN Hard to Train?</a><ul>
                        
                <li>
                    <a href="#a-%e4%bc%a0%e7%bb%9f%e7%9a%84%e8%b7%9d%e7%a6%bb%e5%ba%a6%e9%87%8fkl-%e6%95%a3%e5%ba%a6%e4%b8%8e-js-%e6%95%a3%e5%ba%a6" aria-label="A. ä¼ ç»Ÿçš„è·ç¦»åº¦é‡ï¼šKL æ•£åº¦ä¸ JS æ•£åº¦">A. ä¼ ç»Ÿçš„è·ç¦»åº¦é‡ï¼šKL æ•£åº¦ä¸ JS æ•£åº¦</a></li>
                <li>
                    <a href="#b-%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e9%97%ae%e9%a2%98-vanishing-gradient-problem" aria-label="B. æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ (Vanishing Gradient Problem)">B. æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ (Vanishing Gradient Problem)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88wasserstein-gan-wgan" aria-label="4. è§£å†³æ–¹æ¡ˆï¼šWasserstein GAN (WGAN)">4. è§£å†³æ–¹æ¡ˆï¼šWasserstein GAN (WGAN)</a><ul>
                        
                <li>
                    <a href="#the-solution-wasserstein-gan" aria-label="The Solution: Wasserstein GAN">The Solution: Wasserstein GAN</a></li></ul>
                </li>
                <li>
                    <a href="#5-%e8%bf%9b%e9%98%b6%e5%ba%94%e7%94%a8cyclegan" aria-label="5. è¿›é˜¶åº”ç”¨ï¼šCycleGAN">5. è¿›é˜¶åº”ç”¨ï¼šCycleGAN</a><ul>
                        
                <li>
                    <a href="#advanced-application-cyclegan" aria-label="Advanced Application: CycleGAN">Advanced Application: CycleGAN</a></li></ul>
                </li>
                <li>
                    <a href="#6-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary" aria-label="6. ä¸“æœ‰åè¯è¡¨ (Glossary)">6. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary-4" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#cs5494-week-6-variational-autoencoder-vae" aria-label="CS5494 Week 6: Variational Autoencoder (VAE)">CS5494 Week 6: Variational Autoencoder (VAE)</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%85%ad%e5%91%a8%e5%8f%98%e5%88%86%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8" aria-label="ç¬¬å…­å‘¨ï¼šå˜åˆ†è‡ªç¼–ç å™¨">ç¬¬å…­å‘¨ï¼šå˜åˆ†è‡ªç¼–ç å™¨</a><ul>
                        
                <li>
                    <a href="#1-%e5%89%8d%e7%bd%ae%e6%a6%82%e5%bf%b5%e8%87%aa%e7%bc%96%e7%a0%81%e5%99%a8-autoencoder-ae" aria-label="1. å‰ç½®æ¦‚å¿µï¼šè‡ªç¼–ç å™¨ (Autoencoder, AE)">1. å‰ç½®æ¦‚å¿µï¼šè‡ªç¼–ç å™¨ (Autoencoder, AE)</a><ul>
                        
                <li>
                    <a href="#precursor-autoencoder" aria-label="Precursor: Autoencoder">Precursor: Autoencoder</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e6%a0%b8%e5%bf%83%e7%90%86%e8%ae%ba%e6%bd%9c%e5%8f%98%e9%87%8f%e6%a8%a1%e5%9e%8b%e4%b8%8e%e9%9a%be%e8%a7%a3%e6%80%a7" aria-label="2. æ ¸å¿ƒç†è®ºï¼šæ½œå˜é‡æ¨¡å‹ä¸éš¾è§£æ€§">2. æ ¸å¿ƒç†è®ºï¼šæ½œå˜é‡æ¨¡å‹ä¸éš¾è§£æ€§</a><ul>
                        
                <li>
                    <a href="#latent-variable-models--intractability" aria-label="Latent Variable Models &amp; Intractability">Latent Variable Models &amp; Intractability</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88%e5%8f%98%e5%88%86%e6%8e%a8%e6%96%ad%e4%b8%8e-elbo-%e9%87%8d%e7%82%b9%e9%9a%be%e7%82%b9" aria-label="3. è§£å†³æ–¹æ¡ˆï¼šå˜åˆ†æ¨æ–­ä¸ ELBO (é‡ç‚¹/éš¾ç‚¹)">3. è§£å†³æ–¹æ¡ˆï¼šå˜åˆ†æ¨æ–­ä¸ ELBO (é‡ç‚¹/éš¾ç‚¹)</a><ul>
                        
                <li>
                    <a href="#solution-variational-inference--elbo" aria-label="Solution: Variational Inference &amp; ELBO">Solution: Variational Inference &amp; ELBO</a><ul>
                        
                <li>
                    <a href="#a-%e5%8f%98%e5%88%86%e6%8e%a8%e6%96%ad-variational-inference" aria-label="A. å˜åˆ†æ¨æ–­ (Variational Inference)">A. å˜åˆ†æ¨æ–­ (Variational Inference)</a></li>
                <li>
                    <a href="#b-elbo-%e8%af%81%e6%8d%ae%e4%b8%8b%e7%95%8c" aria-label="B. ELBO (è¯æ®ä¸‹ç•Œ)">B. ELBO (è¯æ®ä¸‹ç•Œ)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-vae-%e7%9a%84%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84%e4%b8%8e%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0" aria-label="4. VAE çš„æ¨¡å‹æ¶æ„ä¸æŸå¤±å‡½æ•°">4. VAE çš„æ¨¡å‹æ¶æ„ä¸æŸå¤±å‡½æ•°</a><ul>
                        
                <li>
                    <a href="#vae-architecture--loss-function" aria-label="VAE Architecture &amp; Loss Function">VAE Architecture &amp; Loss Function</a></li></ul>
                </li>
                <li>
                    <a href="#5-%e5%ae%9e%e7%8e%b0%e7%bb%86%e8%8a%82%e9%87%8d%e5%8f%82%e6%95%b0%e5%8c%96%e6%8a%80%e5%b7%a7-reparameterization-trick" aria-label="5. å®ç°ç»†èŠ‚ï¼šé‡å‚æ•°åŒ–æŠ€å·§ (Reparameterization Trick)">5. å®ç°ç»†èŠ‚ï¼šé‡å‚æ•°åŒ–æŠ€å·§ (Reparameterization Trick)</a><ul>
                        
                <li>
                    <a href="#implementation-the-reparameterization-trick" aria-label="Implementation: The Reparameterization Trick">Implementation: The Reparameterization Trick</a></li></ul>
                </li>
                <li>
                    <a href="#6-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary-1" aria-label="6. ä¸“æœ‰åè¯è¡¨ (Glossary)">6. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary-5" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#cs5494-week-7-diffusion-models-ddpm" aria-label="CS5494 Week 7: Diffusion Models (DDPM)">CS5494 Week 7: Diffusion Models (DDPM)</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%b8%83%e5%91%a8%e6%89%a9%e6%95%a3%e6%a8%a1%e5%9e%8b-ddpm" aria-label="ç¬¬ä¸ƒå‘¨ï¼šæ‰©æ•£æ¨¡å‹ (DDPM)">ç¬¬ä¸ƒå‘¨ï¼šæ‰©æ•£æ¨¡å‹ (DDPM)</a><ul>
                        
                <li>
                    <a href="#1-%e6%a0%b8%e5%bf%83%e7%9b%b4%e8%a7%89%e6%af%81%e6%8e%89%e5%ae%83%e5%86%8d%e5%a4%8d%e5%8e%9f%e5%ae%83" aria-label="1. æ ¸å¿ƒç›´è§‰ï¼šæ¯æ‰å®ƒï¼Œå†å¤åŸå®ƒ">1. æ ¸å¿ƒç›´è§‰ï¼šæ¯æ‰å®ƒï¼Œå†å¤åŸå®ƒ</a><ul>
                        
                <li>
                    <a href="#core-intuition-destroy-and-restore" aria-label="Core Intuition: Destroy and Restore">Core Intuition: Destroy and Restore</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e5%89%8d%e5%90%91%e8%bf%87%e7%a8%8b%e5%8a%a0%e5%99%aa-the-forward-process" aria-label="2. å‰å‘è¿‡ç¨‹ï¼šåŠ å™ª (The Forward Process)">2. å‰å‘è¿‡ç¨‹ï¼šåŠ å™ª (The Forward Process)</a><ul>
                        
                <li>
                    <a href="#adding-noise-systematically" aria-label="Adding Noise systematically">Adding Noise systematically</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e5%8f%8d%e5%90%91%e8%bf%87%e7%a8%8b%e5%8e%bb%e5%99%aa-the-reverse-process" aria-label="3. åå‘è¿‡ç¨‹ï¼šå»å™ª (The Reverse Process)">3. åå‘è¿‡ç¨‹ï¼šå»å™ª (The Reverse Process)</a><ul>
                        
                <li>
                    <a href="#denoising-with-neural-networks" aria-label="Denoising with Neural Networks">Denoising with Neural Networks</a></li></ul>
                </li>
                <li>
                    <a href="#4-%e6%a0%b8%e5%bf%83%e9%9a%be%e7%82%b9%e8%ae%ad%e7%bb%83%e7%9b%ae%e6%a0%87%e6%98%af%e4%bb%80%e4%b9%88" aria-label="4. æ ¸å¿ƒéš¾ç‚¹ï¼šè®­ç»ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ">4. æ ¸å¿ƒéš¾ç‚¹ï¼šè®­ç»ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ</a><ul>
                        
                <li>
                    <a href="#the-hard-part-what-is-the-loss-function" aria-label="The Hard Part: What is the Loss Function?">The Hard Part: What is the Loss Function?</a><ul>
                        
                <li>
                    <a href="#a-%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e8%83%bd%e7%9b%b4%e6%8e%a5%e7%94%b1-x_t-%e6%8e%a8-x_t-1" aria-label="A. ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”± $x_t$ æ¨ $x_{t-1}$ï¼Ÿ">A. ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”± $x_t$ æ¨ $x_{t-1}$ï¼Ÿ</a></li>
                <li>
                    <a href="#b-%e7%ae%80%e5%8c%96%e7%9a%84%e8%ae%ad%e7%bb%83%e7%9b%ae%e6%a0%87-simplified-loss-%e6%9c%80%e9%87%8d%e8%a6%81%e7%bb%93%e8%ae%ba" aria-label="B. ç®€åŒ–çš„è®­ç»ƒç›®æ ‡ (Simplified Loss) [â­æœ€é‡è¦ç»“è®º]">B. ç®€åŒ–çš„è®­ç»ƒç›®æ ‡ (Simplified Loss) [â­æœ€é‡è¦ç»“è®º]</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#5-%e6%8e%a8%e7%90%86%e7%ae%97%e6%b3%95%e6%80%8e%e4%b9%88%e7%94%9f%e6%88%90%e5%9b%be%e7%89%87" aria-label="5. æ¨ç†ç®—æ³•ï¼šæ€ä¹ˆç”Ÿæˆå›¾ç‰‡ï¼Ÿ">5. æ¨ç†ç®—æ³•ï¼šæ€ä¹ˆç”Ÿæˆå›¾ç‰‡ï¼Ÿ</a><ul>
                        
                <li>
                    <a href="#inference-sampling-algorithm" aria-label="Inference: Sampling Algorithm">Inference: Sampling Algorithm</a></li></ul>
                </li>
                <li>
                    <a href="#6-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary-2" aria-label="6. ä¸“æœ‰åè¯è¡¨ (Glossary)">6. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary-6" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#cs5494-week-8-ai-agent" aria-label="CS5494 Week 8: AI Agent">CS5494 Week 8: AI Agent</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%85%ab%e5%91%a8%e4%ba%ba%e5%b7%a5%e6%99%ba%e8%83%bd%e6%99%ba%e8%83%bd%e4%bd%93" aria-label="ç¬¬å…«å‘¨ï¼šäººå·¥æ™ºèƒ½æ™ºèƒ½ä½“">ç¬¬å…«å‘¨ï¼šäººå·¥æ™ºèƒ½æ™ºèƒ½ä½“</a><ul>
                        
                <li>
                    <a href="#1-%e6%a0%b8%e5%bf%83%e5%ae%9a%e4%b9%89%e4%bb%80%e4%b9%88%e6%98%af-ai-agent" aria-label="1. æ ¸å¿ƒå®šä¹‰ï¼šä»€ä¹ˆæ˜¯ AI Agentï¼Ÿ">1. æ ¸å¿ƒå®šä¹‰ï¼šä»€ä¹ˆæ˜¯ AI Agentï¼Ÿ</a><ul>
                        
                <li>
                    <a href="#core-definition-what-is-an-ai-agent" aria-label="Core Definition: What is an AI Agent?">Core Definition: What is an AI Agent?</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e6%a0%b8%e5%bf%83%e8%83%bd%e5%8a%9b%e4%b8%80%e8%a7%84%e5%88%92%e4%b8%8e%e6%80%9d%e8%80%83-planning--reasoning" aria-label="2. æ ¸å¿ƒèƒ½åŠ›ä¸€ï¼šè§„åˆ’ä¸æ€è€ƒ (Planning &amp; Reasoning)">2. æ ¸å¿ƒèƒ½åŠ›ä¸€ï¼šè§„åˆ’ä¸æ€è€ƒ (Planning &amp; Reasoning)</a><ul>
                        
                <li>
                    <a href="#core-capability-1-how-ai-thinks" aria-label="Core Capability 1: How AI Thinks?">Core Capability 1: How AI Thinks?</a><ul>
                        
                <li>
                    <a href="#a-%e8%a7%82%e5%af%9f-%e6%80%9d%e8%80%83-%e8%a1%8c%e5%8a%a8%e5%be%aa%e7%8e%af-obs-think-act-loop" aria-label="A. è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯ (Obs-Think-Act Loop)">A. è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯ (Obs-Think-Act Loop)</a></li>
                <li>
                    <a href="#b-react-%e6%a1%86%e6%9e%b6-reasoning--acting-%e9%87%8d%e7%82%b9" aria-label="B. ReAct æ¡†æ¶ (Reasoning &#43; Acting) [é‡ç‚¹]">B. ReAct æ¡†æ¶ (Reasoning + Acting) [é‡ç‚¹]</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#3-%e6%a0%b8%e5%bf%83%e8%83%bd%e5%8a%9b%e4%ba%8c%e5%b7%a5%e5%85%b7%e4%bd%bf%e7%94%a8-tool-use" aria-label="3. æ ¸å¿ƒèƒ½åŠ›äºŒï¼šå·¥å…·ä½¿ç”¨ (Tool Use)">3. æ ¸å¿ƒèƒ½åŠ›äºŒï¼šå·¥å…·ä½¿ç”¨ (Tool Use)</a><ul>
                        
                <li>
                    <a href="#core-capability-2-how-ai-uses-tools" aria-label="Core Capability 2: How AI Uses Tools?">Core Capability 2: How AI Uses Tools?</a></li></ul>
                </li>
                <li>
                    <a href="#4-%e6%a0%b8%e5%bf%83%e8%83%bd%e5%8a%9b%e4%b8%89%e8%ae%b0%e5%bf%86%e4%b8%8e%e7%bb%8f%e9%aa%8c-memory--experience" aria-label="4. æ ¸å¿ƒèƒ½åŠ›ä¸‰ï¼šè®°å¿†ä¸ç»éªŒ (Memory &amp; Experience)">4. æ ¸å¿ƒèƒ½åŠ›ä¸‰ï¼šè®°å¿†ä¸ç»éªŒ (Memory &amp; Experience)</a><ul>
                        
                <li>
                    <a href="#core-capability-3-adjusting-behavior-via-experience" aria-label="Core Capability 3: Adjusting Behavior via Experience">Core Capability 3: Adjusting Behavior via Experience</a></li></ul>
                </li>
                <li>
                    <a href="#5-%e8%bf%9b%e9%98%b6%e6%a6%82%e5%bf%b5agentic-ai-%e4%b8%8e%e5%a4%9a%e6%99%ba%e8%83%bd%e4%bd%93" aria-label="5. è¿›é˜¶æ¦‚å¿µï¼šAgentic AI ä¸å¤šæ™ºèƒ½ä½“">5. è¿›é˜¶æ¦‚å¿µï¼šAgentic AI ä¸å¤šæ™ºèƒ½ä½“</a><ul>
                        
                <li>
                    <a href="#advanced-concept-agentic-ai-systems" aria-label="Advanced Concept: Agentic AI Systems">Advanced Concept: Agentic AI Systems</a></li></ul>
                </li>
                <li>
                    <a href="#6-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary-3" aria-label="6. ä¸“æœ‰åè¯è¡¨ (Glossary)">6. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary-7" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#cs5494-week-9-3d-vision--generation" aria-label="CS5494 Week 9: 3D Vision &amp; Generation">CS5494 Week 9: 3D Vision &amp; Generation</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%b9%9d%e5%91%a8%e4%b8%89%e7%bb%b4%e8%a7%86%e8%a7%89%e4%b8%8e%e7%94%9f%e6%88%90" aria-label="ç¬¬ä¹å‘¨ï¼šä¸‰ç»´è§†è§‰ä¸ç”Ÿæˆ">ç¬¬ä¹å‘¨ï¼šä¸‰ç»´è§†è§‰ä¸ç”Ÿæˆ</a><ul>
                        
                <li>
                    <a href="#1-%e6%a0%b8%e5%bf%83%e7%97%9b%e7%82%b93d-%e8%a1%a8%e7%a4%ba%e7%9a%84%e5%a4%9a%e6%a0%b7%e6%80%a7" aria-label="1. æ ¸å¿ƒç—›ç‚¹ï¼š3D è¡¨ç¤ºçš„å¤šæ ·æ€§">1. æ ¸å¿ƒç—›ç‚¹ï¼š3D è¡¨ç¤ºçš„å¤šæ ·æ€§</a><ul>
                        
                <li>
                    <a href="#core-pain-point-diversity-of-3d-representations" aria-label="Core Pain Point: Diversity of 3D Representations">Core Pain Point: Diversity of 3D Representations</a><ul>
                        
                <li>
                    <a href="#a-%e6%98%be%e5%bc%8f%e8%a1%a8%e7%a4%ba-explicit-representations" aria-label="A. æ˜¾å¼è¡¨ç¤º (Explicit Representations)">A. æ˜¾å¼è¡¨ç¤º (Explicit Representations)</a></li>
                <li>
                    <a href="#b-%e9%9a%90%e5%bc%8f%e8%a1%a8%e7%a4%ba-implicit-representations-%e9%87%8d%e7%82%b9" aria-label="B. éšå¼è¡¨ç¤º (Implicit Representations) [é‡ç‚¹]">B. éšå¼è¡¨ç¤º (Implicit Representations) [é‡ç‚¹]</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#2-%e6%a0%b8%e5%bf%83%e6%9e%b6%e6%9e%84pointnet-%e5%a4%84%e7%90%86%e7%82%b9%e4%ba%91" aria-label="2. æ ¸å¿ƒæ¶æ„ï¼šPointNet (å¤„ç†ç‚¹äº‘)">2. æ ¸å¿ƒæ¶æ„ï¼šPointNet (å¤„ç†ç‚¹äº‘)</a><ul>
                        
                <li>
                    <a href="#core-architecture-pointnet-processing-point-clouds" aria-label="Core Architecture: PointNet (Processing Point Clouds)">Core Architecture: PointNet (Processing Point Clouds)</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e9%9d%a9%e5%91%bd%e6%80%a7%e6%8a%80%e6%9c%afnerf-%e7%a5%9e%e7%bb%8f%e8%be%90%e5%b0%84%e5%9c%ba" aria-label="3. é©å‘½æ€§æŠ€æœ¯ï¼šNeRF (ç¥ç»è¾å°„åœº)">3. é©å‘½æ€§æŠ€æœ¯ï¼šNeRF (ç¥ç»è¾å°„åœº)</a><ul>
                        
                <li>
                    <a href="#revolutionary-tech-nerf-neural-radiance-fields" aria-label="Revolutionary Tech: NeRF (Neural Radiance Fields)">Revolutionary Tech: NeRF (Neural Radiance Fields)</a></li></ul>
                </li>
                <li>
                    <a href="#4-3d-%e7%94%9f%e6%88%90dreamfusion-%e4%b8%8e-sds" aria-label="4. 3D ç”Ÿæˆï¼šDreamFusion ä¸ SDS">4. 3D ç”Ÿæˆï¼šDreamFusion ä¸ SDS</a><ul>
                        
                <li>
                    <a href="#3d-generation-dreamfusion--sds" aria-label="3D Generation: DreamFusion &amp; SDS">3D Generation: DreamFusion &amp; SDS</a></li></ul>
                </li>
                <li>
                    <a href="#5-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary-3" aria-label="5. ä¸“æœ‰åè¯è¡¨ (Glossary)">5. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary-8" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#cs5494-week-10-large-foundation-models-vlm--vla" aria-label="CS5494 Week 10: Large Foundation Models (VLM &amp; VLA)">CS5494 Week 10: Large Foundation Models (VLM &amp; VLA)</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%8d%81%e5%91%a8%e5%a4%9a%e6%a8%a1%e6%80%81%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b%e4%b8%8e%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd" aria-label="ç¬¬åå‘¨ï¼šå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸å…·èº«æ™ºèƒ½">ç¬¬åå‘¨ï¼šå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸å…·èº«æ™ºèƒ½</a><ul>
                        
                <li>
                    <a href="#1-%e8%8c%83%e5%bc%8f%e8%bd%ac%e5%8f%98%e4%bb%8e%e4%b8%93%e7%94%a8%e6%a8%a1%e5%9e%8b%e5%88%b0%e5%9f%ba%e7%a1%80%e6%a8%a1%e5%9e%8b" aria-label="1. èŒƒå¼è½¬å˜ï¼šä»ä¸“ç”¨æ¨¡å‹åˆ°åŸºç¡€æ¨¡å‹">1. èŒƒå¼è½¬å˜ï¼šä»ä¸“ç”¨æ¨¡å‹åˆ°åŸºç¡€æ¨¡å‹</a><ul>
                        
                <li>
                    <a href="#paradigm-shift-from-specialized-to-foundation-models" aria-label="Paradigm Shift: From Specialized to Foundation Models">Paradigm Shift: From Specialized to Foundation Models</a></li></ul>
                </li>
                <li>
                    <a href="#2-%e8%a7%86%e8%a7%89-%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b-vlm-clip" aria-label="2. è§†è§‰-è¯­è¨€æ¨¡å‹ (VLM): CLIP">2. è§†è§‰-è¯­è¨€æ¨¡å‹ (VLM): CLIP</a><ul>
                        
                <li>
                    <a href="#vision-language-models-vlm-clip" aria-label="Vision-Language Models (VLM): CLIP">Vision-Language Models (VLM): CLIP</a></li></ul>
                </li>
                <li>
                    <a href="#3-%e8%ae%a9-llm-%e7%9c%8b%e6%87%82%e5%9b%be-llava" aria-label="3. è®© LLM çœ‹æ‡‚å›¾: LLaVA">3. è®© LLM çœ‹æ‡‚å›¾: LLaVA</a><ul>
                        
                <li>
                    <a href="#making-llm-see-llava-large-language-and-vision-assistant" aria-label="Making LLM See: LLaVA (Large Language-and-Vision Assistant)">Making LLM See: LLaVA (Large Language-and-Vision Assistant)</a></li></ul>
                </li>
                <li>
                    <a href="#4-%e8%a7%86%e8%a7%89%e5%88%86%e5%89%b2%e5%9f%ba%e5%ba%a7-sam" aria-label="4. è§†è§‰åˆ†å‰²åŸºåº§: SAM">4. è§†è§‰åˆ†å‰²åŸºåº§: SAM</a><ul>
                        
                <li>
                    <a href="#vision-foundation-model-sam-segment-anything-model" aria-label="Vision Foundation Model: SAM (Segment Anything Model)">Vision Foundation Model: SAM (Segment Anything Model)</a></li></ul>
                </li>
                <li>
                    <a href="#5-%e5%85%b7%e8%ba%ab%e6%99%ba%e8%83%bd-vla-%e6%a8%a1%e5%9e%8b-robotic-foundation-models" aria-label="5. å…·èº«æ™ºèƒ½: VLA æ¨¡å‹ (Robotic Foundation Models)">5. å…·èº«æ™ºèƒ½: VLA æ¨¡å‹ (Robotic Foundation Models)</a><ul>
                        
                <li>
                    <a href="#embodied-ai-vision-language-action-models-vla" aria-label="Embodied AI: Vision-Language-Action Models (VLA)">Embodied AI: Vision-Language-Action Models (VLA)</a><ul>
                        
                <li>
                    <a href="#a-rt-1-robotic-transformer-1" aria-label="A. RT-1 (Robotic Transformer 1)">A. RT-1 (Robotic Transformer 1)</a></li>
                <li>
                    <a href="#b-rt-2-%e4%ba%92%e8%81%94%e7%bd%91%e7%9f%a5%e8%af%86%e8%bf%81%e7%a7%bb-internet-knowledge-transfer" aria-label="B. RT-2: äº’è”ç½‘çŸ¥è¯†è¿ç§» (Internet Knowledge Transfer)">B. RT-2: äº’è”ç½‘çŸ¥è¯†è¿ç§» (Internet Knowledge Transfer)</a></li>
                <li>
                    <a href="#c-rt-x--openvla" aria-label="C. RT-X / OpenVLA">C. RT-X / OpenVLA</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#6-%e4%b8%93%e6%9c%89%e5%90%8d%e8%af%8d%e8%a1%a8-glossary-4" aria-label="6. ä¸“æœ‰åè¯è¡¨ (Glossary)">6. ä¸“æœ‰åè¯è¡¨ (Glossary)</a><ul>
                        
                <li>
                    <a href="#%e6%80%bb%e7%bb%93-summary-9" aria-label="æ€»ç»“ (Summary)">æ€»ç»“ (Summary)</a>
                </li>
            </ul>
            </li>
            </ul>
            </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        if (elements) {
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement){
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><h1 id="cs5494-week-1-introduction-to-generative-ai--perception-models">CS5494 Week 1: Introduction to Generative AI &amp; Perception Models<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-1-introduction-to-generative-ai--perception-models">#</a></h1>
<h1 id="ç¬¬ä¸€å‘¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸æ„ŸçŸ¥æ¨¡å‹å¯¼è®º">ç¬¬ä¸€å‘¨ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸æ„ŸçŸ¥æ¨¡å‹å¯¼è®º<a hidden class="anchor" aria-hidden="true" href="#ç¬¬ä¸€å‘¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ä¸æ„ŸçŸ¥æ¨¡å‹å¯¼è®º">#</a></h1>
<h2 id="1-æ ¸å¿ƒæ¦‚å¿µç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹">1. æ ¸å¿ƒæ¦‚å¿µï¼šç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹<a hidden class="anchor" aria-hidden="true" href="#1-æ ¸å¿ƒæ¦‚å¿µç”Ÿæˆæ¨¡å‹ä¸åˆ¤åˆ«æ¨¡å‹">#</a></h2>
<h3 id="core-concept-generative-vs-discriminative-models">Core Concept: Generative vs. Discriminative Models<a hidden class="anchor" aria-hidden="true" href="#core-concept-generative-vs-discriminative-models">#</a></h3>
<p>è¿™æ˜¯æœ¬èŠ‚è¯¾æœ€é‡è¦çš„ç†è®ºåŸºç¡€ï¼Œç†è§£å®ƒä»¬çš„åŒºåˆ«æ˜¯å…¥é—¨çš„å…³é”®ã€‚</p>
<p><strong>1. åˆ¤åˆ«æ¨¡å‹ (Discriminative Models)</strong></p>
<ul>
<li><strong>åŠŸèƒ½ (Function)</strong>: å­¦ä¹ å¦‚ä½•åŒºåˆ†æˆ–é¢„æµ‹ã€‚å°±åƒç»™ä½œä¸šæ‰“åˆ†çš„è€å¸ˆï¼Œçœ‹åˆ°ä¸€ä¸ªè¾“å…¥ï¼ˆè¯•å·ï¼‰ï¼Œç»™å‡ºä¸€ä¸ªè¾“å‡ºï¼ˆåˆ†æ•°ï¼‰ã€‚
<ul>
<li><em>Analogy</em>: Like a teacher grading an exam. Given an input (exam paper), it produces an output (score).</li>
</ul>
</li>
<li><strong>æ•°å­¦è¡¨è¾¾ (Mathematical Form)</strong>: å»ºæ¨¡æ¡ä»¶æ¦‚ç‡ $P(y|x)$ã€‚å³ç»™å®šè¾“å…¥ $x$ï¼ˆå¦‚ä¸€å¼ ç…§ç‰‡ï¼‰ï¼Œé¢„æµ‹æ ‡ç­¾ $y$ï¼ˆå¦‚â€œçŒ«â€æˆ–â€œç‹—â€ï¼‰çš„æ¦‚ç‡ã€‚
<ul>
<li><em>Math</em>: Models the conditional probability $P(y|x)$. Given input $x$, predict the probability of label $y$.</li>
</ul>
</li>
<li><strong>å±€é™ (Limitation)</strong>: å®ƒä»¬æ— æ³•åˆ›é€ æ–°æ•°æ®ï¼Œåªèƒ½å¯¹ç°æœ‰æ•°æ®è¿›è¡Œåˆ†ç±»æˆ–å›å½’ã€‚</li>
</ul>
<p><strong>2. ç”Ÿæˆæ¨¡å‹ (Generative Models)</strong></p>
<ul>
<li><strong>åŠŸèƒ½ (Function)</strong>: å­¦ä¹ æ•°æ®çš„<strong>åº•å±‚åˆ†å¸ƒ (Underlying Distribution)</strong>ï¼Œä»è€Œèƒ½å¤Ÿåˆ›é€ å‡ºä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼ä½†å…¨æ–°çš„æ•°æ®ã€‚å°±åƒä¸€ä¸ªç”»å®¶ï¼Œçœ‹è¿‡å¾ˆå¤šçŒ«ä¹‹åï¼Œåœ¨ä¸€å¼ ç™½çº¸ä¸Šç”»å‡ºä¸€åªä»æœªå­˜åœ¨è¿‡çš„çŒ«ã€‚
<ul>
<li><em>Analogy</em>: Like an artist who, after seeing many cats, draws a new cat that never existed before on a blank piece of paper.</li>
</ul>
</li>
<li><strong>æ•°å­¦è¡¨è¾¾ (Mathematical Form)</strong>: å»ºæ¨¡è”åˆæ¦‚ç‡åˆ†å¸ƒ $P(x,y)$ æˆ–æ•°æ®æœ¬èº«çš„åˆ†å¸ƒ $P(x)$ã€‚
<ul>
<li><em>Math</em>: Models the joint probability distribution $P(x,y)$ or the data distribution $P(x)$ itself.</li>
</ul>
</li>
<li><strong>æ ¸å¿ƒç›®æ ‡ (Core Goal)</strong>: èƒ½å¤Ÿä»å­¦ä¹ åˆ°çš„åˆ†å¸ƒä¸­<strong>é‡‡æ · (Sample)</strong>ï¼Œç”Ÿæˆé€¼çœŸçš„æ–°æ ·æœ¬ï¼ˆå›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ï¼‰ã€‚</li>
</ul>
<hr>
<h2 id="2-æ„ŸçŸ¥æ¨¡å‹ä¸è¡¨ç¤ºå­¦ä¹ ">2. æ„ŸçŸ¥æ¨¡å‹ä¸è¡¨ç¤ºå­¦ä¹ <a hidden class="anchor" aria-hidden="true" href="#2-æ„ŸçŸ¥æ¨¡å‹ä¸è¡¨ç¤ºå­¦ä¹ ">#</a></h2>
<h3 id="perception-models--representation-learning">Perception Models &amp; Representation Learning<a hidden class="anchor" aria-hidden="true" href="#perception-models--representation-learning">#</a></h3>
<p>è¯¾ä»¶ä¸­å¼ºè°ƒï¼Œä¸ºäº†åšå¥½ç”Ÿæˆï¼ˆGenerationï¼‰ï¼Œé¦–å…ˆè¦è§£å†³æ„ŸçŸ¥ï¼ˆPerceptionï¼‰çš„é—®é¢˜ã€‚æ„ŸçŸ¥æ¨¡å‹çš„æ ¸å¿ƒå°±æ˜¯<strong>è¡¨ç¤ºå­¦ä¹ </strong>ã€‚</p>
<p><strong>ä»€ä¹ˆæ˜¯è¡¨ç¤ºå­¦ä¹ ï¼Ÿ (What is Representation Learning?)</strong></p>
<ul>
<li><strong>å®šä¹‰</strong>: å°†åŸå§‹æ•°æ®ï¼ˆRaw Dataï¼‰è½¬åŒ–ä¸ºæœºå™¨æ›´å®¹æ˜“ç†è§£å’Œå¤„ç†çš„å½¢å¼ï¼ˆFeature/Embeddingï¼‰ã€‚
<ul>
<li><em>Definition</em>: Converting raw data into a form (features/embeddings) that is easier for machines to understand and process.</li>
</ul>
</li>
<li><strong>è¿‡ç¨‹</strong>: ä»åŸå§‹åƒç´ ï¼ˆPixelsï¼‰$\rightarrow$ æŠ½è±¡ç‰¹å¾ï¼ˆAbstractionï¼‰$\rightarrow$ æ¦‚å¿µï¼ˆConceptsï¼‰ã€‚
<ul>
<li><em>Process</em>: From raw pixels $\rightarrow$ Abstract features $\rightarrow$ High-level concepts.</li>
</ul>
</li>
<li><strong>é‡è¦æ€§</strong>: å¥½çš„è¡¨ç¤ºï¼ˆRepresentationï¼‰èƒ½è®©æ¨¡å‹â€œç†è§£â€æ•°æ®çš„æœ¬è´¨ï¼Œè€Œä¸ä»…ä»…æ˜¯æ­»è®°ç¡¬èƒŒã€‚AlphaGo çš„æˆåŠŸå°±å½’åŠŸäºå®ƒèƒ½æ¯”äººç±»æ›´å¥½åœ°è¡¨ç¤ºæ£‹ç›˜å±€åŠ¿ã€‚</li>
</ul>
<hr>
<h2 id="3-æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¼”è¿›-deep-neural-networks-evolution">3. æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¼”è¿› (Deep Neural Networks Evolution)<a hidden class="anchor" aria-hidden="true" href="#3-æ·±åº¦ç¥ç»ç½‘ç»œçš„æ¼”è¿›-deep-neural-networks-evolution">#</a></h2>
<h3 id="from-lenet-to-transformers">From LeNet to Transformers<a hidden class="anchor" aria-hidden="true" href="#from-lenet-to-transformers">#</a></h3>
<p>è¿™éƒ¨åˆ†å›é¡¾äº†æ·±åº¦å­¦ä¹ è¿‡å»åå¹´çš„å…³é”®çªç ´ï¼Œæ­£æ˜¯è¿™äº›æŠ€æœ¯è®©ç°åœ¨çš„ GenAI æˆä¸ºå¯èƒ½ã€‚</p>
<h4 id="a-å·ç§¯ç¥ç»ç½‘ç»œ-cnns-çš„å´›èµ·">A. å·ç§¯ç¥ç»ç½‘ç»œ (CNNs) çš„å´›èµ·<a hidden class="anchor" aria-hidden="true" href="#a-å·ç§¯ç¥ç»ç½‘ç»œ-cnns-çš„å´›èµ·">#</a></h4>
<ul>
<li><strong>LeNet (1989/1998)</strong>: å¼•å…¥äº†<strong>å·ç§¯ (Convolution)</strong> å’Œ <strong>æ± åŒ– (Pooling)</strong> çš„æ¦‚å¿µã€‚åˆ©ç”¨æƒå€¼å…±äº«ï¼ˆWeight Sharingï¼‰å¤§å¤§å‡å°‘äº†å‚æ•°é‡ï¼Œé€‚åˆå¤„ç†å›¾åƒã€‚</li>
<li><strong>AlexNet (2012)</strong>: æ·±åº¦å­¦ä¹ çš„çˆ†å‘ç‚¹ã€‚å¼•å…¥äº† <strong>ReLU æ¿€æ´»å‡½æ•°</strong>ï¼ˆè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼‰å’Œ <strong>Dropout</strong>ï¼Œå¹¶ä½¿ç”¨äº† GPU åŠ é€Ÿè®­ç»ƒã€‚</li>
<li><strong>VGG (2014)</strong>: è¯æ˜äº†**â€œè¶Šæ·±è¶Šå¥½â€ (Deeper is better)**ã€‚å®ƒç”¨è¿ç»­çš„ 3x3 å°å·ç§¯æ ¸ä»£æ›¿äº†å¤§å·ç§¯æ ¸ï¼ŒåŠ æ·±äº†ç½‘ç»œç»“æ„ã€‚</li>
</ul>
<h4 id="b-çªç ´ç“¶é¢ˆåˆå§‹åŒ–ä¸å½’ä¸€åŒ–-initialization--normalization">B. çªç ´ç“¶é¢ˆï¼šåˆå§‹åŒ–ä¸å½’ä¸€åŒ– (Initialization &amp; Normalization)<a hidden class="anchor" aria-hidden="true" href="#b-çªç ´ç“¶é¢ˆåˆå§‹åŒ–ä¸å½’ä¸€åŒ–-initialization--normalization">#</a></h4>
<p>éšç€ç½‘ç»œå˜æ·±ï¼Œè®­ç»ƒå˜å¾—æå…¶å›°éš¾ï¼ˆæ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±ï¼‰ã€‚</p>
<ul>
<li><strong>åˆå§‹åŒ– (Initialization)</strong>: Xavier å’Œ Kaiming Initialization æä¾›äº†ç§‘å­¦çš„å‚æ•°åˆå§‹å€¼è®¾å®šæ–¹æ³•ï¼Œè®©ä¿¡å·èƒ½æ›´ç¨³å®šåœ°åœ¨ç½‘ç»œä¸­ä¼ æ’­ã€‚</li>
<li><strong>æ‰¹å½’ä¸€åŒ– (Batch Normalization, BN)</strong>: å¼ºåˆ¶å°†æ¯ä¸€å±‚çš„è¾“å…¥æ‹‰å›åˆ°æ ‡å‡†çš„åˆ†å¸ƒã€‚è¿™è¢«è®¤ä¸ºæ˜¯è®­ç»ƒæ·±å±‚ç½‘ç»œçš„â€œç¥æŠ€â€ï¼Œå¤§å¤§åŠ é€Ÿäº†æ”¶æ•›ã€‚</li>
</ul>
<h4 id="c-æ®‹å·®ç½‘ç»œ-resnet-2015">C. æ®‹å·®ç½‘ç»œ (ResNet, 2015)<a hidden class="anchor" aria-hidden="true" href="#c-æ®‹å·®ç½‘ç»œ-resnet-2015">#</a></h4>
<ul>
<li><strong>é—®é¢˜</strong>: å½“ç½‘ç»œéå¸¸æ·±æ—¶ï¼ˆå¦‚ 100 å±‚ï¼‰ï¼Œç®€å•å †å å±‚æ•°åè€Œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ï¼ˆDegradation problemï¼‰ã€‚</li>
<li><strong>è§£å†³æ–¹æ¡ˆ</strong>: å¼•å…¥ <strong>Shortcut Connection (è·³è·ƒè¿æ¥)</strong>ï¼Œè®©æ•°æ®å¯ä»¥ç›´æ¥è·¨å±‚ä¼ é€’ã€‚
<ul>
<li><em>Mechanism</em>: The network learns the residual (difference) $F(x)$ instead of the original mapping. Mathematically: $y = F(x) + x$.</li>
</ul>
</li>
<li><strong>æ„ä¹‰</strong>: ä½¿å¾—è®­ç»ƒæˆç™¾ä¸Šåƒå±‚çš„ç½‘ç»œæˆä¸ºå¯èƒ½ï¼Œæ˜¯ç°ä»£å¤§æ¨¡å‹ï¼ˆåŒ…æ‹¬ GPT ç³»åˆ—ï¼‰çš„åŸºçŸ³ç»“æ„ã€‚</li>
</ul>
<h4 id="d-transformer-2017--vit-2020">D. Transformer (2017) &amp; ViT (2020)<a hidden class="anchor" aria-hidden="true" href="#d-transformer-2017--vit-2020">#</a></h4>
<ul>
<li><strong>Transformer</strong>: æŠ›å¼ƒäº†å¾ªç¯ï¼ˆRNNï¼‰å’Œå·ç§¯ï¼ˆCNNï¼‰ï¼Œå®Œå…¨ä¾èµ– <strong>Attention Mechanism (æ³¨æ„åŠ›æœºåˆ¶)</strong>ã€‚æ¯ä¸€ä¸ª token éƒ½èƒ½çœ‹åˆ°æ‰€æœ‰å…¶ä»– tokenï¼Œæ‹¥æœ‰<strong>å…¨å±€ä¸Šä¸‹æ–‡ (Global Context)</strong>ã€‚</li>
<li><strong>Vision Transformer (ViT)</strong>: å°†å›¾åƒåˆ‡æˆå°å—ï¼ˆPatchesï¼‰ï¼Œåƒå¤„ç†æ–‡å­—ä¸€æ ·å¤„ç†å›¾åƒã€‚è¿™ç»Ÿä¸€äº†è§†è§‰å’Œè¯­è¨€çš„æ¨¡å‹æ¶æ„ã€‚</li>
</ul>
<hr>
<h2 id="4-ç”Ÿæˆæ¨¡å‹çš„æ¡†æ¶">4. ç”Ÿæˆæ¨¡å‹çš„æ¡†æ¶<a hidden class="anchor" aria-hidden="true" href="#4-ç”Ÿæˆæ¨¡å‹çš„æ¡†æ¶">#</a></h2>
<h3 id="framework-of-generative-models">Framework of Generative Models<a hidden class="anchor" aria-hidden="true" href="#framework-of-generative-models">#</a></h3>
<p>è¯¾ä»¶æœ€åæ€»ç»“äº†æ„å»ºä¸€ä¸ªç”Ÿæˆæ¨¡å‹çš„äº”ä¸ªå…³é”®è¦ç´ ï¼š</p>
<ol>
<li><strong>å½¢å¼åŒ– (Formulation)</strong>: å°†é—®é¢˜å®šä¹‰ä¸ºæ¦‚ç‡å»ºæ¨¡é—®é¢˜ï¼ˆå¦‚ä½•æè¿° $P(x)$ï¼Ÿï¼‰ã€‚</li>
<li><strong>è¡¨ç¤º (Representation)</strong>: ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆå¦‚ ResNet, Transformerï¼‰æ¥æ‹Ÿåˆå¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚</li>
<li><strong>ç›®æ ‡å‡½æ•° (Objective Function)</strong>: å®šä¹‰â€œç”Ÿæˆå¾—å¥½ä¸å¥½â€çš„æ ‡å‡†ï¼ˆLoss Functionï¼‰ï¼Œè¡¡é‡é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„å·®å¼‚ã€‚</li>
<li><strong>ä¼˜åŒ– (Optimization)</strong>: è°ƒæ•´ç½‘ç»œå‚æ•°ä»¥æœ€å°åŒ–ç›®æ ‡å‡½æ•°ï¼ˆé€šå¸¸ä½¿ç”¨åå‘ä¼ æ’­ï¼‰ã€‚</li>
<li><strong>æ¨æ–­ (Inference)</strong>: è®­ç»ƒå¥½åï¼Œå¦‚ä½•é‡‡æ ·ï¼ˆSamplerï¼‰ç”Ÿæˆæ–°æ•°æ®ï¼Ÿ</li>
</ol>
<hr>
<h2 id="5-ä¸“æœ‰åè¯è¡¨-glossary">5. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#5-ä¸“æœ‰åè¯è¡¨-glossary">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>ç”Ÿæˆæ¨¡å‹</strong></td>
          <td style="text-align: left">Generative Model</td>
          <td style="text-align: left">å­¦ä¹ æ•°æ®åˆ†å¸ƒ $P(x)$ ä»¥ç”Ÿæˆæ–°æ ·æœ¬çš„æ¨¡å‹ã€‚å¦‚ GPT, Stable Diffusionã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>åˆ¤åˆ«æ¨¡å‹</strong></td>
          <td style="text-align: left">Discriminative Model</td>
          <td style="text-align: left">å­¦ä¹ æ¡ä»¶æ¦‚ç‡ $P(y|x)$ ä»¥åˆ†ç±»æˆ–é¢„æµ‹æ ‡ç­¾çš„æ¨¡å‹ã€‚å¦‚åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è¡¨ç¤ºå­¦ä¹ </strong></td>
          <td style="text-align: left">Representation Learning</td>
          <td style="text-align: left">è‡ªåŠ¨ä»åŸå§‹æ•°æ®ä¸­æå–æœ‰æ•ˆç‰¹å¾çš„è¿‡ç¨‹ï¼Œå°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°ä½ç»´ã€æŠ½è±¡çš„ç‰¹å¾ç©ºé—´ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å·ç§¯ç¥ç»ç½‘ç»œ</strong></td>
          <td style="text-align: left">CNN (Convolutional Neural Network)</td>
          <td style="text-align: left">ä¸“é—¨å¤„ç†ç½‘æ ¼æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰çš„ç¥ç»ç½‘ç»œï¼Œåˆ©ç”¨å·ç§¯å±‚æå–å±€éƒ¨ç‰¹å¾ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ®‹å·®å­¦ä¹ </strong></td>
          <td style="text-align: left">Residual Learning</td>
          <td style="text-align: left">ResNet çš„æ ¸å¿ƒã€‚é€šè¿‡å¼•å…¥â€œè·³è·ƒè¿æ¥â€ï¼Œè®©ç½‘ç»œå­¦ä¹ æ®‹å·®ï¼ˆå·®å¼‚ï¼‰è€Œä¸æ˜¯å®Œæ•´çš„æ˜ å°„ï¼Œè§£å†³äº†æ·±å±‚ç½‘ç»œçš„é€€åŒ–é—®é¢˜ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ‰¹å½’ä¸€åŒ–</strong></td>
          <td style="text-align: left">Batch Normalization (BN)</td>
          <td style="text-align: left">åœ¨æ¯ä¸€å±‚ç½‘ç»œçš„æ¿€æ´»å‰å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œé˜²æ­¢åˆ†å¸ƒåç§»ï¼ŒåŠ é€Ÿè®­ç»ƒã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ³¨æ„åŠ›æœºåˆ¶</strong></td>
          <td style="text-align: left">Attention Mechanism</td>
          <td style="text-align: left">Transformer çš„æ ¸å¿ƒã€‚å…è®¸æ¨¡å‹åœ¨å¤„ç†ä¸€ä¸ªå…ƒç´ æ—¶ï¼ŒåŠ¨æ€å…³æ³¨åºåˆ—ä¸­çš„å…¶ä»–ç›¸å…³å…ƒç´ ï¼ˆæ— è®ºè·ç¦»å¤šè¿œï¼‰ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>åå‘ä¼ æ’­</strong></td>
          <td style="text-align: left">Backpropagation</td>
          <td style="text-align: left">è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ã€‚æ ¹æ®è¾“å‡ºè¯¯å·®ï¼Œåå‘è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç½‘ç»œå‚æ•°ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ¦‚ç‡åˆ†å¸ƒ</strong></td>
          <td style="text-align: left">Probability Distribution</td>
          <td style="text-align: left">æè¿°éšæœºå˜é‡å–å€¼å¯èƒ½æ€§çš„æ•°å­¦å‡½æ•°ã€‚ç”Ÿæˆæ¨¡å‹æœ¬è´¨ä¸Šå°±æ˜¯åœ¨æ‹Ÿåˆè¿™ä¸ªå¤æ‚çš„å‡½æ•°ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ¨æ–­</strong></td>
          <td style="text-align: left">Inference</td>
          <td style="text-align: left">æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œåˆ©ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹æˆ–ç”Ÿæˆæ–°æ•°æ®çš„è¿‡ç¨‹ã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary">#</a></h3>
<p>è¿™ä»½è¯¾ä»¶çš„æ ¸å¿ƒé€»è¾‘æ˜¯ï¼š<strong>Generative AI çš„æœ¬è´¨æ˜¯æ¦‚ç‡åˆ†å¸ƒçš„å»ºæ¨¡ï¼Œè€Œä¸ºäº†é€šè¿‡æœºå™¨å­¦ä¹ å¥½è¿™ä¸ªåˆ†å¸ƒï¼Œæˆ‘ä»¬éœ€è¦å¼ºå¤§çš„ Deep Learning æ¨¡å‹ï¼ˆå¦‚ ResNet, Transformerï¼‰ä½œä¸ºæ”¯æ’‘ã€‚</strong> æ‰€ä»¥ç¬¬ä¸€å‘¨èŠ±äº†å¾ˆå¤šæ—¶é—´å¤ä¹ æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¶æ„ã€‚</p>
<hr>
<h1 id="cs5494-week-2-basics-of-probability-distributions">CS5494 Week 2: Basics of Probability Distributions<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-2-basics-of-probability-distributions">#</a></h1>
<h1 id="ç¬¬äºŒå‘¨æ¦‚ç‡åˆ†å¸ƒåŸºç¡€">ç¬¬äºŒå‘¨ï¼šæ¦‚ç‡åˆ†å¸ƒåŸºç¡€<a hidden class="anchor" aria-hidden="true" href="#ç¬¬äºŒå‘¨æ¦‚ç‡åˆ†å¸ƒåŸºç¡€">#</a></h1>
<h2 id="1-ç”Ÿæˆæ¨¡å‹-vs-åˆ¤åˆ«æ¨¡å‹-è¿›é˜¶ç‰ˆ">1. ç”Ÿæˆæ¨¡å‹ vs. åˆ¤åˆ«æ¨¡å‹ (è¿›é˜¶ç‰ˆ)<a hidden class="anchor" aria-hidden="true" href="#1-ç”Ÿæˆæ¨¡å‹-vs-åˆ¤åˆ«æ¨¡å‹-è¿›é˜¶ç‰ˆ">#</a></h2>
<h3 id="generative-vs-discriminative-models-revisited">Generative vs. Discriminative Models (Revisited)<a hidden class="anchor" aria-hidden="true" href="#generative-vs-discriminative-models-revisited">#</a></h3>
<p>ä¸Šå‘¨è®²äº†æ¦‚å¿µï¼Œè¿™å‘¨ä»æ•°å­¦è§’åº¦æ·±å…¥å¯¹æ¯”ã€‚</p>
<ul>
<li>
<p><strong>åˆ¤åˆ«æ¨¡å‹ (Discriminative Models)</strong>:</p>
<ul>
<li><strong>ç›®æ ‡ (Goal)</strong>: ç›´æ¥åŒºåˆ† $y$ï¼ˆæ ‡ç­¾ï¼‰ã€‚å»ºç«‹ $x$ åˆ° $y$ çš„æ˜ å°„ã€‚</li>
<li><strong>æ•°å­¦ (Math)</strong>: å»ºæ¨¡æ¡ä»¶æ¦‚ç‡ <strong>$P(y|x)$</strong>ã€‚</li>
<li><strong>å†³ç­–è¾¹ç•Œ (Decision Boundary)</strong>: å®ƒåªå…³å¿ƒæ€ä¹ˆæŠŠä¸¤ç±»æ•°æ®åˆ†å¼€ï¼Œä¸å…³å¿ƒæ•°æ®é•¿ä»€ä¹ˆæ ·ã€‚</li>
<li><em>Analogy</em>: Like learning a rule to distinguish cats from dogs without knowing how to draw them.</li>
</ul>
</li>
<li>
<p><strong>ç”Ÿæˆæ¨¡å‹ (Generative Models)</strong>:</p>
<ul>
<li><strong>ç›®æ ‡ (Goal)</strong>: æè¿°æ•°æ® $x$ æ˜¯å¦‚ä½•ç”Ÿæˆçš„ã€‚</li>
<li><strong>æ•°å­¦ (Math)</strong>: å»ºæ¨¡è”åˆæ¦‚ç‡ <strong>$P(x, y)$</strong> æˆ–è¾¹ç¼˜æ¦‚ç‡ <strong>$P(x)$</strong>ã€‚</li>
<li><strong>è´å¶æ–¯å…¬å¼ (Bayes&rsquo; Rule)</strong>: ç”Ÿæˆæ¨¡å‹å¯ä»¥é€šè¿‡è´å¶æ–¯å…¬å¼è½¬åŒ–ä¸ºåˆ¤åˆ«æ¨¡å‹ï¼š
$$P(y|x) = \frac{P(x|y)P(y)}{P(x)}$$</li>
<li><strong>ä¼˜åŠ¿ (Advantage)</strong>: è¯¾ä»¶ç‰¹åˆ«æåˆ°ï¼Œç”Ÿæˆæ¨¡å‹åœ¨<strong>ç¼ºå¤±æ•°æ® (Missing Data)</strong> çš„æƒ…å†µä¸‹ä¾ç„¶æœ‰æ•ˆã€‚å› ä¸ºå®ƒå¯ä»¥å¯¹æœªè§‚å¯Ÿåˆ°çš„å˜é‡è¿›è¡Œè¾¹ç¼˜åŒ– (Marginalize)ï¼Œè€Œåˆ¤åˆ«æ¨¡å‹å¿…é¡»ä¾èµ–å®Œæ•´çš„ $x$ã€‚</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-æ ¸å¿ƒæŒ‘æˆ˜ç»´æ•°ç¾éš¾">2. æ ¸å¿ƒæŒ‘æˆ˜ï¼šç»´æ•°ç¾éš¾<a hidden class="anchor" aria-hidden="true" href="#2-æ ¸å¿ƒæŒ‘æˆ˜ç»´æ•°ç¾éš¾">#</a></h2>
<h3 id="the-core-challenge-curse-of-dimensionality">The Core Challenge: Curse of Dimensionality<a hidden class="anchor" aria-hidden="true" href="#the-core-challenge-curse-of-dimensionality">#</a></h3>
<p>è¿™æ˜¯æœ¬èŠ‚è¯¾æå‡ºçš„æœ€æ ¹æœ¬é—®é¢˜ï¼šä¸ºä»€ä¹ˆç”Ÿæˆæ¨¡å‹è¿™ä¹ˆéš¾åšï¼Ÿ</p>
<ul>
<li><strong>é—®é¢˜æè¿° (Problem)</strong>: å‡è®¾æˆ‘ä»¬è¦ä¸ºä¸€ä¸ªç®€å•çš„ $28 \times 28$ é»‘ç™½åƒç´ å›¾åƒï¼ˆå¦‚ MNIST æ•°å­—ï¼‰å»ºæ¨¡è”åˆåˆ†å¸ƒã€‚
<ul>
<li>æ¯ä¸ªåƒç´ æœ‰ 2 ç§çŠ¶æ€ï¼ˆ0 æˆ– 1ï¼‰ã€‚</li>
<li>æ€»å…±æœ‰ $784$ ä¸ªåƒç´ ã€‚</li>
<li>é‚£ä¹ˆè¿™å¹…å›¾å¯èƒ½çš„çŠ¶æ€æ€»æ•°æ˜¯ <strong>$2^{784}$</strong>ã€‚</li>
<li><em>Concept</em>: The number of possible configurations grows exponentially with the number of variables (pixels).</li>
</ul>
</li>
<li><strong>ç»“è®º (Conclusion)</strong>: æˆ‘ä»¬ä¸å¯èƒ½åˆ—å‡ºä¸€å¼ è¡¨æ¥è®°å½•æ¯ä¸€ä¸ªå¯èƒ½å›¾åƒçš„æ¦‚ç‡ã€‚æ‰€éœ€çš„å‚æ•°é‡è¿œè¿œè¶…è¿‡äº†å®‡å®™ä¸­åŸå­çš„æ•°é‡ã€‚</li>
<li><strong>è§£å†³æ–¹æ¡ˆ (Solution)</strong>: æˆ‘ä»¬å¿…é¡»å¼•å…¥<strong>å‡è®¾ (Assumptions)</strong> å’Œ <strong>ç»“æ„ (Structure)</strong> æ¥å‡å°‘å‚æ•°é‡ã€‚</li>
</ul>
<hr>
<h2 id="3-ç»“æ„åŒ–æ¨¡å‹è´å¶æ–¯ç½‘ç»œ">3. ç»“æ„åŒ–æ¨¡å‹ï¼šè´å¶æ–¯ç½‘ç»œ<a hidden class="anchor" aria-hidden="true" href="#3-ç»“æ„åŒ–æ¨¡å‹è´å¶æ–¯ç½‘ç»œ">#</a></h2>
<h3 id="structured-models-bayesian-networks">Structured Models: Bayesian Networks<a hidden class="anchor" aria-hidden="true" href="#structured-models-bayesian-networks">#</a></h3>
<p>ä¸ºäº†è§£å†³ç»´æ•°ç¾éš¾ï¼Œæˆ‘ä»¬å¼•å…¥äº†â€œæ¡ä»¶ç‹¬ç«‹æ€§â€å‡è®¾ã€‚</p>
<h4 id="a-é“¾å¼æ³•åˆ™-the-chain-rule">A. é“¾å¼æ³•åˆ™ (The Chain Rule)<a hidden class="anchor" aria-hidden="true" href="#a-é“¾å¼æ³•åˆ™-the-chain-rule">#</a></h4>
<p>ä»»ä½•å¤æ‚çš„è”åˆåˆ†å¸ƒéƒ½å¯ä»¥åˆ†è§£ä¸ºæ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ï¼š
$$P(x_1, x_2, x_3, x_4) = P(x_1) P(x_2|x_1) P(x_3|x_1, x_2) P(x_4|x_1, x_2, x_3)$$</p>
<ul>
<li><strong>è§£é‡Š</strong>: å°±åƒè®²æ•…äº‹ï¼Œåé¢çš„æƒ…èŠ‚å–å†³äºå‰é¢çš„é“ºå«ã€‚</li>
<li><strong>é—®é¢˜</strong>: å³ä½¿åˆ†è§£äº†ï¼Œå‚æ•°é‡å¹¶æ²¡æœ‰å‡å°‘ã€‚æœ€åå‡ é¡¹ä¾ç„¶éå¸¸å¤æ‚ã€‚</li>
</ul>
<h4 id="b-æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾-conditional-independence-assumption">B. æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾ (Conditional Independence Assumption)<a hidden class="anchor" aria-hidden="true" href="#b-æ¡ä»¶ç‹¬ç«‹æ€§å‡è®¾-conditional-independence-assumption">#</a></h4>
<p>è¿™æ˜¯è´å¶æ–¯ç½‘ç»œçš„æ ¸å¿ƒã€‚æˆ‘ä»¬å‡è®¾ï¼š<strong>æ¯ä¸ªå˜é‡åªä¾èµ–äºå®ƒçš„å°‘æ•°å‡ ä¸ªâ€œçˆ¶èŠ‚ç‚¹â€ï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„æ‰€æœ‰å˜é‡ã€‚</strong></p>
<ul>
<li><em>Key Idea</em>: Variable $x_i$ is independent of its non-descendants given its parents.</li>
</ul>
<h4 id="c-è´å¶æ–¯ç½‘ç»œ-bayesian-networks">C. è´å¶æ–¯ç½‘ç»œ (Bayesian Networks)<a hidden class="anchor" aria-hidden="true" href="#c-è´å¶æ–¯ç½‘ç»œ-bayesian-networks">#</a></h4>
<ul>
<li><strong>å®šä¹‰</strong>: ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ (DAG)ï¼Œå…¶ä¸­èŠ‚ç‚¹ä»£è¡¨å˜é‡ï¼Œè¾¹ä»£è¡¨ä¾èµ–å…³ç³»ã€‚</li>
<li><strong>å…¬å¼</strong>: $P(x_1, &hellip;, x_n) = \prod_{i=1}^{n} P(x_i | \text{Parents}(x_i))$</li>
<li><strong>æ•ˆæœ</strong>: æå¤§åœ°å‡å°‘äº†å‚æ•°é‡ã€‚
<ul>
<li><strong>å…¨è¿æ¥ (Fully Connected)</strong>: æ¯ä¸ªå˜é‡éƒ½ä¾èµ–æ‰€æœ‰å‰åºå˜é‡ $\rightarrow$ å‚æ•°çˆ†ç‚¸ã€‚</li>
<li><strong>ç¨€ç–è¿æ¥ (Sparse)</strong>: æ¯ä¸ªå˜é‡åªä¾èµ– 1-2 ä¸ªçˆ¶èŠ‚ç‚¹ $\rightarrow$ å‚æ•°å¯æ§ã€‚</li>
</ul>
</li>
</ul>
<h4 id="d-ç»å…¸æ¡ˆä¾‹æœ´ç´ è´å¶æ–¯-naive-bayes">D. ç»å…¸æ¡ˆä¾‹ï¼šæœ´ç´ è´å¶æ–¯ (Naive Bayes)<a hidden class="anchor" aria-hidden="true" href="#d-ç»å…¸æ¡ˆä¾‹æœ´ç´ è´å¶æ–¯-naive-bayes">#</a></h4>
<ul>
<li>è¿™æ˜¯è´å¶æ–¯ç½‘ç»œçš„ä¸€ä¸ªæç«¯ç‰¹ä¾‹ã€‚</li>
<li><strong>å‡è®¾</strong>: ç»™å®šç±»åˆ« $y$ åï¼Œæ‰€æœ‰çš„ç‰¹å¾ $x_i$ éƒ½æ˜¯<strong>ç›¸äº’ç‹¬ç«‹</strong>çš„ã€‚
<ul>
<li><em>Assumption</em>: All features are independent given the class label.</li>
</ul>
</li>
<li><strong>å±€é™</strong>: è¿™ä¸ªå‡è®¾å¤ªå¼ºäº†ï¼ˆç°å®ä¸­åƒç´ ä¹‹é—´è‚¯å®šæœ‰å…³è”ï¼‰ï¼Œæ‰€ä»¥å®ƒç”Ÿæˆçš„å›¾ç‰‡é€šå¸¸å…¨æ˜¯å™ªç‚¹ï¼Œæ•ˆæœä¸å¥½ã€‚ä½†å®ƒä½œä¸ºåˆ†ç±»å™¨æ•ˆæœè¿˜ä¸é”™ã€‚</li>
</ul>
<hr>
<h2 id="4-ç¥ç»æ¨¡å‹å¼•å…¥æ·±åº¦å­¦ä¹ ">4. ç¥ç»æ¨¡å‹ï¼šå¼•å…¥æ·±åº¦å­¦ä¹ <a hidden class="anchor" aria-hidden="true" href="#4-ç¥ç»æ¨¡å‹å¼•å…¥æ·±åº¦å­¦ä¹ ">#</a></h2>
<h3 id="neural-models-merging-probability-with-deep-learning">Neural Models: Merging Probability with Deep Learning<a hidden class="anchor" aria-hidden="true" href="#neural-models-merging-probability-with-deep-learning">#</a></h3>
<p>ä¼ ç»Ÿçš„å›¾æ¨¡å‹ï¼ˆå¦‚è´å¶æ–¯ç½‘ç»œï¼‰éœ€è¦ä¸“å®¶æ‰‹å·¥è®¾è®¡ä¾èµ–å…³ç³»å›¾ï¼Œè¿™å¾ˆéš¾ã€‚ç°åœ¨çš„è¶‹åŠ¿æ˜¯ç»“åˆç¥ç»ç½‘ç»œã€‚</p>
<ul>
<li><strong>å‚æ•°åŒ– (Parameterization)</strong>:
åœ¨è´å¶æ–¯ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç”¨è¡¨æ ¼æˆ–ç®€å•å‡½æ•°æ¥è¡¨ç¤º $P(x_i | \text{Parents}(x_i))$ã€‚
åœ¨ç¥ç»æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ª<strong>ç¥ç»ç½‘ç»œ</strong>æ¥æ‹Ÿåˆè¿™ä¸ªæ¡ä»¶æ¦‚ç‡å‡½æ•°ã€‚
<ul>
<li><em>Mechanism</em>: Use a neural network to output the probability distribution parameters.</li>
</ul>
</li>
<li><strong>éçº¿æ€§ä¾èµ– (Non-linear Dependence)</strong>:
çº¿æ€§æ¨¡å‹ï¼ˆå¦‚é€»è¾‘å›å½’ï¼‰å‡è®¾å˜é‡é—´æ˜¯çº¿æ€§å…³ç³»ã€‚è€Œç¥ç»ç½‘ç»œé€šè¿‡æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ Sigmoid, ReLUï¼‰å¼•å…¥äº†éçº¿æ€§ï¼Œèƒ½å¤Ÿæ•æ‰æ›´å¤æ‚çš„æ•°æ®å…³ç³»ã€‚
$$y = \sigma(Wx + b)$$</li>
<li><strong>ä»å›¾åˆ°ç½‘ (From Graphs to Nets)</strong>:
æˆ‘ä»¬å¯ä»¥é‡å¤å †å ç¥ç»å±‚ï¼Œæ„å»ºæ·±å±‚ç½‘ç»œã€‚è¿™å®é™…ä¸Šæ˜¯åœ¨å­¦ä¹ æ›´å¤æ‚çš„ã€éšå«çš„ä¾èµ–ç»“æ„ï¼Œè€Œä¸éœ€è¦äººå·¥æ˜¾å¼åœ°ç”»å‡ºæ¯ä¸€æ¡è¾¹ã€‚</li>
</ul>
<hr>
<h2 id="5-ä¸“æœ‰åè¯è¡¨-glossary-1">5. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#5-ä¸“æœ‰åè¯è¡¨-glossary-1">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>è”åˆåˆ†å¸ƒ</strong></td>
          <td style="text-align: left">Joint Distribution</td>
          <td style="text-align: left">$P(x, y)$ æˆ– $P(x_1, &hellip;, x_n)$ã€‚æè¿°æ‰€æœ‰å˜é‡åŒæ—¶å–ç‰¹å®šå€¼çš„æ¦‚ç‡ã€‚è¿™æ˜¯ç”Ÿæˆæ¨¡å‹çš„æ ¸å¿ƒã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ¡ä»¶æ¦‚ç‡</strong></td>
          <td style="text-align: left">Conditional Probability</td>
          <td style="text-align: left">$P(A|B)$ã€‚åœ¨äº‹ä»¶ B å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼Œäº‹ä»¶ A å‘ç”Ÿçš„æ¦‚ç‡ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è¾¹ç¼˜åŒ–</strong></td>
          <td style="text-align: left">Marginalization</td>
          <td style="text-align: left">é€šè¿‡å¯¹æŸäº›å˜é‡æ±‚å’Œï¼ˆæˆ–ç§¯åˆ†ï¼‰ï¼Œä»è”åˆåˆ†å¸ƒä¸­å¾—åˆ°å­é›†å˜é‡åˆ†å¸ƒçš„è¿‡ç¨‹ã€‚å¸¸ç”¨äºå¤„ç†ç¼ºå¤±æ•°æ®ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>ç»´æ•°ç¾éš¾</strong></td>
          <td style="text-align: left">Curse of Dimensionality</td>
          <td style="text-align: left">éšç€æ•°æ®ç»´åº¦ï¼ˆç‰¹å¾æ•°é‡ï¼‰å¢åŠ ï¼Œæ•°æ®ç©ºé—´å‘ˆæŒ‡æ•°çº§çˆ†ç‚¸ï¼Œå¯¼è‡´æ•°æ®å˜å¾—æå…¶ç¨€ç–ï¼Œéš¾ä»¥å»ºæ¨¡ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>é“¾å¼æ³•åˆ™</strong></td>
          <td style="text-align: left">Chain Rule (Probability)</td>
          <td style="text-align: left">æ¦‚ç‡è®ºåŸºæœ¬å®šç†ï¼Œå…è®¸å°†è”åˆæ¦‚ç‡åˆ†è§£ä¸ºä¸€ç³»åˆ—æ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è´å¶æ–¯ç½‘ç»œ</strong></td>
          <td style="text-align: left">Bayesian Network</td>
          <td style="text-align: left">ä¸€ç§æ¦‚ç‡å›¾æ¨¡å‹ï¼Œä½¿ç”¨æœ‰å‘æ— ç¯å›¾ (DAG) æ¥è¡¨ç¤ºå˜é‡é—´çš„æ¡ä»¶ä¾èµ–å…³ç³»ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ¡ä»¶ç‹¬ç«‹</strong></td>
          <td style="text-align: left">Conditional Independence</td>
          <td style="text-align: left">å¦‚æœå·²çŸ¥å˜é‡ Zï¼Œå˜é‡ X å’Œ Y äº’ä¸å½±å“ï¼Œåˆ™ç§° X å’Œ Y å…³äº Z æ¡ä»¶ç‹¬ç«‹ã€‚è¿™æ˜¯ç®€åŒ–æ¨¡å‹çš„å…³é”®ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æœ´ç´ è´å¶æ–¯</strong></td>
          <td style="text-align: left">Naive Bayes</td>
          <td style="text-align: left">ä¸€ç§ç®€å•çš„ç”Ÿæˆæ¨¡å‹ï¼Œå‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚å¸¸ä½œä¸ºåŸºå‡†æ¨¡å‹ (Baseline)ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æœ‰å‘æ— ç¯å›¾</strong></td>
          <td style="text-align: left">DAG (Directed Acyclic Graph)</td>
          <td style="text-align: left">è´å¶æ–¯ç½‘ç»œçš„ç»“æ„åŸºç¡€ï¼Œå›¾ä¸­çš„è¾¹æ˜¯æœ‰æ–¹å‘çš„ï¼Œä¸”ä¸å­˜åœ¨é—­ç¯ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>é€»è¾‘å‡½æ•°</strong></td>
          <td style="text-align: left">Logistic Function (Sigmoid)</td>
          <td style="text-align: left">$\sigma(z) = \frac{1}{1+e^{-z}}$ã€‚å¸¸ç”¨äºç¥ç»ç½‘ç»œä¸­å°†è¾“å‡ºå‹ç¼©åˆ° (0,1) ä¹‹é—´ï¼Œè¡¨ç¤ºæ¦‚ç‡ã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary-1">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary-1">#</a></h3>
<p>Week 2 å‘Šè¯‰ä½ ï¼š<strong>å› ä¸ºä¸–ç•Œå¤ªå¤æ‚ï¼ˆç»´æ•°ç¾éš¾ï¼‰ï¼Œæˆ‘ä»¬ä¸èƒ½è›®åŠ›è®°å½•æ‰€æœ‰å¯èƒ½æ€§ã€‚</strong> æˆ‘ä»¬å¿…é¡»<strong>å·æ‡’</strong>â€”â€”è¦ä¹ˆå‡è®¾å˜é‡ä¹‹é—´æ²¡é‚£ä¹ˆå¤šå…³ç³»ï¼ˆè´å¶æ–¯ç½‘ç»œï¼‰ï¼Œè¦ä¹ˆç”¨ä¸€ä¸ªå¼ºå¤§çš„é»‘ç›’ï¼ˆç¥ç»ç½‘ç»œï¼‰å»æ‹Ÿåˆè¿™äº›å…³ç³»ã€‚ç°ä»£ GenAI æ­£æ˜¯é€‰æ‹©äº†åè€…ã€‚</p>
<hr>
<h1 id="cs5494-week-3-autoregressive-models">CS5494 Week 3: Autoregressive Models<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-3-autoregressive-models">#</a></h1>
<h1 id="ç¬¬ä¸‰å‘¨è‡ªå›å½’æ¨¡å‹">ç¬¬ä¸‰å‘¨ï¼šè‡ªå›å½’æ¨¡å‹<a hidden class="anchor" aria-hidden="true" href="#ç¬¬ä¸‰å‘¨è‡ªå›å½’æ¨¡å‹">#</a></h1>
<h2 id="1-æ ¸å¿ƒå®šä¹‰ä»€ä¹ˆæ˜¯è‡ªå›å½’æ¨¡å‹">1. æ ¸å¿ƒå®šä¹‰ï¼šä»€ä¹ˆæ˜¯è‡ªå›å½’æ¨¡å‹ï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#1-æ ¸å¿ƒå®šä¹‰ä»€ä¹ˆæ˜¯è‡ªå›å½’æ¨¡å‹">#</a></h2>
<h3 id="core-definition-what-are-autoregressive-models">Core Definition: What are Autoregressive Models?<a hidden class="anchor" aria-hidden="true" href="#core-definition-what-are-autoregressive-models">#</a></h3>
<p>è‡ªå›å½’æ¨¡å‹æ˜¯ç”Ÿæˆå¼ AI ä¸­æœ€ä¸»æµçš„ä¸€æ´¾ï¼ˆGPT ä¸­çš„ &ldquo;G&rdquo; å°±æ˜¯ Generativeï¼Œå®é™…ä¸Šæ˜¯ Autoregressive çš„ï¼‰ã€‚</p>
<ul>
<li>
<p><strong>åŸºæœ¬æ€æƒ³ (Basic Idea)</strong>:
å°†ç”Ÿæˆé«˜ç»´æ•°æ®ï¼ˆå¦‚ä¸€å¼ å›¾æˆ–ä¸€æ®µè¯ï¼‰çš„ä»»åŠ¡ï¼Œæ‹†è§£ä¸º<strong>åºåˆ—ç”Ÿæˆ (Sequential Generation)</strong> ä»»åŠ¡ã€‚å³ï¼šæ ¹æ®å‰é¢æ‰€æœ‰çš„å†…å®¹ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå†…å®¹ã€‚</p>
<ul>
<li><em>Concept</em>: Decompose the task of generating high-dimensional data into a <strong>sequential generation</strong> task. Predict the next token based on all previous tokens.</li>
</ul>
</li>
<li>
<p><strong>æ•°å­¦åŸºç¡€ï¼šé“¾å¼æ³•åˆ™ (Mathematical Foundation: Chain Rule)</strong>
æˆ‘ä»¬åˆ©ç”¨æ¦‚ç‡é“¾å¼æ³•åˆ™ï¼Œå°†è”åˆåˆ†å¸ƒåˆ†è§£ä¸ºæ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ï¼Œ<strong>ä¸å¼•å…¥ä»»ä½•ç‹¬ç«‹æ€§å‡è®¾</strong>ï¼š
$$P(x_1, &hellip;, x_n) = \prod_{i=1}^{n} P(x_i | x_1, &hellip;, x_{i-1})$$</p>
<ul>
<li><em>Explanation</em>: $x_i$ depends on <strong>all</strong> previous variables $x_{&lt;i}$. This is different from Naive Bayes (which assumes independence).</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-å…¨å¯è§ä¿¡å¿µç½‘ç»œ-fvbn">2. å…¨å¯è§ä¿¡å¿µç½‘ç»œ (FVBN)<a hidden class="anchor" aria-hidden="true" href="#2-å…¨å¯è§ä¿¡å¿µç½‘ç»œ-fvbn">#</a></h2>
<h3 id="fully-visible-belief-networks-fvbn">Fully Visible Belief Networks (FVBN)<a hidden class="anchor" aria-hidden="true" href="#fully-visible-belief-networks-fvbn">#</a></h3>
<p>è¿™æ˜¯ä¸€ä¸ªç†è®ºæ¨¡å‹ï¼Œç”¨æ¥è§£é‡Šè‡ªå›å½’è¿‡ç¨‹çš„å›¾ç»“æ„ã€‚</p>
<ul>
<li><strong>ç»“æ„ (Structure)</strong>:
è¿™æ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ (DAG)ã€‚æ¯ä¸€ä¸ªèŠ‚ç‚¹ $x_i$ éƒ½è¿æ¥åˆ°å®ƒä¹‹å‰çš„æ‰€æœ‰èŠ‚ç‚¹ ($x_1$ åˆ° $x_{i-1}$)ã€‚
<ul>
<li><em>Structure</em>: Every node $x_i$ is connected to all its predecessors.</li>
</ul>
</li>
<li><strong>å‚æ•°åŒ– (Parameterization)</strong>:
æ—¢ç„¶ $x_i$ ä¾èµ–äºå‰é¢æ‰€æœ‰ $x$ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥æ‹Ÿåˆè¿™ä¸ªå…³ç³»ï¼š
$$P(x_i | x_{&lt;i}) = f(x_{&lt;i}; \theta)$$
<ul>
<li><strong>çº¿æ€§æ–¹æ³•</strong>: ä½¿ç”¨é€»è¾‘å›å½’ (Logistic Regression)ã€‚èƒ½åŠ›æœ‰é™ï¼Œå¤„ç†ä¸äº†å¤æ‚å›¾ç‰‡ã€‚</li>
<li><strong>éçº¿æ€§æ–¹æ³•</strong>: ä½¿ç”¨ç¥ç»ç½‘ç»œ (Neural Networks)ï¼Œå¦‚ <strong>NADE (Neural Autoregressive Distribution Estimation)</strong>ã€‚NADE é€šè¿‡å…±äº«æƒé‡çš„æ–¹æ³•ï¼Œç”¨ç¥ç»ç½‘ç»œæ¥é¢„æµ‹åƒç´ çš„æ¡ä»¶æ¦‚ç‡ã€‚</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-å¾ªç¯ç¥ç»ç½‘ç»œ-rnn">3. å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)<a hidden class="anchor" aria-hidden="true" href="#3-å¾ªç¯ç¥ç»ç½‘ç»œ-rnn">#</a></h2>
<h3 id="recurrent-neural-networks-rnn">Recurrent Neural Networks (RNN)<a hidden class="anchor" aria-hidden="true" href="#recurrent-neural-networks-rnn">#</a></h3>
<p>å½“æˆ‘ä»¬è¦å¤„ç†<strong>åºåˆ—æ•°æ® (Sequential Data)</strong>ï¼ˆå¦‚æ–‡æœ¬ã€éŸ³é¢‘ï¼‰ï¼Œè€Œä¸æ˜¯å›ºå®šå¤§å°çš„å›¾ç‰‡æ—¶ï¼ŒFVBN è¿™ç§å›ºå®šç»“æ„çš„å›¾å°±ä¸å¤Ÿç”¨äº†ã€‚æˆ‘ä»¬éœ€è¦ RNNã€‚</p>
<ul>
<li>
<p><strong>æ ¸å¿ƒæœºåˆ¶ï¼šéšçŠ¶æ€ (Core Mechanism: Hidden State)</strong>
RNN å¼•å…¥äº†ä¸€ä¸ªâ€œè®°å¿†å•å…ƒâ€â€”â€”éšçŠ¶æ€ $h_t$ã€‚
$$h_t = \sigma(W x_t + U h_{t-1})$$</p>
<ul>
<li>$x_t$: å½“å‰çš„è¾“å…¥ï¼ˆè¿™ä¸ªå­—ï¼‰ã€‚</li>
<li>$h_{t-1}$: ä¸Šä¸€æ­¥çš„è®°å¿†ï¼ˆä¸Šæ–‡çš„æ„æ€ï¼‰ã€‚</li>
<li>$h_t$: æ›´æ–°åçš„è®°å¿†ï¼ˆåŒ…å«å½“å‰å­—å’Œä¸Šæ–‡çš„æ„æ€ï¼‰ã€‚</li>
<li><em>Function</em>: The hidden state acts as a summary of the entire history up to time $t$.</li>
</ul>
</li>
<li>
<p><strong>RNN çš„è‡´å‘½å¼±ç‚¹ (Issues with RNNs)</strong>:</p>
<ol>
<li><strong>ä¿¡æ¯ç“¶é¢ˆ (Information Bottleneck)</strong>: æ‰€æœ‰çš„å†å²ä¿¡æ¯ï¼ˆå“ªæ€•æ˜¯ä¸€æœ¬ä¹¦ï¼‰éƒ½å¿…é¡»è¢«å‹ç¼©è¿›ä¸€ä¸ªå›ºå®šå¤§å°çš„å‘é‡ $h_t$ ä¸­ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚</li>
<li><strong>æ¢¯åº¦æ¶ˆå¤± (Vanishing Gradients)</strong>: ä¹Ÿå°±æ˜¯â€œé•¿æ—¶è®°å¿†é—å¿˜â€ã€‚æ¨¡å‹å¾ˆéš¾è®°ä½å¾ˆä¹…ä»¥å‰å‡ºç°çš„è¯ã€‚</li>
<li><strong>æ— æ³•å¹¶è¡Œ (Sequential Computation)</strong>: å¿…é¡»ç®—å®Œ $t-1$ æ‰èƒ½ç®— $t$ï¼Œè®­ç»ƒé€Ÿåº¦éå¸¸æ…¢ã€‚</li>
</ol>
</li>
</ul>
<hr>
<h2 id="4-é©å‘½æ€§çªç ´attention-ä¸-transformer">4. é©å‘½æ€§çªç ´ï¼šAttention ä¸ Transformer<a hidden class="anchor" aria-hidden="true" href="#4-é©å‘½æ€§çªç ´attention-ä¸-transformer">#</a></h2>
<h3 id="the-revolution-attention--transformer">The Revolution: Attention &amp; Transformer<a hidden class="anchor" aria-hidden="true" href="#the-revolution-attention--transformer">#</a></h3>
<p>è¿™æ˜¯æœ¬å‘¨è¯¾ä»¶çš„<strong>é‡ä¸­ä¹‹é‡</strong>ï¼Œä¹Ÿæ˜¯ç°ä»£ LLM çš„åŸºçŸ³ã€‚</p>
<h4 id="a-æ³¨æ„åŠ›æœºåˆ¶-attention-mechanism">A. æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)<a hidden class="anchor" aria-hidden="true" href="#a-æ³¨æ„åŠ›æœºåˆ¶-attention-mechanism">#</a></h4>
<p>ä¸ºäº†è§£å†³ RNN â€œè®°ä¸ä½â€çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† Attentionã€‚</p>
<ul>
<li><strong>åŸç† (Principle)</strong>: åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ—¶ï¼Œæ¨¡å‹ä¸å†åªä¾èµ–é‚£ä¸ªè¢«å‹ç¼©çš„ $h_t$ï¼Œè€Œæ˜¯å¯ä»¥<strong>ç›´æ¥â€œå›å¤´çœ‹â€</strong> åŸå§‹è¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªè¯ï¼Œå¹¶æ ¹æ®ç›¸å…³æ€§ç»™äºˆä¸åŒçš„æƒé‡ã€‚
<ul>
<li><em>Analogy</em>: Instead of reading the whole book and trying to summarize it in one sentence (RNN), you can flip back to any previous page to find the relevant information when needed.</li>
</ul>
</li>
</ul>
<h4 id="b-transformer-æ¶æ„">B. Transformer æ¶æ„<a hidden class="anchor" aria-hidden="true" href="#b-transformer-æ¶æ„">#</a></h4>
<p>Transformer å½»åº•æŠ›å¼ƒäº† RNN çš„å¾ªç¯ç»“æ„ï¼Œå®Œå…¨åŸºäº Attentionã€‚</p>
<ul>
<li><strong>è‡ªæ³¨æ„åŠ› (Self-Attention)</strong>:
åºåˆ—ä¸­çš„æ¯ä¸ªè¯éƒ½ä¼šå»â€œå…³æ³¨â€åºåˆ—ä¸­çš„å…¶ä»–æ‰€æœ‰è¯ï¼Œè®¡ç®—å®ƒä»¬ä¹‹é—´çš„å…³è”ã€‚
<ul>
<li><strong>Query (Q), Key (K), Value (V)</strong>: è¿™æ˜¯ Attention çš„æ ¸å¿ƒè®¡ç®—å…¬å¼ã€‚
$$Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$</li>
</ul>
</li>
<li><strong>æ©ç è‡ªæ³¨æ„åŠ› (Masked Self-Attention)</strong>:
<ul>
<li><strong>å…³é”®ç‚¹</strong>: åœ¨è®­ç»ƒç”Ÿæˆæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ä¸èƒ½è®©æ¨¡å‹â€œå·çœ‹â€åˆ°åé¢çš„ç­”æ¡ˆã€‚</li>
<li><em>Explanation</em>: We apply a mask (setting future positions to $-\infty$) so that when predicting position $t$, the model can only attend to positions $1$ to $t-1$. This preserves the autoregressive property.</li>
</ul>
</li>
<li><strong>ä¼˜åŠ¿ (Advantages)</strong>:
<ol>
<li><strong>å¹¶è¡Œè®¡ç®— (Parallelization)</strong>: ä¸ç”¨åƒ RNN é‚£æ ·æ’é˜Ÿç®—ï¼Œå¯ä»¥ä¸€æ¬¡æ€§è¾“å…¥æ•´ä¸ªå¥å­ï¼Œåˆ©ç”¨ GPU ç–¯ç‹‚åŠ é€Ÿã€‚</li>
<li><strong>å…¨å±€è§†é‡ (Global Context)</strong>:æ— è®ºä¸¤ä¸ªè¯è·ç¦»å¤šè¿œï¼Œå®ƒä»¬ä¹‹é—´çš„äº¤äº’è·ç¦»éƒ½æ˜¯ 1ï¼ˆç›´æ¥ç›¸è¿ï¼‰ã€‚</li>
</ol>
</li>
</ul>
<hr>
<h2 id="5-ä¸“æœ‰åè¯è¡¨-glossary-2">5. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#5-ä¸“æœ‰åè¯è¡¨-glossary-2">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>è‡ªå›å½’æ¨¡å‹</strong></td>
          <td style="text-align: left">Autoregressive Model</td>
          <td style="text-align: left">æ ¹æ®ä¹‹å‰çš„åºåˆ—å€¼æ¥é¢„æµ‹å½“å‰å€¼çš„æ¨¡å‹ã€‚æ¯”å¦‚æ ¹æ®å‰æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>é“¾å¼æ³•åˆ™</strong></td>
          <td style="text-align: left">Chain Rule</td>
          <td style="text-align: left">æ¦‚ç‡è®ºå®šç†ï¼Œå…è®¸å°† $P(A, B, C)$ åˆ†è§£ä¸º $P(A)P(B|A)P(C|A,B)$ã€‚æ˜¯è‡ªå›å½’æ¨¡å‹çš„æ•°å­¦åŸºç¡€ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å…¨å¯è§ä¿¡å¿µç½‘ç»œ</strong></td>
          <td style="text-align: left">FVBN (Fully Visible Belief Network)</td>
          <td style="text-align: left">ä¸€ç§æ¦‚ç‡å›¾æ¨¡å‹ï¼Œæ¯ä¸ªå˜é‡éƒ½ä¾èµ–äºå®ƒä¹‹å‰çš„æ‰€æœ‰å˜é‡ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å¾ªç¯ç¥ç»ç½‘ç»œ</strong></td>
          <td style="text-align: left">RNN (Recurrent Neural Network)</td>
          <td style="text-align: left">ä¸“é—¨å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œå…·æœ‰â€œè®°å¿†â€åŠŸèƒ½ï¼ˆéšçŠ¶æ€ï¼‰ï¼Œèƒ½æ•æ‰æ—¶é—´ä¾èµ–æ€§ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ¢¯åº¦æ¶ˆå¤±</strong></td>
          <td style="text-align: left">Vanishing Gradient</td>
          <td style="text-align: left">æ·±åº¦ç½‘ç»œï¼ˆå°¤å…¶æ˜¯ RNNï¼‰è®­ç»ƒä¸­çš„å¸¸è§é—®é¢˜ï¼ŒæŒ‡åå‘ä¼ æ’­æ—¶æ¢¯åº¦å˜å¾—æå°ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•å­¦ä¹ é•¿è·ç¦»ä¾èµ–ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ³¨æ„åŠ›æœºåˆ¶</strong></td>
          <td style="text-align: left">Attention Mechanism</td>
          <td style="text-align: left">å…è®¸æ¨¡å‹åœ¨å¤„ç†å½“å‰ä»»åŠ¡æ—¶ï¼ŒåŠ¨æ€åœ°å…³æ³¨è¾“å…¥åºåˆ—ä¸­ä¸åŒéƒ¨åˆ†çš„æŠ€æœ¯ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è‡ªæ³¨æ„åŠ›</strong></td>
          <td style="text-align: left">Self-Attention</td>
          <td style="text-align: left">Attention çš„ä¸€ç§å˜ä½“ï¼ŒæŒ‡ä¸€ä¸ªåºåˆ—å†…éƒ¨çš„å…ƒç´ ä¹‹é—´äº’ç›¸è¿›è¡Œ Attention è®¡ç®—ï¼Œä»¥æ•æ‰åºåˆ—å†…éƒ¨çš„å…³è”ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ©ç </strong></td>
          <td style="text-align: left">Masking</td>
          <td style="text-align: left">åœ¨ Transformer è®­ç»ƒä¸­ï¼Œé€šè¿‡é®æŒ¡æœªæ¥çš„ Tokenï¼Œé˜²æ­¢æ¨¡å‹â€œä½œå¼Šâ€ï¼ˆçœ‹åˆ°æœªæ¥ï¼‰ï¼Œä»è€Œå¼ºåˆ¶æ¨¡å‹å­¦ä¹ é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Transformer</strong></td>
          <td style="text-align: left">Transformer</td>
          <td style="text-align: left">ç”± Google æå‡ºçš„åŸºäº Attention æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ï¼Œæ˜¯ BERTã€GPT ç­‰ç°ä»£å¤§æ¨¡å‹çš„åŸºç¡€ã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary-2">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary-2">#</a></h3>
<p>Week 3 çš„é€»è¾‘é“¾æ¡æ˜¯ï¼š
<strong>æˆ‘ä»¬è¦ç”Ÿæˆåºåˆ— $\rightarrow$ æœ€åˆç”¨ç®€å•çš„ç»Ÿè®¡æ–¹æ³• (N-gram) $\rightarrow$ åæ¥ç”¨ RNN (ä½†æœ‰è®°å¿†ç“¶é¢ˆä¸”æ…¢) $\rightarrow$ ç°åœ¨ç”¨ Transformer (é€šè¿‡ Attention æœºåˆ¶è§£å†³äº†è®°å¿†å’Œé€Ÿåº¦é—®é¢˜)</strong>ã€‚</p>
<p>ç†è§£äº† <strong>Masked Self-Attention</strong>ï¼Œä½ å°±ç†è§£äº† GPT (Generative Pre-trained Transformer) ä¸ºä»€ä¹ˆèƒ½åƒæ¥é¾™ä¸€æ ·ä¸€ä¸ªä¸ªå­—åœ°ç”Ÿæˆæ–‡æœ¬ã€‚</p>
<hr>
<h1 id="cs5494-week-4-large-language-models-decoder-only-architecture">CS5494 Week 4: Large Language Models (Decoder-only Architecture)<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-4-large-language-models-decoder-only-architecture">#</a></h1>
<h1 id="ç¬¬å››å‘¨å¤§è¯­è¨€æ¨¡å‹ä»…è§£ç å™¨æ¶æ„">ç¬¬å››å‘¨ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆä»…è§£ç å™¨æ¶æ„ï¼‰<a hidden class="anchor" aria-hidden="true" href="#ç¬¬å››å‘¨å¤§è¯­è¨€æ¨¡å‹ä»…è§£ç å™¨æ¶æ„">#</a></h1>
<h2 id="1-å®è§‚æ¶æ„decoder-only-æ¨¡å‹">1. å®è§‚æ¶æ„ï¼šDecoder-only æ¨¡å‹<a hidden class="anchor" aria-hidden="true" href="#1-å®è§‚æ¶æ„decoder-only-æ¨¡å‹">#</a></h2>
<h3 id="macro-architecture-decoder-only-models">Macro Architecture: Decoder-only Models<a hidden class="anchor" aria-hidden="true" href="#macro-architecture-decoder-only-models">#</a></h3>
<p>è¿™æ˜¯æœ¬å‘¨æœ€é‡è¦çš„æ¦‚å¿µã€‚åŸå§‹çš„ Transformer (2017) æ˜¯ Encoder-Decoder æ¶æ„ï¼ˆç”¨äºç¿»è¯‘ï¼‰ã€‚ä½†åæ¥çš„ GPT ç³»åˆ—ï¼ˆåŒ…æ‹¬ Llama, DeepSeekï¼‰å‘ç°ï¼Œåš<strong>ç”Ÿæˆä»»åŠ¡</strong>åªéœ€è¦<strong>Decoder</strong>ã€‚</p>
<ul>
<li><strong>Decoder-only çš„é€»è¾‘</strong>:
æˆ‘ä»¬ä¸éœ€è¦æŠŠè¾“å…¥å‹ç¼©æˆä¸€ä¸ªå‘é‡å†è§£å‹ï¼ˆEncoder-Decoderï¼‰ï¼Œè€Œæ˜¯ç›´æ¥è¿›è¡Œâ€œæ¥é¾™â€ã€‚æ¯ä¸€ä¸ª token æ—¢æ˜¯è¾“å…¥ä¹Ÿæ˜¯è¾“å‡ºçš„ä¸€éƒ¨åˆ†ã€‚
<ul>
<li><em>Concept</em>: Instead of encoding text into a vector and then decoding it, we treat the task as a continuous sequence generation.</li>
<li><strong>ç»“æ„ (Structure)</strong>: å®ƒæ˜¯å¤šä¸ª <strong>Decoder Block</strong> çš„å †å ï¼ˆStackï¼‰ã€‚æ¯ä¸ª Block åŒ…å«ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š
<ol>
<li><strong>Masked Self-Attention</strong> (å¸¦æ©ç çš„è‡ªæ³¨æ„åŠ›)</li>
<li><strong>Feed Forward Neural Network (FFN/MLP)</strong> (å‰é¦ˆç¥ç»ç½‘ç»œ)</li>
</ol>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-é¢„å¤„ç†ç¬¬ä¸€æ­¥åˆ†è¯å™¨-tokenizer">2. é¢„å¤„ç†ç¬¬ä¸€æ­¥ï¼šåˆ†è¯å™¨ (Tokenizer)<a hidden class="anchor" aria-hidden="true" href="#2-é¢„å¤„ç†ç¬¬ä¸€æ­¥åˆ†è¯å™¨-tokenizer">#</a></h2>
<h3 id="preprocessing-step-1-tokenizer">Preprocessing Step 1: Tokenizer<a hidden class="anchor" aria-hidden="true" href="#preprocessing-step-1-tokenizer">#</a></h3>
<p>æ¨¡å‹çœ‹ä¸æ‡‚å•è¯ï¼Œåªèƒ½çœ‹æ‡‚æ•°å­—ã€‚Tokenizer çš„ä½œç”¨å°±æ˜¯æŠŠæ–‡æœ¬åˆ‡å—å¹¶è½¬åŒ–ä¸ºæ•°å­— IDã€‚</p>
<ul>
<li><strong>ä¸ºä»€ä¹ˆä¸æ˜¯æŒ‰å­—ç¬¦ï¼ˆCharacterï¼‰åˆ‡ï¼Ÿ</strong>
<ul>
<li>â€œappleâ€æŒ‰å­—ç¬¦åˆ‡æ˜¯ 5 ä¸ª IDã€‚åºåˆ—å¤ªé•¿ï¼Œä¸”å•ä¸ªå­—æ¯æ²¡æœ‰è¯­ä¹‰ã€‚</li>
</ul>
</li>
<li><strong>ä¸ºä»€ä¹ˆä¸æ˜¯æŒ‰å•è¯ï¼ˆWordï¼‰åˆ‡ï¼Ÿ</strong>
<ul>
<li>è‹±è¯­å•è¯å¤ªå¤šäº†ï¼ˆå‡ åä¸‡ä¸ªï¼‰ï¼Œè¯è¡¨ï¼ˆVocabularyï¼‰ä¼šçˆ†ç‚¸ï¼Œä¸”æ— æ³•å¤„ç†ç”Ÿåƒ»è¯ï¼ˆOOV - Out of Vocabularyï¼‰ã€‚</li>
</ul>
</li>
<li><strong>æœ€ä½³æ–¹æ¡ˆï¼šå­è¯åˆ†è¯ (Sub-word Tokenization)</strong>
<ul>
<li><strong>BPE (Byte Pair Encoding)</strong>: è¿™æ˜¯ç›®å‰æœ€ä¸»æµçš„æ–¹æ³•ã€‚å®ƒä¼šç»Ÿè®¡è¯­æ–™ä¸­å‡ºç°é¢‘ç‡æœ€é«˜çš„â€œå­—ç¬¦å¯¹â€ï¼ŒæŠŠå®ƒä»¬åˆå¹¶æˆä¸€ä¸ªæ–°çš„ tokenã€‚</li>
<li><em>Mechanism</em>: Frequently occurring character pairs are merged into a single token. e.g., &ldquo;learning&rdquo; might be split into &ldquo;learn&rdquo; + &ldquo;ing&rdquo;.</li>
<li><strong>ä¼˜åŠ¿</strong>: æ—¢æ§åˆ¶äº†è¯è¡¨å¤§å°ï¼ˆé€šå¸¸ 3ä¸‡-10ä¸‡ï¼‰ï¼Œåˆèƒ½é€šè¿‡ç»„åˆæ¥è¡¨ç¤ºä»»ä½•è¯ã€‚</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-é¢„å¤„ç†ç¬¬äºŒæ­¥åµŒå…¥å±‚-embedding">3. é¢„å¤„ç†ç¬¬äºŒæ­¥ï¼šåµŒå…¥å±‚ (Embedding)<a hidden class="anchor" aria-hidden="true" href="#3-é¢„å¤„ç†ç¬¬äºŒæ­¥åµŒå…¥å±‚-embedding">#</a></h2>
<h3 id="preprocessing-step-2-embedding">Preprocessing Step 2: Embedding<a hidden class="anchor" aria-hidden="true" href="#preprocessing-step-2-embedding">#</a></h3>
<p>æŠŠ Tokenizer ç»™å‡ºçš„æ•°å­— IDï¼ˆä¾‹å¦‚ &ldquo;cat&rdquo; -&gt; 4521ï¼‰å˜æˆä¸€ä¸ªé«˜ç»´å‘é‡ã€‚</p>
<ul>
<li><strong>è¯­ä¹‰ç©ºé—´ (Semantic Space)</strong>:
Embedding å°†ç¦»æ•£çš„ ID æ˜ å°„åˆ°ä¸€ä¸ªè¿ç»­çš„å‘é‡ç©ºé—´ã€‚åœ¨è¿™ä¸ªç©ºé—´é‡Œï¼Œè¯­ä¹‰ç›¸è¿‘çš„è¯è·ç¦»æ›´è¿‘ã€‚
<ul>
<li><em>Example</em>: Vector(&ldquo;King&rdquo;) - Vector(&ldquo;Man&rdquo;) + Vector(&ldquo;Woman&rdquo;) $\approx$ Vector(&ldquo;Queen&rdquo;).</li>
</ul>
</li>
<li><strong>æŸ¥è¡¨ (Lookup Table)</strong>:
æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå·¨å¤§çš„çŸ©é˜µï¼Œæ¨¡å‹é€šè¿‡ ID å»åœ¨è¿™ä¸ªçŸ©é˜µé‡ŒæŸ¥æ‰¾å¯¹åº”çš„è¡Œã€‚</li>
</ul>
<hr>
<h2 id="4-å…³é”®æŠ€æœ¯ä½ç½®ç¼–ç -positional-embeddingencoding">4. å…³é”®æŠ€æœ¯ï¼šä½ç½®ç¼–ç  (Positional Embedding/Encoding)<a hidden class="anchor" aria-hidden="true" href="#4-å…³é”®æŠ€æœ¯ä½ç½®ç¼–ç -positional-embeddingencoding">#</a></h2>
<h3 id="key-technique-positional-embedding">Key Technique: Positional Embedding<a hidden class="anchor" aria-hidden="true" href="#key-technique-positional-embedding">#</a></h3>
<p>è¿™æ˜¯ Transformer æ¶æ„ä¸­æœ€å¤©æ‰ä½†ä¹Ÿæœ€éš¾ç†è§£çš„è®¾è®¡ä¹‹ä¸€ã€‚</p>
<ul>
<li>
<p><strong>é—®é¢˜ (The Problem)</strong>:
Attention æœºåˆ¶æœ¬è´¨ä¸Šæ˜¯â€œè¯è¢‹æ¨¡å‹â€ï¼ˆBag of Wordsï¼‰ã€‚å®ƒè®¡ç®—çš„æ˜¯ä¸¤ä¸¤ä¹‹é—´çš„å…³è”ï¼Œå¦‚æœä¸åŠ å¹²é¢„ï¼Œâ€œæˆ‘çˆ±ä½ â€å’Œâ€œä½ çˆ±æˆ‘â€åœ¨ Attention çœ‹æ¥æ˜¯ä¸€æ ·çš„ã€‚<strong>æ¨¡å‹ä¸çŸ¥é“è¯çš„é¡ºåºã€‚</strong></p>
<ul>
<li><em>Concept</em>: Attention is permutation invariant. It has no inherent notion of sequence order.</li>
</ul>
</li>
<li>
<p><strong>è§£å†³æ–¹æ¡ˆ (The Solution)</strong>:
æˆ‘ä»¬éœ€è¦ç»™æ¯ä¸ª token çš„å‘é‡é‡Œâ€œåŠ â€ä¸€ç‚¹ä¿¡æ¯ï¼Œå‘Šè¯‰æ¨¡å‹å®ƒåœ¨ç¬¬å‡ ä¸ªä½ç½®ã€‚
$$Input = TokenEmbedding + PositionalEmbedding$$</p>
</li>
<li>
<p><strong>æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç  (Sinusoidal/Cosinusoidal PE)</strong>:
è¯¾ä»¶ä¸­ç‰¹åˆ«æåˆ°äº†è¿™ä¸ªç»å…¸æ–¹æ³•ï¼ˆåŸå§‹ Transformer ä½¿ç”¨ï¼‰ã€‚</p>
<ul>
<li><strong>åŸç†</strong>: ä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦æ³¢æ¥è¡¨ç¤ºä½ç½®ã€‚</li>
<li><em>Reasoning</em>: å°±åƒæ—¶é’Ÿçš„æŒ‡é’ˆï¼Œç§’é’ˆè½¬å¾—å¿«ï¼Œåˆ†é’ˆè½¬å¾—æ…¢ã€‚é€šè¿‡ä¸åŒé¢‘ç‡çš„æ³¢ç»„åˆï¼Œå¯ä»¥å”¯ä¸€åœ°æ ‡è¯†æ¯ä¸€ä¸ªç»å¯¹ä½ç½®ã€‚</li>
<li><strong>å…¬å¼</strong>: $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$</li>
</ul>
</li>
</ul>
<hr>
<h2 id="5-æ ¸å¿ƒæœºåˆ¶å› æœæ³¨æ„åŠ›-causal-attention">5. æ ¸å¿ƒæœºåˆ¶ï¼šå› æœæ³¨æ„åŠ› (Causal Attention)<a hidden class="anchor" aria-hidden="true" href="#5-æ ¸å¿ƒæœºåˆ¶å› æœæ³¨æ„åŠ›-causal-attention">#</a></h2>
<h3 id="core-mechanism-causal-attention-masked-attention">Core Mechanism: Causal Attention (Masked Attention)<a hidden class="anchor" aria-hidden="true" href="#core-mechanism-causal-attention-masked-attention">#</a></h3>
<p>è¿™æ˜¯ GPT (Decoder-only) å’Œ BERT (Encoder-only) çš„æ ¹æœ¬åŒºåˆ«ã€‚</p>
<ul>
<li>
<p><strong>ä¸èƒ½å·çœ‹æœªæ¥ (No Peeking at the Future)</strong>:
åœ¨åšæ–‡æœ¬ç”Ÿæˆï¼ˆè‡ªå›å½’ï¼‰æ—¶ï¼Œå½“æˆ‘ä»¬é¢„æµ‹ç¬¬ $t$ ä¸ªè¯ï¼Œæˆ‘ä»¬åªèƒ½çœ‹åˆ° $1$ åˆ° $t-1$ ä¸ªè¯ã€‚å¦‚æœè®©æ¨¡å‹çœ‹åˆ°äº†ç¬¬ $t+1$ ä¸ªè¯ï¼Œé‚£å°±æ˜¯ä½œå¼Šï¼Œè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ¯«æ— ç”¨å¤„ã€‚</p>
</li>
<li>
<p><strong>æ©ç çŸ©é˜µ (Mask Matrix)</strong>:
æˆ‘ä»¬åœ¨è®¡ç®— Attention Score æ—¶ï¼Œäººä¸ºåœ°æŠŠâ€œæœªæ¥â€ä½ç½®çš„åˆ†æ•°è®¾ä¸ºè´Ÿæ— ç©·ï¼ˆ$-\infty$ï¼‰ã€‚è¿™æ ·ç»è¿‡ Softmax åï¼Œæœªæ¥çš„æƒé‡å°±å˜æˆäº† 0ã€‚</p>
<ul>
<li><em>Visual</em>: è¿™é€šå¸¸è¡¨ç°ä¸ºä¸€ä¸ª<strong>ä¸‹ä¸‰è§’çŸ©é˜µ (Lower Triangular Matrix)</strong>ã€‚</li>
<li><em>Result</em>: <code>Token 3</code> can attend to <code>Token 1, 2, 3</code>, but NOT <code>Token 4, 5</code>.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="6-è®­ç»ƒä¸æ¨ç†-training-vs-inference">6. è®­ç»ƒä¸æ¨ç† (Training vs. Inference)<a hidden class="anchor" aria-hidden="true" href="#6-è®­ç»ƒä¸æ¨ç†-training-vs-inference">#</a></h2>
<h3 id="training-phase-vs-generation-phase">Training Phase vs. Generation Phase<a hidden class="anchor" aria-hidden="true" href="#training-phase-vs-generation-phase">#</a></h3>
<p>è¿™ä¸¤ä¸ªé˜¶æ®µåœ¨æ“ä½œä¸Šæœ‰æ˜¾è‘—åŒºåˆ«ï¼Œè¯¾ä»¶æœ€åä¸“é—¨åšäº†å¯¹æ¯”ã€‚</p>
<ul>
<li>
<p><strong>è®­ç»ƒé˜¶æ®µ (Training)</strong>: <strong>å¹¶è¡Œ (Parallel)</strong></p>
<ul>
<li>æˆ‘ä»¬å·²ç»æ‹¥æœ‰å®Œæ•´çš„å¥å­ï¼ˆGround Truthï¼‰ã€‚è™½ç„¶æˆ‘ä»¬è¦æ¨¡æ‹Ÿâ€œä¸èƒ½çœ‹æœªæ¥â€ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä¸€æ¬¡æ€§æŠŠæ•´å¥è¯è¾“å…¥è¿›å»ï¼Œé€šè¿‡ Mask çŸ©é˜µå¹¶è¡Œè®¡ç®—æ‰€æœ‰ä½ç½®çš„ Lossã€‚è¿™å« <strong>Teacher Forcing</strong>ã€‚</li>
<li><em>Advantage</em>: Very fast, utilizes GPU parallelism.</li>
</ul>
</li>
<li>
<p><strong>ç”Ÿæˆ/æ¨ç†é˜¶æ®µ (Generation/Inference)</strong>: <strong>ä¸²è¡Œ (Sequential)</strong></p>
<ul>
<li>æˆ‘ä»¬çœŸçš„ä¸çŸ¥é“ä¸‹ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆã€‚åªèƒ½ç”Ÿæˆä¸€ä¸ªè¯ï¼ŒæŠŠå®ƒåŠ åˆ°è¾“å…¥é‡Œï¼Œå†ç”Ÿæˆä¸‹ä¸€ä¸ªã€‚</li>
<li><strong>KV Cache</strong>: ä¸ºäº†åŠ é€Ÿï¼Œæˆ‘ä»¬ä¼šæŠŠä¹‹å‰ç®—è¿‡çš„ Key å’Œ Value å­˜èµ·æ¥ï¼Œä¸ç”¨æ¯æ¬¡éƒ½é‡ç®—ä»¥å‰çš„è¯ã€‚</li>
</ul>
</li>
</ul>
<hr>
<h2 id="7-ä¸“æœ‰åè¯è¡¨-glossary">7. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#7-ä¸“æœ‰åè¯è¡¨-glossary">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>ä»…è§£ç å™¨æ¶æ„</strong></td>
          <td style="text-align: left">Decoder-only Architecture</td>
          <td style="text-align: left">åƒ GPT è¿™æ ·çš„æ¶æ„ï¼Œå»æ‰äº† Transformer çš„ç¼–ç å™¨ï¼Œä¸“æ³¨äºè‡ªå›å½’ç”Ÿæˆçš„ç»“æ„ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>åˆ†è¯å™¨</strong></td>
          <td style="text-align: left">Tokenizer</td>
          <td style="text-align: left">å°†æ–‡æœ¬åˆ†å‰²æˆ Tokenï¼ˆæ•°å­— IDï¼‰çš„ç»„ä»¶ã€‚å¸¸è§ç®—æ³•æœ‰ BPE (Byte Pair Encoding)ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>åµŒå…¥</strong></td>
          <td style="text-align: left">Embedding</td>
          <td style="text-align: left">å°†ç¦»æ•£çš„ Token ID è½¬åŒ–ä¸ºè¿ç»­çš„é«˜ç»´å‘é‡ï¼Œæ•æ‰è¯ä¹‰ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>ä½ç½®ç¼–ç </strong></td>
          <td style="text-align: left">Positional Embedding (PE)</td>
          <td style="text-align: left">æ³¨å…¥åˆ° Embedding ä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œä½¿ Attention æœºåˆ¶èƒ½å¤Ÿè¯†åˆ«è¯åºã€‚å¸¸è§æœ‰ Sinusoidal PEã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å› æœæ³¨æ„åŠ›</strong></td>
          <td style="text-align: left">Causal Attention</td>
          <td style="text-align: left">ä¹Ÿå« Masked Attentionã€‚é€šè¿‡æ©ç ç¡®ä¿æ¨¡å‹åœ¨é¢„æµ‹å½“å‰è¯æ—¶ï¼Œåªèƒ½å…³æ³¨åˆ°ä¹‹å‰çš„è¯ï¼Œä¸èƒ½å…³æ³¨æœªæ¥çš„è¯ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ©ç </strong></td>
          <td style="text-align: left">Mask</td>
          <td style="text-align: left">åœ¨ Attention çŸ©é˜µä¸­å°†ç‰¹å®šåŒºåŸŸï¼ˆå¦‚ä¸Šä¸‰è§’åŒºåŸŸï¼‰è®¾ä¸ºè´Ÿæ— ç©·ï¼Œä»¥å±è”½ä¿¡æ¯ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Teacher Forcing</strong></td>
          <td style="text-align: left">Teacher Force</td>
          <td style="text-align: left">è®­ç»ƒæ—¶çš„ä¸€ç§ç­–ç•¥ï¼Œä¸ç®¡æ¨¡å‹ä¸Šä¸€æ­¥é¢„æµ‹å¾—å¯¹ä¸å¯¹ï¼Œä¸‹ä¸€æ­¥è¾“å…¥éƒ½å¼ºåˆ¶ä½¿ç”¨çœŸå®çš„æ­£ç¡®ç­”æ¡ˆã€‚è¿™å…è®¸å¹¶è¡Œè®­ç»ƒã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è‡ªå›å½’</strong></td>
          <td style="text-align: left">Autoregressive</td>
          <td style="text-align: left">æ¯ä¸€ä¸ªè¾“å‡ºéƒ½ä¾èµ–äºä¹‹å‰çš„è¾“å‡ºã€‚å³ $P(x_t</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å‰é¦ˆç¥ç»ç½‘ç»œ</strong></td>
          <td style="text-align: left">FFN / MLP</td>
          <td style="text-align: left">Transformer Block ä¸­çš„å…¨è¿æ¥å±‚éƒ¨åˆ†ï¼Œè´Ÿè´£å¤„ç†å’Œæ•´åˆ Attention æå–çš„ä¿¡æ¯ã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary-3">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary-3">#</a></h3>
<p>Week 4 å‘Šè¯‰ä½ å¦‚ä½•æŠŠä¹‹å‰çš„ç†è®ºæ‹¼è£…æˆä¸€ä¸ª ChatGPTï¼š</p>
<ol>
<li>ç”¨ <strong>Tokenizer</strong> åˆ‡ç¢æ–‡å­—ã€‚</li>
<li>ç”¨ <strong>Embedding</strong> å˜æˆå‘é‡ã€‚</li>
<li>ç”¨ <strong>Positional Encoding</strong> æ ‡è®°é¡ºåºã€‚</li>
<li>è¿›å…¥ <strong>Decoder Block</strong>ï¼Œåˆ©ç”¨ <strong>Causal Attention</strong> ç¡®ä¿åªçœ‹å†å²ä¸çœ‹æœªæ¥ã€‚</li>
<li>å±‚å±‚å †å ï¼Œæœ€åç®—å‡ºä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡ã€‚</li>
</ol>
<hr>
<h1 id="cs5494-week-5-generative-adversarial-networks-gan">CS5494 Week 5: Generative Adversarial Networks (GAN)<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-5-generative-adversarial-networks-gan">#</a></h1>
<h1 id="ç¬¬äº”å‘¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ">ç¬¬äº”å‘¨ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œ<a hidden class="anchor" aria-hidden="true" href="#ç¬¬äº”å‘¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ">#</a></h1>
<h2 id="1-æ ¸å¿ƒç›´è§‰å¯¹æŠ—åšå¼ˆ">1. æ ¸å¿ƒç›´è§‰ï¼šå¯¹æŠ—åšå¼ˆ<a hidden class="anchor" aria-hidden="true" href="#1-æ ¸å¿ƒç›´è§‰å¯¹æŠ—åšå¼ˆ">#</a></h2>
<h3 id="core-intuition-the-adversarial-game">Core Intuition: The Adversarial Game<a hidden class="anchor" aria-hidden="true" href="#core-intuition-the-adversarial-game">#</a></h3>
<p>GAN (Goodfellow et al., 2014) çš„æ ¸å¿ƒæ€æƒ³ä¸åŒäºæ˜¾å¼åœ°å»è®¡ç®—æ¦‚ç‡å¯†åº¦ $P(x)$ï¼ˆåƒ PixelRNN æˆ– GPT é‚£æ ·ï¼‰ï¼Œè€Œæ˜¯é€šè¿‡ä¸¤ä¸ªç¥ç»ç½‘ç»œçš„<strong>å¯¹æŠ—</strong>æ¥éšå¼åœ°å­¦ä¹ åˆ†å¸ƒã€‚</p>
<ul>
<li>
<p><strong>æ¯”å–» (Analogy)</strong>:</p>
<ul>
<li><strong>ç”Ÿæˆå™¨ (Generator, G)</strong>: å°±åƒä¸€ä¸ª<strong>ä¼ªé’åˆ¶é€ è€… (Counterfeiter)</strong>ã€‚å®ƒçš„ç›®æ ‡æ˜¯åˆ¶é€ å‡ºè¿è­¦å¯Ÿéƒ½åˆ†ä¸æ¸…çœŸå‡çš„å‡é’ã€‚</li>
<li><strong>åˆ¤åˆ«å™¨ (Discriminator, D)</strong>: å°±åƒ<strong>è­¦å¯Ÿ (Police)</strong>ã€‚å®ƒçš„ç›®æ ‡æ˜¯å‡†ç¡®åœ°é‰´åˆ«å‡ºå“ªå¼ æ˜¯çœŸé’ï¼Œå“ªå¼ æ˜¯å‡é’ã€‚</li>
<li><strong>è¿‡ç¨‹</strong>: éšç€åšå¼ˆçš„è¿›è¡Œï¼Œè­¦å¯Ÿçš„çœ¼åŠ›è¶Šæ¥è¶Šå¥½ï¼Œé€ å‡è€…çš„æŠ€æœ¯ä¹Ÿå¿…é¡»è¶Šæ¥è¶Šé«˜è¶…ã€‚æœ€ç»ˆï¼Œé€ å‡ºæ¥çš„å‡é’é€¼çœŸåˆ°è­¦å¯Ÿæ— æ³•åˆ†è¾¨ï¼ˆæ¦‚ç‡ä¸º 0.5ï¼‰ã€‚</li>
</ul>
</li>
<li>
<p><strong>éšå¼å¯†åº¦ä¼°è®¡ (Implicit Density Estimation)</strong>:
GAN ä¸ä¼šç›´æ¥å‘Šè¯‰ä½ è¿™å¼ å›¾å‡ºç°çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Œå®ƒåªä¼šç»™ä½ ç”Ÿæˆè¿™å¼ å›¾ã€‚</p>
<ul>
<li><em>Concept</em>: GANs learn a mechanism to sample from the distribution without explicitly defining the probability density function.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-æ•°å­¦æ¶æ„æå°æå¤§åšå¼ˆ">2. æ•°å­¦æ¶æ„ï¼šæå°æå¤§åšå¼ˆ<a hidden class="anchor" aria-hidden="true" href="#2-æ•°å­¦æ¶æ„æå°æå¤§åšå¼ˆ">#</a></h2>
<h3 id="mathematical-architecture-minimax-game">Mathematical Architecture: Minimax Game<a hidden class="anchor" aria-hidden="true" href="#mathematical-architecture-minimax-game">#</a></h3>
<p>ç†è§£ GAN çš„å…³é”®åœ¨äºå®ƒçš„ç›®æ ‡å‡½æ•°ï¼ˆLoss Functionï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ª<strong>æå°æå¤§ (Minimax)</strong> é—®é¢˜ã€‚</p>
<p>$$\min_G \max_D V(D, G) = \mathbb{E}<em>{x \sim p</em>{data}}[\log D(x)] + \mathbb{E}<em>{z \sim p</em>{z}}[\log(1 - D(G(z)))]$$</p>
<ul>
<li>
<p><strong>åˆ¤åˆ«å™¨ D çš„è§†è§’ (Max D)</strong>:</p>
<ul>
<li>å®ƒæƒ³è®© $\log D(x)$ å˜å¤§ï¼ˆæŠŠçœŸå›¾ $x$ åˆ¤ä¸º 1ï¼‰ã€‚</li>
<li>å®ƒæƒ³è®© $\log(1 - D(G(z)))$ å˜å¤§ï¼ˆæŠŠå‡å›¾ $G(z)$ åˆ¤ä¸º 0ï¼‰ã€‚</li>
<li><em>Goal</em>: Maximize the probability of correctly assigning labels to both real and fake examples.</li>
</ul>
</li>
<li>
<p><strong>ç”Ÿæˆå™¨ G çš„è§†è§’ (Min G)</strong>:</p>
<ul>
<li>å®ƒæƒ³è®© $D(G(z))$ æ¥è¿‘ 1ï¼Œä»è€Œä½¿ $\log(1 - D(G(z)))$ å˜å¾—æå°ï¼ˆè´Ÿæ— ç©·ï¼‰ã€‚</li>
<li>å³ï¼šéª—è¿‡åˆ¤åˆ«å™¨ã€‚</li>
<li><em>Goal</em>: Minimize the probability that the discriminator is correct.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-ç†è®ºç—›ç‚¹ä¸ºä»€ä¹ˆåŸå§‹-gan-å¾ˆéš¾è®­ç»ƒ">3. ç†è®ºç—›ç‚¹ï¼šä¸ºä»€ä¹ˆåŸå§‹ GAN å¾ˆéš¾è®­ç»ƒï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#3-ç†è®ºç—›ç‚¹ä¸ºä»€ä¹ˆåŸå§‹-gan-å¾ˆéš¾è®­ç»ƒ">#</a></h2>
<h3 id="theoretical-pain-point-why-is-vanilla-gan-hard-to-train">Theoretical Pain Point: Why is Vanilla GAN Hard to Train?<a hidden class="anchor" aria-hidden="true" href="#theoretical-pain-point-why-is-vanilla-gan-hard-to-train">#</a></h3>
<p>è¯¾ä»¶èŠ±äº†å¾ˆå¤§ç¯‡å¹…è®¨è®ºè·ç¦»åº¦é‡ï¼ˆMetricï¼‰çš„é—®é¢˜ã€‚è¿™æ˜¯ç†è§£ WGAN çš„å‰ç½®çŸ¥è¯†ã€‚</p>
<h4 id="a-ä¼ ç»Ÿçš„è·ç¦»åº¦é‡kl-æ•£åº¦ä¸-js-æ•£åº¦">A. ä¼ ç»Ÿçš„è·ç¦»åº¦é‡ï¼šKL æ•£åº¦ä¸ JS æ•£åº¦<a hidden class="anchor" aria-hidden="true" href="#a-ä¼ ç»Ÿçš„è·ç¦»åº¦é‡kl-æ•£åº¦ä¸-js-æ•£åº¦">#</a></h4>
<ul>
<li><strong>KL æ•£åº¦ (KL Divergence)</strong>: è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒ $P$ å’Œ $Q$ çš„å·®å¼‚ã€‚å®ƒæ˜¯ä¸å¯¹ç§°çš„ã€‚</li>
<li><strong>JS æ•£åº¦ (Jensen-Shannon Divergence)</strong>: KL æ•£åº¦çš„å¯¹ç§°ç‰ˆæœ¬ã€‚åŸå§‹ GAN çš„ä¼˜åŒ–ç›®æ ‡æœ¬è´¨ä¸Šæ˜¯åœ¨æœ€å°åŒ–ç”Ÿæˆåˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´çš„ JS æ•£åº¦ã€‚</li>
</ul>
<h4 id="b-æ¢¯åº¦æ¶ˆå¤±é—®é¢˜-vanishing-gradient-problem">B. æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ (Vanishing Gradient Problem)<a hidden class="anchor" aria-hidden="true" href="#b-æ¢¯åº¦æ¶ˆå¤±é—®é¢˜-vanishing-gradient-problem">#</a></h4>
<ul>
<li><strong>ç°è±¡</strong>: å½“ç”Ÿæˆå™¨ç”Ÿæˆçš„å›¾åƒå¾ˆå·®ï¼Œæˆ–è€…çœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒ<strong>æ²¡æœ‰é‡å  (No Overlap)</strong> æ—¶ï¼ˆåœ¨é«˜ç»´ç©ºé—´ä¸­è¿™å¾ˆå¸¸è§ï¼‰ï¼ŒJS æ•£åº¦æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼ˆ$\log 2$ï¼‰ã€‚</li>
<li><strong>åæœ</strong>: å¸¸æ•°çš„æ¢¯åº¦æ˜¯ 0ã€‚è¿™æ„å‘³ç€<strong>ç”Ÿæˆå™¨å¾—ä¸åˆ°ä»»ä½•åé¦ˆ</strong>ï¼Œä¸çŸ¥é“å¾€å“ªä¸ªæ–¹å‘æ”¹æ‰èƒ½å˜å¥½ã€‚
<ul>
<li><em>Explanation</em>: If the real and fake distributions are disjoint, the JS divergence is constant. This causes vanishing gradients, meaning the generator learns nothing.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-è§£å†³æ–¹æ¡ˆwasserstein-gan-wgan">4. è§£å†³æ–¹æ¡ˆï¼šWasserstein GAN (WGAN)<a hidden class="anchor" aria-hidden="true" href="#4-è§£å†³æ–¹æ¡ˆwasserstein-gan-wgan">#</a></h2>
<h3 id="the-solution-wasserstein-gan">The Solution: Wasserstein GAN<a hidden class="anchor" aria-hidden="true" href="#the-solution-wasserstein-gan">#</a></h3>
<p>ä¸ºäº†è§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„è·ç¦»åº¦é‡â€”â€”<strong>æ¨åœŸæœºè·ç¦» (Earth-Mover Distance)</strong>ï¼Œå³ Wasserstein Distanceã€‚</p>
<ul>
<li>
<p><strong>ç›´è§‚ç†è§£ (Intuition)</strong>:
æƒ³è±¡æŠŠâ€œç”Ÿæˆåˆ†å¸ƒâ€çœ‹ä½œä¸€å †åœŸï¼ŒæŠŠâ€œçœŸå®åˆ†å¸ƒâ€çœ‹ä½œä¸€ä¸ªå‘ã€‚Wasserstein è·ç¦»å°±æ˜¯æŠŠè¿™å †åœŸæ¬åˆ°é‚£ä¸ªå‘é‡Œæ‰€éœ€çš„<strong>æœ€å°å·¥ä½œé‡</strong>ï¼ˆè·ç¦» $\times$ åœŸé‡ï¼‰ã€‚</p>
<ul>
<li><em>Analogy</em>: The minimum cost to transport mass from one distribution to transform it into the other.</li>
</ul>
</li>
<li>
<p><strong>ä¼˜åŠ¿ (Advantages)</strong>:
å³ä½¿ä¸¤ä¸ªåˆ†å¸ƒå®Œå…¨ä¸é‡å ï¼ŒWasserstein è·ç¦»ä¾ç„¶æ˜¯ä¸€ä¸ªå¹³æ»‘çš„æ•°å€¼ï¼ˆä»£è¡¨è·ç¦»å¤šè¿œï¼‰ï¼Œè€Œä¸æ˜¯å¸¸æ•°ã€‚è¿™èƒ½æä¾›<strong>æŒç»­çš„ã€æœ‰æ„ä¹‰çš„æ¢¯åº¦</strong>ï¼ŒæŒ‡å¯¼ç”Ÿæˆå™¨æ…¢æ…¢é è¿‘çœŸå®åˆ†å¸ƒã€‚</p>
<ul>
<li><em>Benefit</em>: Provides meaningful gradients even when distributions are disjoint, stabilizing training.</li>
</ul>
</li>
<li>
<p><strong>å®ç°ç»†èŠ‚ (Implementation Detail)</strong>:
ä¸ºäº†è®¡ç®— Wasserstein è·ç¦»ï¼Œæˆ‘ä»¬éœ€è¦é™åˆ¶åˆ¤åˆ«å™¨ï¼ˆæ­¤æ—¶æ”¹å« Criticï¼‰çš„èƒ½åŠ›ï¼Œè¦æ±‚å®ƒæ»¡è¶³ <strong>1-Lipschitz è¿ç»­æ€§</strong>ã€‚åœ¨ä»£ç ä¸­é€šå¸¸é€šè¿‡<strong>æƒé‡å‰ªè£ (Weight Clipping)</strong> æˆ– <strong>æ¢¯åº¦æƒ©ç½š (Gradient Penalty, WGAN-GP)</strong> æ¥å®ç°ã€‚</p>
</li>
</ul>
<hr>
<h2 id="5-è¿›é˜¶åº”ç”¨cyclegan">5. è¿›é˜¶åº”ç”¨ï¼šCycleGAN<a hidden class="anchor" aria-hidden="true" href="#5-è¿›é˜¶åº”ç”¨cyclegan">#</a></h2>
<h3 id="advanced-application-cyclegan">Advanced Application: CycleGAN<a hidden class="anchor" aria-hidden="true" href="#advanced-application-cyclegan">#</a></h3>
<p>è¯¾ä»¶æœ€åæåˆ°äº† CycleGANï¼Œè¿™æ˜¯è§£å†³<strong>æ— é…å¯¹å›¾åƒç¿»è¯‘ (Unpaired Image-to-Image Translation)</strong> çš„ç¥å™¨ã€‚</p>
<ul>
<li><strong>åœºæ™¯</strong>: ä½ æƒ³æŠŠâ€œé©¬â€å˜æˆâ€œæ–‘é©¬â€ï¼Œä½†ä½ æ²¡æœ‰åŒä¸€åŒ¹é©¬å˜æˆæ–‘é©¬å‰åçš„å¯¹æ¯”ç…§ã€‚ä½ åªæœ‰ä¸€å †é©¬çš„ç…§ç‰‡å’Œä¸€å †æ–‘é©¬çš„ç…§ç‰‡ã€‚</li>
<li><strong>å¾ªç¯ä¸€è‡´æ€§æŸå¤± (Cycle Consistency Loss)</strong>:
<ul>
<li>æ€è·¯ï¼šå¦‚æœæˆ‘æŠŠé©¬å˜æˆæ–‘é©¬ï¼Œå†æŠŠæ–‘é©¬å˜å›é©¬ï¼Œå®ƒåº”è¯¥é•¿å¾—å’ŒåŸæ¥ä¸€æ¨¡ä¸€æ ·ã€‚</li>
<li>å…¬å¼ï¼š$F(G(x)) \approx x$</li>
<li><em>Mechanism</em>: Translating an image to the other domain and back should yield the original image. This constraint prevents mode collapse and ensures the content is preserved.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="6-ä¸“æœ‰åè¯è¡¨-glossary">6. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#6-ä¸“æœ‰åè¯è¡¨-glossary">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</strong></td>
          <td style="text-align: left">GAN (Generative Adversarial Network)</td>
          <td style="text-align: left">ç”±ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ç»„æˆçš„å¯¹æŠ—å¼ç”Ÿæˆæ¨¡å‹æ¡†æ¶ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æå°æå¤§åšå¼ˆ</strong></td>
          <td style="text-align: left">Minimax Game</td>
          <td style="text-align: left">åšå¼ˆè®ºæœ¯è¯­ã€‚åœ¨ GAN ä¸­æŒ‡ç”Ÿæˆå™¨è¯•å›¾æœ€å°åŒ–åˆ¤åˆ«å™¨çš„å‡†ç¡®ç‡ï¼Œè€Œåˆ¤åˆ«å™¨è¯•å›¾æœ€å¤§åŒ–è‡ªå·±çš„å‡†ç¡®ç‡ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>KL æ•£åº¦</strong></td>
          <td style="text-align: left">KL Divergence</td>
          <td style="text-align: left">è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„éå¯¹ç§°æŒ‡æ ‡ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>JS æ•£åº¦</strong></td>
          <td style="text-align: left">JS Divergence (Jensen-Shannon)</td>
          <td style="text-align: left">KL æ•£åº¦çš„å¯¹ç§°å¹³æ»‘ç‰ˆæœ¬ã€‚åŸå§‹ GAN å®é™…ä¸Šæ˜¯åœ¨ä¼˜åŒ–è¿™ä¸ªæŒ‡æ ‡ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ¨¡å¼å´©æºƒ</strong></td>
          <td style="text-align: left">Mode Collapse</td>
          <td style="text-align: left">GAN è®­ç»ƒä¸­çš„å¸¸è§å¤±è´¥æ¨¡å¼ã€‚ç”Ÿæˆå™¨å‘ç°åªèƒ½ç”ŸæˆæŸä¸€ç§ç‰¹å®šçš„æ ·æœ¬ï¼ˆå¦‚åªç”ŸæˆåŒä¸€å¼ è„¸ï¼‰å°±èƒ½éª—è¿‡åˆ¤åˆ«å™¨ï¼Œä»è€Œå¤±å»äº†å¤šæ ·æ€§ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Wasserstein è·ç¦»</strong></td>
          <td style="text-align: left">Wasserstein Distance (Earth-Mover)</td>
          <td style="text-align: left">ä¸€ç§è¡¡é‡åˆ†å¸ƒè·ç¦»çš„æ–¹æ³•ã€‚ç›¸æ¯” JS æ•£åº¦ï¼Œå®ƒåœ¨åˆ†å¸ƒä¸é‡å æ—¶ä¹Ÿèƒ½æä¾›æœ‰æ•ˆçš„æ¢¯åº¦ï¼Œæå¤§ç¨³å®šäº† GAN çš„è®­ç»ƒã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>1-Lipschitz è¿ç»­</strong></td>
          <td style="text-align: left">1-Lipschitz Continuity</td>
          <td style="text-align: left">æ•°å­¦çº¦æŸï¼Œè¦æ±‚å‡½æ•°çš„å˜åŒ–ç‡ä¸èƒ½è¶…è¿‡æŸä¸ªå¸¸æ•°ï¼ˆæ–œç‡ä¸è¶…è¿‡ 1ï¼‰ã€‚è¿™æ˜¯ WGAN æˆç«‹çš„å¿…è¦æ¡ä»¶ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å¾ªç¯ä¸€è‡´æ€§</strong></td>
          <td style="text-align: left">Cycle Consistency</td>
          <td style="text-align: left">CycleGAN çš„æ ¸å¿ƒæ¦‚å¿µã€‚è¦æ±‚ $X \rightarrow Y \rightarrow X$ çš„å˜æ¢ç»“æœåº”è¿˜åŸä¸º $X$ï¼Œä¿è¯äº†ç¿»è¯‘è¿‡ç¨‹ä¸­å†…å®¹çš„ä¿ç•™ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>éšå¼å¯†åº¦</strong></td>
          <td style="text-align: left">Implicit Density</td>
          <td style="text-align: left">ä¸æ˜¾å¼å®šä¹‰ $P(x)$ ä¸åŒï¼Œæ¨¡å‹ä¸ç›´æ¥è®¡ç®—æ¦‚ç‡å€¼ï¼Œè€Œæ˜¯é€šè¿‡é‡‡æ ·æœºåˆ¶é—´æ¥åæ˜ æ•°æ®åˆ†å¸ƒã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary-4">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary-4">#</a></h3>
<p>Week 5 çš„æ ¸å¿ƒæ•™è®­æ˜¯ï¼š<strong>è™½ç„¶ GAN ç”Ÿæˆçš„å›¾ç‰‡å¾ˆé€¼çœŸï¼Œä½†è®­ç»ƒå®ƒéå¸¸å›°éš¾ï¼ˆä¸ç¨³å®šï¼‰ã€‚</strong>
ä¸ºäº†è®© GAN å¬è¯ï¼Œæˆ‘ä»¬éœ€è¦æ›´é«˜çº§çš„æ•°å­¦å·¥å…·ï¼ˆWasserstein è·ç¦»ï¼‰æ¥æ›¿ä»£ä¼ ç»Ÿçš„æ¦‚ç‡åº¦é‡ï¼ˆJS æ•£åº¦ï¼‰ï¼Œä»è€Œä¿è¯ç”Ÿæˆå™¨åœ¨ä»»ä½•æ—¶å€™éƒ½èƒ½æ”¶åˆ°â€œæ”¹è¿›æ–¹å‘â€çš„ä¿¡å·ã€‚</p>
<hr>
<h1 id="cs5494-week-6-variational-autoencoder-vae">CS5494 Week 6: Variational Autoencoder (VAE)<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-6-variational-autoencoder-vae">#</a></h1>
<h1 id="ç¬¬å…­å‘¨å˜åˆ†è‡ªç¼–ç å™¨">ç¬¬å…­å‘¨ï¼šå˜åˆ†è‡ªç¼–ç å™¨<a hidden class="anchor" aria-hidden="true" href="#ç¬¬å…­å‘¨å˜åˆ†è‡ªç¼–ç å™¨">#</a></h1>
<h2 id="1-å‰ç½®æ¦‚å¿µè‡ªç¼–ç å™¨-autoencoder-ae">1. å‰ç½®æ¦‚å¿µï¼šè‡ªç¼–ç å™¨ (Autoencoder, AE)<a hidden class="anchor" aria-hidden="true" href="#1-å‰ç½®æ¦‚å¿µè‡ªç¼–ç å™¨-autoencoder-ae">#</a></h2>
<h3 id="precursor-autoencoder">Precursor: Autoencoder<a hidden class="anchor" aria-hidden="true" href="#precursor-autoencoder">#</a></h3>
<p>åœ¨è®² VAE ä¹‹å‰ï¼Œè¯¾ä»¶å…ˆå›é¡¾äº†æ™®é€šçš„è‡ªç¼–ç å™¨ã€‚</p>
<ul>
<li><strong>ç»“æ„ (Structure)</strong>:
<ul>
<li><strong>Encoder (ç¼–ç å™¨)</strong>: æŠŠé«˜ç»´è¾“å…¥ $x$ï¼ˆå¦‚ä¸€å¼ å›¾ï¼‰å‹ç¼©æˆä½ç»´å‘é‡ $z$ï¼ˆLatent Codeï¼‰ã€‚</li>
<li><strong>Decoder (è§£ç å™¨)</strong>: æŠŠ $z$ è¿˜åŸå› $x$ã€‚</li>
<li><em>Analogy</em>: å°±åƒæŠŠæ–‡ä»¶å‹ç¼©æˆ zip (Encoder)ï¼Œå†è§£å‹å‡ºæ¥ (Decoder)ã€‚</li>
</ul>
</li>
<li><strong>ç“¶é¢ˆå±‚ (Bottleneck)</strong>:
<ul>
<li>ä¸­é—´çš„ $z$ ç»´åº¦è¿œå°äº $x$ï¼ˆä¾‹å¦‚ 784ç»´ $\rightarrow$ 20ç»´ï¼‰ã€‚è¿™è¿«ä½¿æ¨¡å‹å­¦ä¼šæå–â€œç²¾åâ€ç‰¹å¾ï¼Œè€Œä¸æ˜¯æ­»è®°ç¡¬èƒŒã€‚</li>
</ul>
</li>
<li><strong>å±€é™æ€§ (Limitation)</strong>:
<ul>
<li><strong>å®ƒä¸æ˜¯ç”Ÿæˆæ¨¡å‹</strong>ã€‚AE å­¦ä¹ åˆ°çš„æ½œåœ¨ç©ºé—´ï¼ˆLatent Spaceï¼‰æ˜¯ä¸è¿ç»­çš„ã€‚å¦‚æœä½ åœ¨ä¸¤ä¸ªè®­ç»ƒè¿‡çš„ $z$ ä¹‹é—´éšæœºå–ä¸€ä¸ªç‚¹ï¼Œè§£ç å‡ºæ¥çš„å¯èƒ½æ˜¯ä¸€å †ä¹±ç ã€‚å®ƒåªèƒ½â€œå¤è¯»â€ï¼Œä¸èƒ½â€œåˆ›é€ â€ã€‚</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-æ ¸å¿ƒç†è®ºæ½œå˜é‡æ¨¡å‹ä¸éš¾è§£æ€§">2. æ ¸å¿ƒç†è®ºï¼šæ½œå˜é‡æ¨¡å‹ä¸éš¾è§£æ€§<a hidden class="anchor" aria-hidden="true" href="#2-æ ¸å¿ƒç†è®ºæ½œå˜é‡æ¨¡å‹ä¸éš¾è§£æ€§">#</a></h2>
<h3 id="latent-variable-models--intractability">Latent Variable Models &amp; Intractability<a hidden class="anchor" aria-hidden="true" href="#latent-variable-models--intractability">#</a></h3>
<p>ä¸ºäº†è®©æ¨¡å‹èƒ½â€œåˆ›é€ â€ï¼Œæˆ‘ä»¬éœ€è¦å¼•å…¥æ¦‚ç‡ã€‚æˆ‘ä»¬å‡è®¾å›¾ç‰‡ $x$ æ˜¯ç”±æŸä¸ªçœ‹ä¸è§çš„éšå˜é‡ $z$ ç”Ÿæˆçš„ã€‚</p>
<ul>
<li><strong>ç”Ÿæˆè¿‡ç¨‹ (Generation Process)</strong>:
<ol>
<li>å…ˆä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ª $z \sim \mathcal{N}(0, I)$ã€‚</li>
<li>é€šè¿‡ç¥ç»ç½‘ç»œç”Ÿæˆ $x = g(z)$ã€‚</li>
</ol>
</li>
<li><strong>æ•°å­¦éš¾é¢˜ (The Math Problem)</strong>:
<ul>
<li>ä¸ºäº†è®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦æœ€å¤§åŒ–æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶ $\log P(x)$ã€‚</li>
<li>æ ¹æ®æ¦‚ç‡å…¬å¼ï¼š$P(x) = \int P(x|z)P(z) dz$ã€‚</li>
<li><strong>éš¾ç‚¹ (Hard Point)</strong>: è¿™ä¸ªç§¯åˆ†<strong>æå…¶éš¾ç®— (Intractable)</strong>ã€‚å› ä¸º $z$ çš„å¯èƒ½æ€§æ— ç©·æ— å°½ï¼Œæˆ‘ä»¬ä¸å¯èƒ½éå†æ‰€æœ‰çš„ $z$ æ¥ç®—å‡º $P(x)$ã€‚</li>
<li><em>Concept</em>: We cannot calculate the marginal likelihood because integrating over all possible latent variables is computationally impossible.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-è§£å†³æ–¹æ¡ˆå˜åˆ†æ¨æ–­ä¸-elbo-é‡ç‚¹éš¾ç‚¹">3. è§£å†³æ–¹æ¡ˆï¼šå˜åˆ†æ¨æ–­ä¸ ELBO (é‡ç‚¹/éš¾ç‚¹)<a hidden class="anchor" aria-hidden="true" href="#3-è§£å†³æ–¹æ¡ˆå˜åˆ†æ¨æ–­ä¸-elbo-é‡ç‚¹éš¾ç‚¹">#</a></h2>
<h3 id="solution-variational-inference--elbo">Solution: Variational Inference &amp; ELBO<a hidden class="anchor" aria-hidden="true" href="#solution-variational-inference--elbo">#</a></h3>
<p>æ—¢ç„¶ç®—ä¸å‡ºçœŸå®çš„ $P(x)$ï¼Œæˆ‘ä»¬é€€è€Œæ±‚å…¶æ¬¡ï¼Œå»å¯»æ‰¾ä¸€ä¸ª<strong>ä¸‹ç•Œ (Lower Bound)</strong>ï¼Œåªè¦æŠŠè¿™ä¸ªä¸‹ç•Œæ¨å¾—è¶Šé«˜ï¼ŒçœŸå®çš„æ¦‚ç‡ä¹Ÿå°±è¶Šé«˜ã€‚</p>
<h4 id="a-å˜åˆ†æ¨æ–­-variational-inference">A. å˜åˆ†æ¨æ–­ (Variational Inference)<a hidden class="anchor" aria-hidden="true" href="#a-å˜åˆ†æ¨æ–­-variational-inference">#</a></h4>
<p>æˆ‘ä»¬æ— æ³•çŸ¥é“çœŸå®çš„åéªŒæ¦‚ç‡ $P(z|x)$ï¼ˆå³ï¼šç»™å®šè¿™å¼ å›¾ï¼Œå®ƒå¯¹åº”çš„ $z$ åˆ°åº•æ˜¯å¤šå°‘ï¼Ÿï¼‰ã€‚
æ‰€ä»¥ï¼Œæˆ‘ä»¬å¼•å…¥ä¸€ä¸ªæ–°çš„åˆ†å¸ƒ <strong>$Q(z|x)$</strong>ï¼ˆç”±ç¥ç»ç½‘ç»œ Encoder æ‹Ÿåˆï¼‰æ¥<strong>è¿‘ä¼¼</strong>çœŸå®çš„ $P(z|x)$ã€‚</p>
<h4 id="b-elbo-è¯æ®ä¸‹ç•Œ">B. ELBO (è¯æ®ä¸‹ç•Œ)<a hidden class="anchor" aria-hidden="true" href="#b-elbo-è¯æ®ä¸‹ç•Œ">#</a></h4>
<p>è¿™æ˜¯æœ¬å‘¨æœ€æ ¸å¿ƒçš„å…¬å¼ã€‚é€šè¿‡æ•°å­¦æ¨å¯¼ï¼ˆJensenä¸ç­‰å¼ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºï¼š</p>
<p>$$\log P(x) \ge \text{ELBO}$$</p>
<p><strong>ELBO çš„ç»„æˆ (The Decomposition of ELBO)</strong>:
$$\text{ELBO} = \mathbb{E}<em>{z \sim Q}[\log P(x|z)] - D</em>{KL}(Q(z|x) || P(z))$$</p>
<p>è¿™è¡Œå…¬å¼å¯¹åº”äº† VAE çš„ä¸¤ä¸ª Loss éƒ¨åˆ†ï¼Œéå¸¸å…³é”®ï¼Œè¯·ä»”ç»†çœ‹ä¸‹é¢çš„æ·±åº¦è§£é‡Šã€‚</p>
<hr>
<h2 id="4-vae-çš„æ¨¡å‹æ¶æ„ä¸æŸå¤±å‡½æ•°">4. VAE çš„æ¨¡å‹æ¶æ„ä¸æŸå¤±å‡½æ•°<a hidden class="anchor" aria-hidden="true" href="#4-vae-çš„æ¨¡å‹æ¶æ„ä¸æŸå¤±å‡½æ•°">#</a></h2>
<h3 id="vae-architecture--loss-function">VAE Architecture &amp; Loss Function<a hidden class="anchor" aria-hidden="true" href="#vae-architecture--loss-function">#</a></h3>
<p>VAE æŠŠä¸Šé¢çš„æ•°å­¦å…¬å¼å˜æˆäº†å…·ä½“çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚</p>
<ul>
<li>
<p><strong>ç¬¬ä¸€éƒ¨åˆ†ï¼šé‡å»ºæŸå¤± (Reconstruction Loss)</strong></p>
<ul>
<li>å¯¹åº”å…¬å¼ï¼š$\mathbb{E}_{z \sim Q}[\log P(x|z)]$</li>
<li><strong>å«ä¹‰</strong>: å°±åƒæ™®é€šçš„ Autoencoder ä¸€æ ·ï¼Œè¦æ±‚è§£ç å‡ºæ¥çš„å›¾ $\hat{x}$ å’ŒåŸå›¾ $x$ è¶Šåƒè¶Šå¥½ã€‚</li>
<li><em>Intuition</em>: Make sure the output looks like the input.</li>
</ul>
</li>
<li>
<p><strong>ç¬¬äºŒéƒ¨åˆ†ï¼šæ­£åˆ™åŒ–é¡¹ (KL Divergence Regularization)</strong> [âš ï¸éš¾ç‚¹]</p>
<ul>
<li>å¯¹åº”å…¬å¼ï¼š$D_{KL}(Q(z|x) || P(z))$</li>
<li><strong>å«ä¹‰</strong>: å¼ºè¿« Encoder è¾“å‡ºçš„åˆ†å¸ƒ $Q(z|x)$ å°½å¯èƒ½æ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒ $P(z) = \mathcal{N}(0, 1)$ã€‚</li>
<li><strong>ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšï¼Ÿ (Why?)</strong>:
å¦‚æœä¸åŠ è¿™ä¸€é¡¹ï¼Œæ¨¡å‹ä¼šâ€œä½œå¼Šâ€ï¼Œå®ƒä¼šæŠŠæ¯ä¸ªæ•°æ®çš„ $z$ è®°å¾—ç¦»å¾—è¿œè¿œçš„ï¼ˆäº’ä¸é‡å ï¼‰ï¼Œé€€åŒ–æˆæ™®é€šçš„ Autoencoderã€‚
åŠ ä¸Šè¿™ä¸€é¡¹ï¼Œæ‰€æœ‰å›¾ç‰‡çš„ $z$ éƒ½è¢«è¿«æŒ¤åœ¨ä¸€ä¸ªæ ‡å‡†åœ†é‡Œã€‚è¿™æ ·ï¼Œ$z$ ç©ºé—´å°±å˜å¾—<strong>è¿ç»­</strong>äº†ã€‚ä½ åœ¨è¿™ä¸ªåœ†é‡Œéšä¾¿æ’å€¼é‡‡æ ·ï¼Œéƒ½èƒ½ç”Ÿæˆä¸€å¼ åƒæ¨¡åƒæ ·çš„æ–°å›¾ã€‚</li>
</ul>
</li>
</ul>
<hr>
<h2 id="5-å®ç°ç»†èŠ‚é‡å‚æ•°åŒ–æŠ€å·§-reparameterization-trick">5. å®ç°ç»†èŠ‚ï¼šé‡å‚æ•°åŒ–æŠ€å·§ (Reparameterization Trick)<a hidden class="anchor" aria-hidden="true" href="#5-å®ç°ç»†èŠ‚é‡å‚æ•°åŒ–æŠ€å·§-reparameterization-trick">#</a></h2>
<h3 id="implementation-the-reparameterization-trick">Implementation: The Reparameterization Trick<a hidden class="anchor" aria-hidden="true" href="#implementation-the-reparameterization-trick">#</a></h3>
<p>è¿™æ˜¯è¯¾ä»¶ä¸­æåˆ°â€œVariational parametersâ€æ—¶éšå«çš„ä¸€ä¸ªå…³é”®å·¥ç¨‹æŠ€å·§ï¼Œä¹Ÿæ˜¯è€ƒè¯•å¸¸è€ƒç‚¹ã€‚</p>
<ul>
<li><strong>é—®é¢˜</strong>: VAE ä¸­éœ€è¦è¿›è¡Œé‡‡æ · ($z \sim \mathcal{N}(\mu, \sigma^2)$)ã€‚ç”±äºâ€œé‡‡æ ·â€è¿™ä¸ªæ“ä½œæ˜¯éšæœºçš„ï¼Œä¸å¯å¯¼ï¼Œå¯¼è‡´æ— æ³•è¿›è¡Œåå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰ã€‚</li>
<li><strong>æŠ€å·§</strong>: æˆ‘ä»¬æŠŠéšæœºæ€§è½¬ç§»åˆ°ä¸€ä¸ªç‹¬ç«‹çš„å˜é‡ $\epsilon$ ä¸Šã€‚
$$z = \mu + \sigma \odot \epsilon, \quad \text{where } \epsilon \sim \mathcal{N}(0, 1)$$</li>
<li><strong>æ•ˆæœ</strong>: ç°åœ¨ $\mu$ å’Œ $\sigma$ å˜æˆäº†ç¡®å®šæ€§çš„å‚æ•°ï¼Œæ¢¯åº¦å¯ä»¥ç›´æ¥ä¼ å¯¼ç»™ Encoderï¼Œè€Œéšæœºæ€§åªåœ¨å¸¸æ•° $\epsilon$ é‡Œã€‚</li>
</ul>
<hr>
<h2 id="6-ä¸“æœ‰åè¯è¡¨-glossary-1">6. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#6-ä¸“æœ‰åè¯è¡¨-glossary-1">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>è‡ªç¼–ç å™¨</strong></td>
          <td style="text-align: left">Autoencoder (AE)</td>
          <td style="text-align: left">ä¸€ç§ç¥ç»ç½‘ç»œï¼Œæ—¨åœ¨å­¦ä¹ å°†è¾“å…¥å‹ç¼©ä¸ºä½ç»´ç¼–ç ï¼Œå†å°†å…¶é‡å»ºå›åŸå§‹è¾“å…¥ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å˜åˆ†è‡ªç¼–ç å™¨</strong></td>
          <td style="text-align: left">VAE (Variational Autoencoder)</td>
          <td style="text-align: left">åœ¨ AE åŸºç¡€ä¸Šå¼•å…¥æ¦‚ç‡ç”Ÿæˆæ¨¡å‹çš„æ¦‚å¿µï¼Œé€šè¿‡å­¦ä¹ æ•°æ®çš„æ½œåœ¨åˆ†å¸ƒæ¥ç”Ÿæˆæ–°æ•°æ®ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ½œå˜é‡æ¨¡å‹</strong></td>
          <td style="text-align: left">Latent Variable Model</td>
          <td style="text-align: left">å‡è®¾è§‚æµ‹æ•°æ® $x$ æ˜¯ç”±ä¸å¯è§çš„æ½œå˜é‡ $z$ ç”Ÿæˆçš„æ¦‚ç‡æ¨¡å‹ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å˜åˆ†æ¨æ–­</strong></td>
          <td style="text-align: left">Variational Inference</td>
          <td style="text-align: left">ç”¨ä¸€ä¸ªç®€å•çš„åˆ†å¸ƒ $Q$ å»è¿‘ä¼¼éš¾ä»¥è®¡ç®—çš„å¤æ‚åéªŒåˆ†å¸ƒ $P$ çš„æ–¹æ³•ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è¯æ®ä¸‹ç•Œ</strong></td>
          <td style="text-align: left">ELBO (Evidence Lower Bound)</td>
          <td style="text-align: left">å¯¹æ•°ä¼¼ç„¶å‡½æ•°çš„ä¸€ä¸ªä¸‹ç•Œã€‚è®­ç»ƒ VAE çš„æœ¬è´¨å°±æ˜¯æœ€å¤§åŒ– ELBOã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>KL æ•£åº¦</strong></td>
          <td style="text-align: left">KL Divergence</td>
          <td style="text-align: left">è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æŒ‡æ ‡ã€‚åœ¨ VAE ä¸­ç”¨äºæ‹‰è¿‘ $Q(z|x)$ å’Œ $P(z)$ çš„è·ç¦»ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>éš¾è§£æ€§</strong></td>
          <td style="text-align: left">Intractability</td>
          <td style="text-align: left">æŒ‡è®¡ç®—é‡è¿‡å¤§ï¼ˆé€šå¸¸æ˜¯æŒ‡æ•°çº§æˆ–æ— ç©·ç§¯åˆ†ï¼‰ï¼Œæ— æ³•åœ¨æœ‰é™æ—¶é—´å†…ç²¾ç¡®ç®—å‡ºç»“æœã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>é‡å‚æ•°åŒ–æŠ€å·§</strong></td>
          <td style="text-align: left">Reparameterization Trick</td>
          <td style="text-align: left">å°†éšæœºå˜é‡åˆ†è§£ä¸ºç¡®å®šæ€§éƒ¨åˆ†å’Œéšæœºå™ªå£°éƒ¨åˆ†ï¼ˆ$z=\mu+\sigma\epsilon$ï¼‰ï¼Œä½¿é‡‡æ ·è¿‡ç¨‹å˜å¾—å¯å¯¼ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>åéªŒåˆ†å¸ƒ</strong></td>
          <td style="text-align: left">Posterior Distribution</td>
          <td style="text-align: left">$P(z|x)$ã€‚å³â€œçœ‹åˆ°è¿™å¼ å›¾åï¼Œæ¨æ–­å®ƒçš„æ½œåœ¨ç‰¹å¾æ˜¯ä»€ä¹ˆâ€ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å…ˆéªŒåˆ†å¸ƒ</strong></td>
          <td style="text-align: left">Prior Distribution</td>
          <td style="text-align: left">$P(z)$ã€‚æˆ‘ä»¬é¢„å…ˆå‡è®¾çš„æ½œå˜é‡åˆ†å¸ƒï¼ŒVAE ä¸­é€šå¸¸å‡è®¾ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒ $\mathcal{N}(0, 1)$ã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary-5">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary-5">#</a></h3>
<p>Week 6 å‘Šè¯‰ä½ ï¼š<strong>ä¸ºäº†è®©ç¥ç»ç½‘ç»œå…·å¤‡åˆ›é€ åŠ›ï¼Œæˆ‘ä»¬ä¸èƒ½è®©å®ƒæ­»è®°ç¡¬èƒŒåæ ‡ï¼ˆå›ºå®šæ•°å€¼ï¼‰ï¼Œè€Œè¦è®©å®ƒå­¦ä¹ èŒƒå›´ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚</strong>
VAE é€šè¿‡ <strong>ELBO</strong> å°†â€œç”Ÿæˆéš¾é¢˜â€è½¬åŒ–ä¸ºäº†â€œä¼˜åŒ–éš¾é¢˜â€ï¼Œå¹¶é€šè¿‡ <strong>KL æ•£åº¦</strong> ä¿è¯äº†å­¦ä¹ åˆ°çš„ç©ºé—´æ˜¯å¹³æ»‘ã€è¿ç»­çš„ï¼Œä»è€Œèƒ½ç”Ÿæˆæ–°çš„å›¾ç‰‡ã€‚</p>
<hr>
<h1 id="cs5494-week-7-diffusion-models-ddpm">CS5494 Week 7: Diffusion Models (DDPM)<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-7-diffusion-models-ddpm">#</a></h1>
<h1 id="ç¬¬ä¸ƒå‘¨æ‰©æ•£æ¨¡å‹-ddpm">ç¬¬ä¸ƒå‘¨ï¼šæ‰©æ•£æ¨¡å‹ (DDPM)<a hidden class="anchor" aria-hidden="true" href="#ç¬¬ä¸ƒå‘¨æ‰©æ•£æ¨¡å‹-ddpm">#</a></h1>
<h2 id="1-æ ¸å¿ƒç›´è§‰æ¯æ‰å®ƒå†å¤åŸå®ƒ">1. æ ¸å¿ƒç›´è§‰ï¼šæ¯æ‰å®ƒï¼Œå†å¤åŸå®ƒ<a hidden class="anchor" aria-hidden="true" href="#1-æ ¸å¿ƒç›´è§‰æ¯æ‰å®ƒå†å¤åŸå®ƒ">#</a></h2>
<h3 id="core-intuition-destroy-and-restore">Core Intuition: Destroy and Restore<a hidden class="anchor" aria-hidden="true" href="#core-intuition-destroy-and-restore">#</a></h3>
<p>æ‰©æ•£æ¨¡å‹çš„çµæ„Ÿæ¥æºäºéå¹³è¡¡çƒ­åŠ›å­¦ã€‚æƒ³è±¡ä¸€æ»´å¢¨æ°´æ»´å…¥æ¸…æ°´ä¸­ï¼Œéšç€æ—¶é—´æ¨ç§»ï¼Œå¢¨æ°´åˆ†å­ä¼šæ‰©æ•£ï¼Œç›´åˆ°æ•´æ¯æ°´å˜æˆå‡åŒ€çš„æµ‘æµŠé¢œè‰²ã€‚</p>
<ul>
<li>
<p><strong>å‰å‘è¿‡ç¨‹ (Forward Process / Diffusion)</strong>:</p>
<ul>
<li>å°±åƒå¢¨æ°´æ‰©æ•£ã€‚æˆ‘ä»¬å¾€ä¸€å¼ æ¸…æ™°çš„å›¾ç‰‡ä¸Š<strong>ä¸€æ­¥æ­¥åŠ å™ªç‚¹</strong>ã€‚</li>
<li>æœ€åï¼ˆæ¯”å¦‚åŠ äº† 1000 æ­¥åï¼‰ï¼Œå›¾ç‰‡å˜æˆäº†ä¸€å¼ çº¯ç²¹çš„ã€æ²¡æœ‰ä»»ä½•ä¿¡æ¯çš„<strong>é«˜æ–¯å™ªå£° (Gaussian Noise)</strong>ã€‚</li>
<li><em>Analogy</em>: Slowly adding static to a TV screen until the image is gone.</li>
</ul>
</li>
<li>
<p><strong>åå‘è¿‡ç¨‹ (Reverse Process / Denoising)</strong>:</p>
<ul>
<li>è¿™æ˜¯æ¨¡å‹çš„ä»»åŠ¡ã€‚å¦‚æœæˆ‘ä»¬èƒ½å­¦ä¼š**â€œæ—¶é—´å€’æµâ€**ï¼Œä»è¿™ä¸€å¼ çº¯å™ªå£°ä¸­ï¼Œä¸€æ­¥æ­¥æŠŠå™ªç‚¹æ‹¿æ‰ï¼Œå°±èƒ½å˜å›ä¸€å¼ æ¸…æ™°çš„å›¾ã€‚</li>
<li>å› ä¸ºæ˜¯ä»çº¯å™ªå£°ï¼ˆéšæœºæ•°ï¼‰å¼€å§‹â€œå»å™ªâ€ï¼Œæ¯æ¬¡ç”Ÿæˆçš„å›¾éƒ½ä¸ä¸€æ ·ï¼Œè¿™å°±æ˜¯<strong>ç”Ÿæˆ</strong>çš„è¿‡ç¨‹ã€‚</li>
<li></li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-å‰å‘è¿‡ç¨‹åŠ å™ª-the-forward-process">2. å‰å‘è¿‡ç¨‹ï¼šåŠ å™ª (The Forward Process)<a hidden class="anchor" aria-hidden="true" href="#2-å‰å‘è¿‡ç¨‹åŠ å™ª-the-forward-process">#</a></h2>
<h3 id="adding-noise-systematically">Adding Noise systematically<a hidden class="anchor" aria-hidden="true" href="#adding-noise-systematically">#</a></h3>
<p>è¿™æ˜¯ä¸€ä¸ª<strong>å›ºå®š</strong>çš„è¿‡ç¨‹ï¼Œä¸éœ€è¦ä»»ä½•ç¥ç»ç½‘ç»œè®­ç»ƒã€‚æˆ‘ä»¬è§„å®šå¥½æ¯ä¸€æ­¥åŠ å¤šå°‘å™ªå£°ï¼ˆè¿™å« Noise Scheduleï¼Œ$\beta_t$ï¼‰ã€‚</p>
<ul>
<li>
<p><strong>å•æ­¥åŠ å™ª</strong>:
$$q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$$</p>
<ul>
<li>æ„æ€å°±æ˜¯ï¼šç°åœ¨çš„å›¾ $x_t$ = ç¨å¾®æš—ä¸€ç‚¹çš„ä¸Šä¸€å¼ å›¾ $x_{t-1}$ + ä¸€ç‚¹ç‚¹æ–°çš„å™ªå£°ã€‚</li>
</ul>
</li>
<li>
<p><strong>ä»»æ„æ­¥åŠ å™ª (The &ldquo;Jump&rdquo; Property)</strong>: [é‡è¦è€ƒç‚¹]
æˆ‘ä»¬ä¸éœ€è¦ä¸€æ­¥æ­¥ç®—ã€‚æ•°å­¦ä¸Šå¯ä»¥è¯æ˜ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ç®—å‡ºç¬¬ $t$ æ­¥çš„å›¾ $x_t$ é•¿ä»€ä¹ˆæ ·ï¼ˆåŸºäºåŸå›¾ $x_0$ï¼‰ï¼š
$$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$$</p>
<ul>
<li>$\epsilon \sim \mathcal{N}(0, I)$ æ˜¯è¿™ä¸€æ­¥çš„æ€»å™ªå£°ã€‚</li>
<li>$\bar{\alpha}_t$ æ˜¯ä¸€ä¸ªéšæ—¶é—´ $t$ å˜å°çš„ç³»æ•°ï¼ˆä¿¡å·ä¿ç•™æ¯”ä¾‹ï¼‰ã€‚</li>
<li><strong>ç›´è§‰</strong>: $x_t$ å°±æ˜¯â€œåŸå›¾â€å’Œâ€œå™ªå£°â€çš„åŠ æƒæ··åˆã€‚$t$ è¶Šå¤§ï¼ŒåŸå›¾æˆåˆ†è¶Šå°‘ï¼Œå™ªå£°æˆåˆ†è¶Šå¤šã€‚</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-åå‘è¿‡ç¨‹å»å™ª-the-reverse-process">3. åå‘è¿‡ç¨‹ï¼šå»å™ª (The Reverse Process)<a hidden class="anchor" aria-hidden="true" href="#3-åå‘è¿‡ç¨‹å»å™ª-the-reverse-process">#</a></h2>
<h3 id="denoising-with-neural-networks">Denoising with Neural Networks<a hidden class="anchor" aria-hidden="true" href="#denoising-with-neural-networks">#</a></h3>
<p>è¿™æ˜¯æˆ‘ä»¬éœ€è¦è®­ç»ƒçš„éƒ¨åˆ†ã€‚æˆ‘ä»¬æƒ³æ±‚ $q(x_{t-1} | x_t)$ï¼Œå³ï¼šç»™å®šç°åœ¨çš„è„å›¾ï¼Œå®ƒä¸Šä¸€æ—¶åˆ»é•¿ä»€ä¹ˆæ ·ï¼Ÿ
ä½†è¿™ä¸ªåˆ†å¸ƒå¤ªå¤æ‚äº†ï¼Œç®—ä¸å‡ºæ¥ã€‚</p>
<ul>
<li><strong>è§£å†³æ–¹æ¡ˆ</strong>: ç”¨ç¥ç»ç½‘ç»œ $p_\theta(x_{t-1} | x_t)$ å»<strong>æ‹Ÿåˆ</strong>å®ƒã€‚</li>
<li><strong>å‡è®¾</strong>: å½“æ¯ä¸€æ­¥åŠ çš„å™ªå£°æå°æ—¶ï¼Œåå‘è¿‡ç¨‹ä¹Ÿå¯ä»¥è¿‘ä¼¼ä¸ºä¸€ä¸ª<strong>é«˜æ–¯åˆ†å¸ƒ</strong>ï¼š
$$p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$$</li>
<li><strong>ä»»åŠ¡</strong>: ç¥ç»ç½‘ç»œåªéœ€è¦è¾“å…¥ä¸€å¼ è„å›¾ $x_t$ å’Œæ—¶é—´ $t$ï¼Œç„¶åé¢„æµ‹å‡ºè¿™ä¸ªé«˜æ–¯åˆ†å¸ƒçš„<strong>å‡å€¼ $\mu_\theta$</strong> å³å¯ï¼ˆæ–¹å·®é€šå¸¸è®¾ä¸ºå›ºå®šå€¼ï¼‰ã€‚</li>
</ul>
<hr>
<h2 id="4-æ ¸å¿ƒéš¾ç‚¹è®­ç»ƒç›®æ ‡æ˜¯ä»€ä¹ˆ">4. æ ¸å¿ƒéš¾ç‚¹ï¼šè®­ç»ƒç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#4-æ ¸å¿ƒéš¾ç‚¹è®­ç»ƒç›®æ ‡æ˜¯ä»€ä¹ˆ">#</a></h2>
<h3 id="the-hard-part-what-is-the-loss-function">The Hard Part: What is the Loss Function?<a hidden class="anchor" aria-hidden="true" href="#the-hard-part-what-is-the-loss-function">#</a></h3>
<p>è¿™éƒ¨åˆ†æ˜¯è¯¾ä»¶ä¸­å…¬å¼æœ€å¤šçš„åœ°æ–¹ï¼Œä¹Ÿæ˜¯ç†è§£ DDPM (Denoising Diffusion Probabilistic Models) çš„å…³é”®ã€‚</p>
<h4 id="a-ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”±-x_t-æ¨-x_t-1">A. ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”± $x_t$ æ¨ $x_{t-1}$ï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#a-ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”±-x_t-æ¨-x_t-1">#</a></h4>
<p>å› ä¸ºä» $x_t$ å›åˆ° $x_{t-1}$ æœ‰æ— æ•°ç§å¯èƒ½ï¼Œç›´æ¥è®­å¾ˆéš¾æ”¶æ•›ã€‚
ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬<strong>çŸ¥é“åŸå›¾ $x_0$</strong>ï¼Œé‚£ä¹ˆä» $x_t$ æ¨å¯¼ $x_{t-1}$ å°±å˜å¾—<strong>ç¡®å®šä¸”ç®€å•</strong>äº†ã€‚
è¿™å°±å¼•å‡ºäº†æ‰©æ•£æ¨¡å‹æœ€å¤©æ‰çš„æ•°å­¦æ¨å¯¼â€”â€”<strong>åéªŒåˆ†å¸ƒ $q(x_{t-1} | x_t, x_0)$</strong>ã€‚</p>
<h4 id="b-ç®€åŒ–çš„è®­ç»ƒç›®æ ‡-simplified-loss-æœ€é‡è¦ç»“è®º">B. ç®€åŒ–çš„è®­ç»ƒç›®æ ‡ (Simplified Loss) [â­æœ€é‡è¦ç»“è®º]<a hidden class="anchor" aria-hidden="true" href="#b-ç®€åŒ–çš„è®­ç»ƒç›®æ ‡-simplified-loss-æœ€é‡è¦ç»“è®º">#</a></h4>
<p>è™½ç„¶æ•°å­¦æ¨å¯¼ï¼ˆELBOï¼‰å¾ˆå¤æ‚ï¼Œä½† Ho et al. (2020) å‘ç°ï¼Œè®­ç»ƒæ‰©æ•£æ¨¡å‹å…¶å®åªéœ€è¦åšä¸€ä»¶äº‹ï¼š
<strong>é¢„æµ‹å™ªå£° (Predict the Noise)</strong>ã€‚</p>
<ol>
<li>éšæœºæŠ½ä¸€å¼ åŸå›¾ $x_0$ã€‚</li>
<li>éšæœºç”Ÿæˆä¸€ä¸ªå™ªå£° $\epsilon$ã€‚</li>
<li>æŠŠå™ªå£°åŠ åˆ°å›¾ä¸Šå¾—åˆ° $x_t$ï¼ˆåˆ©ç”¨ç¬¬2èŠ‚çš„å…¬å¼ï¼‰ã€‚</li>
<li>æŠŠ $x_t$ ä¸¢ç»™ç¥ç»ç½‘ç»œï¼Œè®©å®ƒçŒœï¼š<strong>â€œåˆšæ‰åŠ åœ¨è¿™ä¸ªå›¾é‡Œçš„å™ªå£° $\epsilon$ é•¿ä»€ä¹ˆæ ·ï¼Ÿâ€</strong></li>
<li><strong>Loss Function</strong>:
$$L_{simple} = || \epsilon - \epsilon_\theta(\underbrace{\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}<em>t} \epsilon}</em>{è¾“å…¥ x_t}, t) ||^2$$
<ul>
<li>å³ï¼šçœŸå®å™ªå£° $\epsilon$ ä¸é¢„æµ‹å™ªå£° $\epsilon_\theta$ çš„<strong>å‡æ–¹è¯¯å·® (MSE)</strong>ã€‚</li>
</ul>
</li>
</ol>
<p><strong>æ€»ç»“</strong>: æ‰©æ•£æ¨¡å‹æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ª**â€œçŒœå™ªå£°â€çš„å»å™ªè‡ªç¼–ç å™¨ (Denoising Autoencoder)**ã€‚</p>
<hr>
<h2 id="5-æ¨ç†ç®—æ³•æ€ä¹ˆç”Ÿæˆå›¾ç‰‡">5. æ¨ç†ç®—æ³•ï¼šæ€ä¹ˆç”Ÿæˆå›¾ç‰‡ï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#5-æ¨ç†ç®—æ³•æ€ä¹ˆç”Ÿæˆå›¾ç‰‡">#</a></h2>
<h3 id="inference-sampling-algorithm">Inference: Sampling Algorithm<a hidden class="anchor" aria-hidden="true" href="#inference-sampling-algorithm">#</a></h3>
<p>è®­ç»ƒå¥½æ¨¡å‹ï¼ˆä¼šçŒœå™ªå£°ï¼‰åï¼Œæˆ‘ä»¬æ€ä¹ˆç”Ÿæˆæ–°å›¾ï¼Ÿ</p>
<ol>
<li><strong>ä»çº¯å™ªå£°å¼€å§‹</strong>: $x_T \sim \mathcal{N}(0, I)$ã€‚</li>
<li><strong>å¾ªç¯ $T$ æ­¥ (ä¾‹å¦‚ 1000 æ­¥)</strong>: ä» $T$ å€’æ•°åˆ° $1$ã€‚
<ul>
<li>è®¡ç®—å½“å‰æ­¥é¢„æµ‹çš„å™ªå£° $\epsilon_\theta(x_t, t)$ã€‚</li>
<li><strong>å»å™ªå…¬å¼ (æ ¸å¿ƒ)</strong>:
$$x_{t-1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}<em>t}} \epsilon</em>\theta(x_t, t)) + \sigma_t z$$</li>
<li><strong>è§£é‡Š</strong>: æŠŠé¢„æµ‹å‡ºæ¥çš„å™ªå£°ä»å›¾é‡Œå‡æ‰ä¸€éƒ¨åˆ†ï¼Œç„¶å<strong>å†åŠ å›ä¸€ç‚¹ç‚¹éšæœºå™ªå£° $z$</strong> (Langevin Dynamics)ã€‚</li>
<li><em>Why add noise back?</em> ä¸ºäº†ä¿æŒç”Ÿæˆçš„éšæœºæ€§å’Œå¤šæ ·æ€§ï¼Œé˜²æ­¢æ¨¡å‹åç¼©åˆ°å•ä¸€ç»“æœã€‚</li>
</ul>
</li>
<li><strong>ç»“æŸ</strong>: å¾—åˆ° $x_0$ï¼Œå°±æ˜¯ç”Ÿæˆçš„ç²¾ç¾å›¾ç‰‡ã€‚</li>
</ol>
<hr>
<h2 id="6-ä¸“æœ‰åè¯è¡¨-glossary-2">6. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#6-ä¸“æœ‰åè¯è¡¨-glossary-2">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>æ‰©æ•£æ¨¡å‹</strong></td>
          <td style="text-align: left">Diffusion Model</td>
          <td style="text-align: left">ä¸€ç±»é€šè¿‡åè½¬å™ªå£°æ‰©æ•£è¿‡ç¨‹æ¥ç”Ÿæˆæ•°æ®çš„ç”Ÿæˆæ¨¡å‹ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>DDPM</strong></td>
          <td style="text-align: left">Denoising Diffusion Probabilistic Models</td>
          <td style="text-align: left">2020å¹´æå‡ºçš„ç»å…¸æ‰©æ•£æ¨¡å‹ç®—æ³•ï¼Œè¯æ˜äº†é¢„æµ‹å™ªå£°ç­‰ä»·äºä¼˜åŒ– ELBOã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å‰å‘è¿‡ç¨‹</strong></td>
          <td style="text-align: left">Forward Process</td>
          <td style="text-align: left">é€æ­¥å‘æ•°æ®æ·»åŠ é«˜æ–¯å™ªå£°ç›´åˆ°å…¶å˜ä¸ºçº¯å™ªå£°çš„è¿‡ç¨‹ã€‚$q(x_t|x_{t-1})$ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>åå‘è¿‡ç¨‹</strong></td>
          <td style="text-align: left">Reverse Process</td>
          <td style="text-align: left">é€æ­¥å»é™¤å™ªå£°ä»¥æ¢å¤æ•°æ®çš„è¿‡ç¨‹ã€‚$p_\theta(x_{t-1}|x_t)$ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å™ªå£°è°ƒåº¦</strong></td>
          <td style="text-align: left">Noise Schedule ($\beta_t$)</td>
          <td style="text-align: left">æ§åˆ¶æ¯ä¸€æ­¥æ·»åŠ å¤šå°‘å™ªå£°çš„è¶…å‚æ•°ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>ELBO</strong></td>
          <td style="text-align: left">Evidence Lower Bound</td>
          <td style="text-align: left">å˜åˆ†æ¨æ–­ä¸­çš„è¯æ®ä¸‹ç•Œã€‚æ‰©æ•£æ¨¡å‹çš„ Loss æœ¬è´¨ä¸Šæ˜¯åœ¨æœ€å¤§åŒ– ELBOã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>é‡å‚æ•°åŒ–æŠ€å·§</strong></td>
          <td style="text-align: left">Reparameterization Trick</td>
          <td style="text-align: left">åœ¨æ‰©æ•£æ¨¡å‹ä¸­æŒ‡ $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†éšæœºæ€§å‰¥ç¦»ï¼Œç›´æ¥å¯¹å™ªå£°è¿›è¡Œæ±‚å¯¼ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>é©¬å°”å¯å¤«é“¾</strong></td>
          <td style="text-align: left">Markov Chain</td>
          <td style="text-align: left">ä¸€ä¸ªéšæœºè¿‡ç¨‹ï¼Œå…¶ä¸­æœªæ¥çš„çŠ¶æ€ä»…å–å†³äºå½“å‰çŠ¶æ€ï¼ˆä¸è¿‡å»æ— å…³ï¼‰ã€‚æ‰©æ•£è¿‡ç¨‹æ˜¯ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>U-Net</strong></td>
          <td style="text-align: left">U-Net</td>
          <td style="text-align: left">æ‰©æ•£æ¨¡å‹ä¸­æœ€å¸¸ç”¨çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå½¢çŠ¶åƒä¸ª Uï¼Œæ“…é•¿å¤„ç†å›¾åƒçš„ç»†èŠ‚å’Œæ•´ä½“ç‰¹å¾ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>Langevin åŠ¨åŠ›å­¦</strong></td>
          <td style="text-align: left">Langevin Dynamics</td>
          <td style="text-align: left">ä¸€ç§ç‰©ç†è¿‡ç¨‹ï¼Œæ‰©æ•£æ¨¡å‹çš„é‡‡æ ·è¿‡ç¨‹ï¼ˆå»å™ª+åŠ éšæœºæ‰°åŠ¨ï¼‰åœ¨æ•°å­¦ä¸Šç±»ä¼¼äº Langevin åŠ¨åŠ›å­¦é‡‡æ ·ã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary-6">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary-6">#</a></h3>
<p>Week 7 çš„æ ¸å¿ƒé€»è¾‘æ˜¯ï¼š
<strong>ç”Ÿæˆ = å»å™ªã€‚</strong>
æˆ‘ä»¬ä¸éœ€è¦åƒ GAN é‚£æ ·ç”¨åˆ¤åˆ«å™¨å»â€œé€¼â€æ¨¡å‹ç”Ÿæˆï¼Œä¹Ÿä¸éœ€è¦åƒ VAE é‚£æ ·æŠŠåˆ†å¸ƒå‹ç¼©åˆ°æ½œç©ºé—´ã€‚
æˆ‘ä»¬åªéœ€è¦æ•™ä¼šç¥ç»ç½‘ç»œä¸€ä»¶äº‹ï¼š<strong>çœ‹åˆ°ä¸€å¼ å…¨æ˜¯å™ªç‚¹çš„å›¾ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œåˆšæ‰åŠ è¿›å»çš„å™ªç‚¹é•¿ä»€ä¹ˆæ ·ï¼Ÿ</strong> åªè¦å®ƒèƒ½æŠŠå™ªç‚¹é¢„æµ‹å‡ºæ¥ï¼Œæˆ‘ä»¬æŠŠå®ƒå‡æ‰ï¼Œå›¾å°±å˜æ¸…æ™°äº†ã€‚</p>
<hr>
<h1 id="cs5494-week-8-ai-agent">CS5494 Week 8: AI Agent<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-8-ai-agent">#</a></h1>
<h1 id="ç¬¬å…«å‘¨äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“">ç¬¬å…«å‘¨ï¼šäººå·¥æ™ºèƒ½æ™ºèƒ½ä½“<a hidden class="anchor" aria-hidden="true" href="#ç¬¬å…«å‘¨äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“">#</a></h1>
<h2 id="1-æ ¸å¿ƒå®šä¹‰ä»€ä¹ˆæ˜¯-ai-agent">1. æ ¸å¿ƒå®šä¹‰ï¼šä»€ä¹ˆæ˜¯ AI Agentï¼Ÿ<a hidden class="anchor" aria-hidden="true" href="#1-æ ¸å¿ƒå®šä¹‰ä»€ä¹ˆæ˜¯-ai-agent">#</a></h2>
<h3 id="core-definition-what-is-an-ai-agent">Core Definition: What is an AI Agent?<a hidden class="anchor" aria-hidden="true" href="#core-definition-what-is-an-ai-agent">#</a></h3>
<p>ä¼ ç»Ÿçš„ LLM (å¦‚ ChatGPT) å°±åƒä¸€ä¸ªâ€œå¤§è„‘åœ¨ç¼¸ä¸­â€ï¼Œå®ƒä»€ä¹ˆéƒ½çŸ¥é“ï¼Œä½†ä»€ä¹ˆéƒ½åšä¸äº†ï¼ˆé™¤äº†è¯´è¯ï¼‰ã€‚AI Agent å°±æ˜¯ç»™è¿™ä¸ªå¤§è„‘è£…ä¸Šäº†<strong>æ‰‹è„šï¼ˆå·¥å…·ï¼‰<strong>å’Œ</strong>é•¿æœŸè®°å¿†</strong>ã€‚</p>
<ul>
<li>
<p><strong>å…¬å¼ (The Formula)</strong>:
$$Agent = LLM + Planning + Memory + Tools$$
[cite_start][cite: 3]</p>
</li>
<li>
<p><strong>åŒºåˆ« (Difference)</strong>:</p>
<ul>
<li><strong>Generative AI (Chatbot)</strong>:
<ul>
<li><strong>æ¨¡å¼</strong>: è¾“å…¥ Prompt $\rightarrow$ è¾“å‡ºæ–‡å­—/å›¾ç‰‡ã€‚</li>
<li><strong>ç‰¹ç‚¹</strong>: <strong>Reactive (è¢«åŠ¨ååº”)</strong>ã€‚ä½ ä¸é—®ï¼Œå®ƒä¸è¯´ã€‚</li>
<li><em>Analogy</em>: Like a library. [cite_start]It contains knowledge but sits still until you open a book. [cite: 53]</li>
</ul>
</li>
<li><strong>AI Agent</strong>:
<ul>
<li><strong>æ¨¡å¼</strong>: è®¾å®šç›®æ ‡ Goal $\rightarrow$ è‡ªä¸»è§„åˆ’ $\rightarrow$ ä½¿ç”¨å·¥å…· $\rightarrow$ å®Œæˆä»»åŠ¡ã€‚</li>
<li><strong>ç‰¹ç‚¹</strong>: <strong>Proactive (ä¸»åŠ¨)</strong>ã€‚å®ƒå¯ä»¥ä¸ºäº†ä¸€ä¸ªç›®æ ‡è¿›è¡Œå¤šæ­¥æ“ä½œã€‚</li>
<li><em>Analogy</em>: Like a personal assistant. [cite_start]You say &ldquo;plan a trip,&rdquo; and it books flights, hotels, and restaurants on its own. [cite: 54]</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-æ ¸å¿ƒèƒ½åŠ›ä¸€è§„åˆ’ä¸æ€è€ƒ-planning--reasoning">2. æ ¸å¿ƒèƒ½åŠ›ä¸€ï¼šè§„åˆ’ä¸æ€è€ƒ (Planning &amp; Reasoning)<a hidden class="anchor" aria-hidden="true" href="#2-æ ¸å¿ƒèƒ½åŠ›ä¸€è§„åˆ’ä¸æ€è€ƒ-planning--reasoning">#</a></h2>
<h3 id="core-capability-1-how-ai-thinks">Core Capability 1: How AI Thinks?<a hidden class="anchor" aria-hidden="true" href="#core-capability-1-how-ai-thinks">#</a></h3>
<p>è¿™æ˜¯æœ¬è¯¾çš„éš¾ç‚¹ã€‚æ¨¡å‹é€šè¿‡ä»€ä¹ˆæ–¹å¼æ¥â€œæ€è€ƒâ€ä¸‹ä¸€æ­¥è¯¥åšä»€ä¹ˆï¼Ÿ</p>
<h4 id="a-è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯-obs-think-act-loop">A. è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯ (Obs-Think-Act Loop)<a hidden class="anchor" aria-hidden="true" href="#a-è§‚å¯Ÿ-æ€è€ƒ-è¡ŒåŠ¨å¾ªç¯-obs-think-act-loop">#</a></h4>
<p>Agent ä¸ä¼šç›´æ¥è·³åˆ°ç»“è®ºï¼Œè€Œæ˜¯ç»å†ä¸€ä¸ªå¾ªç¯ï¼š</p>
<ol>
<li><strong>è§‚å¯Ÿ (Observation)</strong>: ç°åœ¨çš„çŠ¶æ€æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆæ¯”å¦‚ï¼šæ£‹ç›˜ç°åœ¨é•¿ä»€ä¹ˆæ ·ï¼Ÿï¼‰</li>
<li><strong>æ€è€ƒ (Thought)</strong>: åŸºäºè§‚å¯Ÿï¼Œæˆ‘è¯¥æ€ä¹ˆåšï¼Ÿ</li>
<li>[cite_start]<strong>è¡ŒåŠ¨ (Action)</strong>: æ‰§è¡Œæ“ä½œï¼ˆæ¯”å¦‚ï¼šä¸‹æ£‹ï¼Œæˆ–è€…æœç´¢è°·æ­Œï¼‰ã€‚[cite: 3]</li>
</ol>
<h4 id="b-react-æ¡†æ¶-reasoning--acting-é‡ç‚¹">B. ReAct æ¡†æ¶ (Reasoning + Acting) [é‡ç‚¹]<a hidden class="anchor" aria-hidden="true" href="#b-react-æ¡†æ¶-reasoning--acting-é‡ç‚¹">#</a></h4>
<p>è¯¾ä»¶ä¸­éšå«çš„ ReAct æ¦‚å¿µæ˜¯ Agent çš„æ ¸å¿ƒå·¥ä½œæµã€‚</p>
<ul>
<li><strong>åŸç†</strong>: è¦æ±‚ LLM åœ¨æ‰§è¡ŒåŠ¨ä½œä¹‹å‰ï¼Œå…ˆæ˜¾å¼åœ°æŠŠ<strong>æ€è€ƒè¿‡ç¨‹ (Chain of Thought)</strong> å†™å‡ºæ¥ã€‚</li>
<li><strong>ä¾‹å­</strong>:
<ul>
<li><em>ç”¨æˆ·</em>: &ldquo;é©¬æ–¯å…‹ç°åœ¨çš„èº«ä»·æ˜¯å¤šå°‘ï¼Ÿ&rdquo;</li>
<li><em>Agent æ€è€ƒ</em>: &ldquo;æˆ‘éœ€è¦æŸ¥æœ€æ–°çš„æ•°æ®ï¼Œå› ä¸ºæˆ‘çš„è®­ç»ƒæ•°æ®å¯èƒ½è¿‡æ—¶äº†ã€‚&rdquo; (Reasoning)</li>
<li><em>Agent è¡ŒåŠ¨</em>: <code>Search(&quot;Elon Musk net worth today&quot;)</code> (Acting)</li>
<li><em>Agent è§‚å¯Ÿ</em>: &ldquo;æœç´¢ç»“æœæ˜¾ç¤º 2500äº¿ç¾å…ƒã€‚&rdquo; (Observation)</li>
<li><em>Agent å›ç­”</em>: &ldquo;é©¬æ–¯å…‹ç›®å‰èº«ä»·çº¦2500äº¿ç¾å…ƒã€‚&rdquo;</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-æ ¸å¿ƒèƒ½åŠ›äºŒå·¥å…·ä½¿ç”¨-tool-use">3. æ ¸å¿ƒèƒ½åŠ›äºŒï¼šå·¥å…·ä½¿ç”¨ (Tool Use)<a hidden class="anchor" aria-hidden="true" href="#3-æ ¸å¿ƒèƒ½åŠ›äºŒå·¥å…·ä½¿ç”¨-tool-use">#</a></h2>
<h3 id="core-capability-2-how-ai-uses-tools">Core Capability 2: How AI Uses Tools?<a hidden class="anchor" aria-hidden="true" href="#core-capability-2-how-ai-uses-tools">#</a></h3>
<p>LLM æœ¬èº«æ— æ³•è”ç½‘ï¼Œä¹Ÿä¸æ“…é•¿ç²¾ç¡®æ•°å­¦è¿ç®—ã€‚æˆ‘ä»¬éœ€è¦æ•™å®ƒä½¿ç”¨å¤–éƒ¨å·¥å…·ã€‚</p>
<ul>
<li>
<p><strong>å‡½æ•°è°ƒç”¨ (Function Calling)</strong>:</p>
<ul>
<li><strong>åŸç†</strong>: æˆ‘ä»¬ä¸ç›´æ¥æŠŠå·¥å…·ç»™ LLMï¼Œè€Œæ˜¯ç»™å®ƒä¸€æœ¬<strong>è¯´æ˜ä¹¦ (System Prompt)</strong>ã€‚</li>
<li>[cite_start]<em>Prompt ç¤ºä¾‹</em>: &ldquo;å¦‚æœä½ é‡åˆ°ä¸æ‡‚çš„é—®é¢˜ï¼Œä½ å¯ä»¥è¾“å‡º <code>&lt;tool&gt;Search(query)&lt;/tool&gt;</code> æ¥ä½¿ç”¨æœç´¢å¼•æ“ã€‚&rdquo; [cite: 27]</li>
<li><strong>æµç¨‹</strong>:
<ol>
<li>LLM è¾“å‡ºä¸€æ®µç‰¹å®šçš„æ–‡æœ¬ï¼ˆå¦‚ XML æ ‡ç­¾ï¼‰ã€‚</li>
<li>ç³»ç»Ÿæ‹¦æˆªè¿™æ®µæ–‡æœ¬ï¼Œå»è¿è¡ŒçœŸæ­£çš„ Python ä»£ç æˆ– APIã€‚</li>
<li>ç³»ç»ŸæŠŠè¿è¡Œç»“æœï¼ˆæ¯”å¦‚å¤©æ°”æ˜¯ 30åº¦ï¼‰å¡å›ç»™ LLMã€‚</li>
<li>[cite_start]LLM æ ¹æ®ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚ [cite: 29, 33, 34]</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong>è‡ªæˆ‘åˆ¤æ–­ (Judgment)</strong>:</p>
<ul>
<li><strong>éš¾ç‚¹</strong>: å·¥å…·ä¸æ€»æ˜¯å¯¹çš„ã€‚Agent éœ€è¦å…·å¤‡<strong>æ‰¹åˆ¤æ€§æ€ç»´</strong>ã€‚</li>
<li><em>Case</em>: å¦‚æœå¤©æ°” API è¿”å› &ldquo;500Â°C&rdquo;ï¼ŒLLM ä¸åº”ç›´æ¥å¤è¿°ï¼Œè€Œåº”è¯¥åˆ¤æ–­ &ldquo;è¿™æ•°æ®ä¸åˆç†ï¼Œå¯èƒ½æ˜¯å·¥å…·å‡ºé”™äº†&rdquo;ã€‚</li>
<li>[cite_start]<em>Concept</em>: <strong>Internal Knowledge (è‡ªèº«å¸¸è¯†)</strong> vs. <strong>External Knowledge (å·¥å…·ç»“æœ)</strong>ã€‚Agent éœ€è¦æƒè¡¡è¿™ä¸¤è€…ã€‚ [cite: 49, 50, 52]</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-æ ¸å¿ƒèƒ½åŠ›ä¸‰è®°å¿†ä¸ç»éªŒ-memory--experience">4. æ ¸å¿ƒèƒ½åŠ›ä¸‰ï¼šè®°å¿†ä¸ç»éªŒ (Memory &amp; Experience)<a hidden class="anchor" aria-hidden="true" href="#4-æ ¸å¿ƒèƒ½åŠ›ä¸‰è®°å¿†ä¸ç»éªŒ-memory--experience">#</a></h2>
<h3 id="core-capability-3-adjusting-behavior-via-experience">Core Capability 3: Adjusting Behavior via Experience<a hidden class="anchor" aria-hidden="true" href="#core-capability-3-adjusting-behavior-via-experience">#</a></h3>
<p>å¦‚æœ Agent æ¯æ¬¡ä»»åŠ¡ç»“æŸå°±é‡ç½®ï¼Œå®ƒæ°¸è¿œä¸ä¼šè¿›æ­¥ã€‚å®ƒéœ€è¦è®°å¿†ã€‚</p>
<ul>
<li><strong>çŸ­æœŸè®°å¿† (Short-term Memory)</strong>: ä¸Šä¸‹æ–‡çª—å£ (Context Window)ï¼Œè®°å½•å½“å‰çš„å¯¹è¯å†å²ã€‚</li>
<li><strong>é•¿æœŸè®°å¿† (Long-term Memory)</strong>:
<ul>
<li>[cite_start]<strong>RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)</strong>: å°†è¿‡å¾€çš„ç»éªŒå­˜å…¥å‘é‡æ•°æ®åº“ (Vector DB)ã€‚å½“é‡åˆ°ç±»ä¼¼ä»»åŠ¡æ—¶ï¼Œæ£€ç´¢å‡ºç›¸å…³çš„ç»éªŒä¾› LLM å‚è€ƒã€‚ [cite: 15, 19]</li>
<li>[cite_start]<strong>åæ€ (Reflection)</strong>: Agent åœ¨è¡ŒåŠ¨åï¼Œä¼šç”Ÿæˆä¸€æ®µâ€œåæ€æ€»ç»“â€ï¼Œå­˜å…¥è®°å¿†ã€‚æ¯”å¦‚ &ldquo;ä¸Šæ¬¡æˆ‘ç›´æ¥æœç´¢å¤±è´¥äº†ï¼Œä¸‹æ¬¡åº”è¯¥å…ˆæ‹†è§£é—®é¢˜&rdquo;ã€‚ [cite: 21]</li>
</ul>
</li>
</ul>
<hr>
<h2 id="5-è¿›é˜¶æ¦‚å¿µagentic-ai-ä¸å¤šæ™ºèƒ½ä½“">5. è¿›é˜¶æ¦‚å¿µï¼šAgentic AI ä¸å¤šæ™ºèƒ½ä½“<a hidden class="anchor" aria-hidden="true" href="#5-è¿›é˜¶æ¦‚å¿µagentic-ai-ä¸å¤šæ™ºèƒ½ä½“">#</a></h2>
<h3 id="advanced-concept-agentic-ai-systems">Advanced Concept: Agentic AI Systems<a hidden class="anchor" aria-hidden="true" href="#advanced-concept-agentic-ai-systems">#</a></h3>
<p>è¿™æ˜¯ä»â€œå•ä¸ªå‘˜å·¥â€åˆ°â€œå…¬å¸å›¢é˜Ÿâ€çš„è·¨è¶Šã€‚</p>
<ul>
<li><strong>Agentic AI System</strong>:
ä¸æ˜¯å•æ‰“ç‹¬æ–—ï¼Œè€Œæ˜¯å¤šä¸ª Agent åä½œã€‚</li>
<li>[cite_start]<strong>å·¥ä½œæµç¤ºä¾‹ (Youtube -&gt; Rednote)</strong>: [cite: 55]
<ol>
<li><strong>Agent 1 (å¬å½•å‘˜)</strong>: æŠŠè§†é¢‘è½¬æˆæ–‡å­—ã€‚</li>
<li><strong>Agent 2 (æ ‡é¢˜å…š)</strong>: æ ¹æ®æ–‡å­—èµ·ä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜ã€‚</li>
<li><strong>Agent 3 (å†™æ‰‹)</strong>: æ’°å†™æ­£æ–‡ã€‚</li>
<li><strong>Agent 4 (ç¼–è¾‘)</strong>: æ¶¦è‰²å’Œæ’ç‰ˆã€‚</li>
</ol>
</li>
<li><strong>ä¼˜åŠ¿</strong>: æœ¯ä¸šæœ‰ä¸“æ”»ï¼Œæ¯ä¸ª Agent åªéœ€è¦ä¸“æ³¨äºä¸€ä¸ªå°ä»»åŠ¡ï¼Œæ•´ä½“æ•ˆæœæ›´å¥½ã€‚</li>
</ul>
<hr>
<h2 id="6-ä¸“æœ‰åè¯è¡¨-glossary-3">6. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#6-ä¸“æœ‰åè¯è¡¨-glossary-3">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“</strong></td>
          <td style="text-align: left">AI Agent</td>
          <td style="text-align: left">ä¸€ä¸ªæ‹¥æœ‰æ„ŸçŸ¥ã€è§„åˆ’èƒ½åŠ›çš„ç³»ç»Ÿï¼Œèƒ½ä½¿ç”¨å·¥å…·è‡ªä¸»å®Œæˆç›®æ ‡ã€‚å…¬å¼ï¼šLLM + è§„åˆ’ + è®°å¿† + å·¥å…·ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è¢«åŠ¨ååº”</strong></td>
          <td style="text-align: left">Reactive</td>
          <td style="text-align: left">ä¼ ç»Ÿçš„ GenAI æ¨¡å¼ï¼Œåªæœ‰æ”¶åˆ°æŒ‡ä»¤æ‰ä¼šæœ‰è¾“å‡ºï¼Œä¸ä¼šä¸»åŠ¨å‘èµ·è¡ŒåŠ¨ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å‡½æ•°è°ƒç”¨</strong></td>
          <td style="text-align: left">Function Calling</td>
          <td style="text-align: left">å…è®¸ LLM è¾“å‡ºç‰¹å®šæ ¼å¼çš„æŒ‡ä»¤æ¥è§¦å‘å¤–éƒ¨ä»£ç ï¼ˆå¦‚æŸ¥å¤©æ°”ã€æœç½‘é¡µï¼‰çš„æŠ€æœ¯ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ£€ç´¢å¢å¼ºç”Ÿæˆ</strong></td>
          <td style="text-align: left">RAG (Retrieval-Augmented Generation)</td>
          <td style="text-align: left">ç»“åˆäº†æœç´¢ï¼ˆæ£€ç´¢ï¼‰å’Œç”Ÿæˆçš„æŠ€æœ¯ã€‚è®© LLM åœ¨å›ç­”å‰å…ˆå»â€œç¿»ä¹¦â€ï¼ˆæ•°æ®åº“ï¼‰ï¼Œä»è€Œå‡å°‘èƒ¡è¯´å…«é“ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>æ€ç»´é“¾</strong></td>
          <td style="text-align: left">Chain of Thought (CoT)</td>
          <td style="text-align: left">æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œå¼•å¯¼æ¨¡å‹ä¸€æ­¥æ­¥æŠŠé€»è¾‘å†™å‡ºæ¥ï¼Œèƒ½æ˜¾è‘—æå‡å¤æ‚æ¨ç†ä»»åŠ¡çš„å‡†ç¡®ç‡ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>ReAct æ¡†æ¶</strong></td>
          <td style="text-align: left">ReAct (Reason+Act)</td>
          <td style="text-align: left">ä¸€ç§é€šè¿‡äº¤æ›¿è¿›è¡Œâ€œæ¨ç†â€å’Œâ€œè¡ŒåŠ¨â€æ¥è§£å†³ä»»åŠ¡çš„æ¨¡å¼ã€‚æ˜¯ Agent çš„æ ¸å¿ƒæ€è€ƒæ–¹å¼ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ</strong></td>
          <td style="text-align: left">Multi-Agent System</td>
          <td style="text-align: left">ç”±å¤šä¸ªæ‰®æ¼”ä¸åŒè§’è‰²çš„ Agent ç»„æˆçš„ç³»ç»Ÿï¼Œå®ƒä»¬åƒäººç±»å›¢é˜Ÿä¸€æ ·åä½œå®Œæˆå¤æ‚ä»»åŠ¡ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å¹»è§‰</strong></td>
          <td style="text-align: left">Hallucination</td>
          <td style="text-align: left">LLM ä¸€æœ¬æ­£ç»åœ°èƒ¡è¯´å…«é“ã€‚é€šè¿‡ä½¿ç”¨å·¥å…·ï¼ˆå¦‚æœç´¢ã€è®¡ç®—å™¨ï¼‰å¯ä»¥æœ‰æ•ˆå‡å°‘å¹»è§‰ã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary-7">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary-7">#</a></h3>
<p>Week 8 çš„æ ¸å¿ƒåœ¨äº**â€œActionâ€**ã€‚
å¦‚æœè¯´å‰å‡ å‘¨æ˜¯æ•™ä½ é€ ä¸€ä¸ªâ€œå…¨çŸ¥å…¨èƒ½çš„å“²å­¦å®¶â€ï¼ˆGenAIï¼‰ï¼Œè¿™å‘¨å°±æ˜¯æ•™ä½ æŠŠè¿™ä¸ªå“²å­¦å®¶å˜æˆä¸€ä¸ªâ€œèƒ½å¹²æ´»çš„æ‰“å·¥äººâ€ï¼ˆAgentï¼‰ã€‚
å…³é”®åœ¨äºï¼š</p>
<ol>
<li><strong>ç»™å®ƒå·¥å…·</strong>ï¼ˆFunction Callï¼‰ã€‚</li>
<li><strong>æ•™å®ƒæµç¨‹</strong>ï¼ˆReAct Loop: è§‚å¯Ÿ-&gt;æ€è€ƒ-&gt;è¡ŒåŠ¨ï¼‰ã€‚</li>
<li><strong>ç»™å®ƒè®°å¿†</strong>ï¼ˆRAG/Databaseï¼‰ï¼Œè®©å®ƒèƒ½ä»ç»éªŒä¸­å­¦ä¹ ã€‚</li>
</ol>
<hr>
<h1 id="cs5494-week-9-3d-vision--generation">CS5494 Week 9: 3D Vision &amp; Generation<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-9-3d-vision--generation">#</a></h1>
<h1 id="ç¬¬ä¹å‘¨ä¸‰ç»´è§†è§‰ä¸ç”Ÿæˆ">ç¬¬ä¹å‘¨ï¼šä¸‰ç»´è§†è§‰ä¸ç”Ÿæˆ<a hidden class="anchor" aria-hidden="true" href="#ç¬¬ä¹å‘¨ä¸‰ç»´è§†è§‰ä¸ç”Ÿæˆ">#</a></h1>
<h2 id="1-æ ¸å¿ƒç—›ç‚¹3d-è¡¨ç¤ºçš„å¤šæ ·æ€§">1. æ ¸å¿ƒç—›ç‚¹ï¼š3D è¡¨ç¤ºçš„å¤šæ ·æ€§<a hidden class="anchor" aria-hidden="true" href="#1-æ ¸å¿ƒç—›ç‚¹3d-è¡¨ç¤ºçš„å¤šæ ·æ€§">#</a></h2>
<h3 id="core-pain-point-diversity-of-3d-representations">Core Pain Point: Diversity of 3D Representations<a hidden class="anchor" aria-hidden="true" href="#core-pain-point-diversity-of-3d-representations">#</a></h3>
<p>åœ¨ 2D ä¸­ï¼Œæˆ‘ä»¬åªæœ‰ä¸€ç§æ ‡å‡†æ ¼å¼ï¼šåƒç´ ç½‘æ ¼ (Pixel Grid)ã€‚ä½†åœ¨ 3D ä¸­ï¼Œæ²¡æœ‰ç»Ÿä¸€çš„æ ‡å‡†ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®ä»»åŠ¡é€‰æ‹©ä¸åŒçš„<strong>æ•°æ®ç»“æ„</strong>ã€‚</p>
<h4 id="a-æ˜¾å¼è¡¨ç¤º-explicit-representations">A. æ˜¾å¼è¡¨ç¤º (Explicit Representations)<a hidden class="anchor" aria-hidden="true" href="#a-æ˜¾å¼è¡¨ç¤º-explicit-representations">#</a></h4>
<p>ç›´æ¥å®šä¹‰ç‰©ä½“çš„å‡ ä½•å½¢çŠ¶ã€‚</p>
<ol>
<li>
<p><strong>ä½“ç´  (Voxels)</strong>:</p>
<ul>
<li><strong>å®šä¹‰</strong>: 3D ç‰ˆçš„åƒç´  (Volume Pixel)ã€‚å°±åƒã€Šæˆ‘çš„ä¸–ç•Œã€‹(Minecraft) é‡Œçš„æ–¹å—ã€‚</li>
<li><strong>ä¼˜ç‚¹</strong>: è§„åˆ™ç»“æ„ï¼Œå¯ä»¥ç›´æ¥ç”¨ 3D å·ç§¯ç¥ç»ç½‘ç»œ (3D CNN) å¤„ç†ã€‚</li>
<li><strong>ç¼ºç‚¹</strong>: <strong>å†…å­˜çˆ†ç‚¸</strong>ã€‚åˆ†è¾¨ç‡æ¯å¢åŠ  1 å€ï¼Œå†…å­˜æ¶ˆè€—å¢åŠ  8 å€ ($O(N^3)$)ã€‚è¿™å°±å«â€œç»´æ•°ç¾éš¾â€ã€‚</li>
<li></li>
</ul>
</li>
<li>
<p><strong>ç‚¹äº‘ (Point Clouds)</strong>:</p>
<ul>
<li><strong>å®šä¹‰</strong>: ä¸€å † $(x, y, z)$ åæ ‡ç‚¹çš„é›†åˆã€‚é€šå¸¸ç”±æ¿€å…‰é›·è¾¾ (LiDAR) é‡‡é›†ã€‚</li>
<li><strong>ç‰¹ç‚¹</strong>: <strong>æ— åºæ€§ (Unordered)</strong>ã€‚ç‚¹çš„é¡ºåºä¸å½±å“å½¢çŠ¶ï¼Œè¿™å¯¹ç¥ç»ç½‘ç»œæ˜¯ä¸ªæŒ‘æˆ˜ï¼ˆåé¢ä¼šè®² PointNetï¼‰ã€‚</li>
<li><strong>ç¼ºç‚¹</strong>: æ²¡æœ‰è¡¨é¢ä¿¡æ¯ï¼Œæ”¾å¤§çœ‹å…¨æ˜¯æ´ã€‚</li>
</ul>
</li>
<li>
<p><strong>ç½‘æ ¼ (Meshes)</strong>:</p>
<ul>
<li><strong>å®šä¹‰</strong>: ç”±é¡¶ç‚¹ (Vertices)ã€è¾¹ (Edges) å’Œé¢ (Faces) ç»„æˆçš„é›†åˆã€‚æ˜¯æ¸¸æˆå’Œç”µå½±å·¥ä¸šçš„æ ‡å‡†ã€‚</li>
<li><strong>ç¼ºç‚¹</strong>: æ‹“æ‰‘ç»“æ„å¤æ‚ï¼Œç¥ç»ç½‘ç»œå¾ˆéš¾ç›´æ¥ç”Ÿæˆï¼ˆå¾ˆéš¾å¤„ç†â€œè¿™å°±å¤šäº†ä¸€ä¸ªæ´â€è¿™ç§æ‹“æ‰‘å˜åŒ–ï¼‰ã€‚</li>
<li></li>
</ul>
</li>
</ol>
<h4 id="b-éšå¼è¡¨ç¤º-implicit-representations-é‡ç‚¹">B. éšå¼è¡¨ç¤º (Implicit Representations) [é‡ç‚¹]<a hidden class="anchor" aria-hidden="true" href="#b-éšå¼è¡¨ç¤º-implicit-representations-é‡ç‚¹">#</a></h4>
<p>ä¸ç›´æ¥å­˜å‚¨ç‚¹ï¼Œè€Œæ˜¯å­˜å‚¨ä¸€ä¸ª<strong>æ•°å­¦å‡½æ•°</strong>ã€‚</p>
<ul>
<li><strong>æ€æƒ³</strong>: å®šä¹‰ä¸€ä¸ªå‡½æ•° $f(x, y, z)$ã€‚</li>
<li><strong>SDF (Signed Distance Function)</strong>: å‡½æ•°è¿”å›è¯¥ç‚¹è·ç¦»ç‰©ä½“è¡¨é¢çš„è·ç¦»ã€‚è´Ÿæ•°åœ¨å†…éƒ¨ï¼Œæ­£æ•°åœ¨å¤–éƒ¨ï¼Œ0 å°±æ˜¯è¡¨é¢ã€‚</li>
<li><strong>ä¼˜ç‚¹</strong>: åˆ†è¾¨ç‡æ— é™ï¼Œå¯ä»¥è¡¨ç¤ºæå…¶å¤æ‚çš„æ‹“æ‰‘ç»“æ„ã€‚</li>
</ul>
<hr>
<h2 id="2-æ ¸å¿ƒæ¶æ„pointnet-å¤„ç†ç‚¹äº‘">2. æ ¸å¿ƒæ¶æ„ï¼šPointNet (å¤„ç†ç‚¹äº‘)<a hidden class="anchor" aria-hidden="true" href="#2-æ ¸å¿ƒæ¶æ„pointnet-å¤„ç†ç‚¹äº‘">#</a></h2>
<h3 id="core-architecture-pointnet-processing-point-clouds">Core Architecture: PointNet (Processing Point Clouds)<a hidden class="anchor" aria-hidden="true" href="#core-architecture-pointnet-processing-point-clouds">#</a></h3>
<p>è¿™æ˜¯æ·±åº¦å­¦ä¹ å¤„ç† 3D ç‚¹äº‘çš„å¼€å±±ä¹‹ä½œã€‚</p>
<ul>
<li><strong>éš¾ç‚¹</strong>: ç‚¹äº‘æ˜¯æ— åºçš„ã€‚è¾“å…¥ $(Point A, Point B)$ å’Œ $(Point B, Point A)$ åº”è¯¥ä»£è¡¨åŒä¸€ä¸ªç‰©ä½“ã€‚æ™®é€šçš„ CNN æˆ– RNN å¯¹é¡ºåºæ•æ„Ÿï¼Œæ— æ³•å¤„ç†ã€‚</li>
<li><strong>PointNet çš„è§£å†³æ–¹æ¡ˆ</strong>:
<ol>
<li><strong>å…±äº« MLP (Shared MLP)</strong>: å¯¹æ¯ä¸€ä¸ªç‚¹å•ç‹¬è¿›è¡Œç‰¹å¾æå–ï¼ˆæ˜ å°„åˆ°é«˜ç»´ç©ºé—´ï¼‰ï¼Œæ¯ä¸ªç‚¹ç”¨çš„æƒé‡æ˜¯ä¸€æ ·çš„ã€‚</li>
<li><strong>å¯¹ç§°å‡½æ•° (Symmetric Function)</strong>: ä½¿ç”¨ <strong>Max Pooling</strong>ã€‚ä¸ç®¡è¾“å…¥çš„ç‚¹é¡ºåºæ€ä¹ˆå˜ï¼Œå–â€œæœ€å¤§å€¼â€è¿™ä¸ªæ“ä½œå¾—åˆ°çš„ç»“æœæ°¸è¿œæ˜¯ä¸€æ ·çš„ã€‚</li>
</ol>
<ul>
<li><em>Mechanism</em>: Map each point individually to a higher dimension, then aggregate them using a symmetric operation (max pooling) to get a global feature vector.</li>
</ul>
</li>
</ul>
<hr>
<h2 id="3-é©å‘½æ€§æŠ€æœ¯nerf-ç¥ç»è¾å°„åœº">3. é©å‘½æ€§æŠ€æœ¯ï¼šNeRF (ç¥ç»è¾å°„åœº)<a hidden class="anchor" aria-hidden="true" href="#3-é©å‘½æ€§æŠ€æœ¯nerf-ç¥ç»è¾å°„åœº">#</a></h2>
<h3 id="revolutionary-tech-nerf-neural-radiance-fields">Revolutionary Tech: NeRF (Neural Radiance Fields)<a hidden class="anchor" aria-hidden="true" href="#revolutionary-tech-nerf-neural-radiance-fields">#</a></h3>
<p>è¿™æ˜¯æœ¬å‘¨è¯¾ä»¶æœ€ç¡¬æ ¸ã€ä¹Ÿæ˜¯ç°ä»£ 3D ç”Ÿæˆçš„åŸºçŸ³ã€‚å®ƒä¸ä»…æ˜¯ä¸€ç§è¡¨ç¤ºï¼Œæ›´æ˜¯ä¸€ç§æ¸²æŸ“æŠ€æœ¯ã€‚</p>
<ul>
<li>
<p><strong>æ ¸å¿ƒå®šä¹‰</strong>:
NeRF ä¸æ˜¯å­˜ç½‘æ ¼ï¼Œè€Œæ˜¯è®­ç»ƒä¸€ä¸ª<strong>å…¨è¿æ¥ç¥ç»ç½‘ç»œ (MLP)</strong> æ¥éšå¼åœ°å­˜å‚¨åœºæ™¯ã€‚
$$F_\theta(x, y, z, \theta, \phi) \rightarrow (r, g, b, \sigma)$$</p>
<ul>
<li><strong>è¾“å…¥</strong>: ç©ºé—´åæ ‡ $(x, y, z)$ + è§‚çœ‹è§’åº¦ $(\theta, \phi)$ã€‚</li>
<li><strong>è¾“å‡º</strong>: è¯¥ç‚¹çš„é¢œè‰² $(r, g, b)$ + <strong>å¯†åº¦ (Density $\sigma$)</strong>ã€‚</li>
</ul>
</li>
<li>
<p><strong>ä½“æ¸²æŸ“ (Volumetric Rendering)</strong>:
æ€ä¹ˆæŠŠè¿™ä¸ªç¥ç»ç½‘ç»œå˜æˆä¸€å¼ å›¾ç‰‡ï¼Ÿ</p>
<ol>
<li><strong>å…‰çº¿æŠ•å°„ (Ray Marching)</strong>: ä»ç›¸æœºä½ç½®å‘å±å¹•çš„æ¯ä¸ªåƒç´ å‘å°„ä¸€æ¡å°„çº¿ã€‚</li>
<li><strong>é‡‡æ · (Sampling)</strong>: åœ¨è¿™æ¡å°„çº¿ä¸Šå–å¾ˆå¤šé‡‡æ ·ç‚¹ã€‚</li>
<li><strong>æŸ¥è¯¢ (Query)</strong>: æŠŠæ¯ä¸ªé‡‡æ ·ç‚¹çš„åæ ‡ä¸¢ç»™ MLPï¼Œé—®å®ƒï¼šâ€œè¿™é‡Œæœ‰ä»€ä¹ˆé¢œè‰²ï¼Ÿå¯†åº¦å¤šå°‘ï¼Ÿâ€</li>
<li><strong>ç§¯åˆ† (Integration)</strong>: æ ¹æ®å¯†åº¦æŠŠé¢œè‰²ç´¯åŠ èµ·æ¥ã€‚å¯†åº¦å¤§çš„åœ°æ–¹ä¼šé®æŒ¡åé¢çš„ç‚¹ã€‚</li>
</ol>
<ul>
<li><em>Analogy</em>: å°±åƒåœ¨é›¾ä¸­çœ‹ä¸œè¥¿ã€‚è§†çº¿ç©¿è¿‡é›¾æ°”ï¼ˆç´¯åŠ é¢œè‰²ï¼‰ï¼Œç›´åˆ°è¢«åšå®çš„ä¸œè¥¿æŒ¡ä½ã€‚</li>
</ul>
</li>
<li>
<p><strong>ä¼˜åŠ¿</strong>: ç”Ÿæˆçš„è§†å›¾å…·æœ‰<strong>ç…§ç‰‡çº§çœŸå®æ„Ÿ (Photorealistic)</strong>ï¼Œä¸”è§†ç‚¹è¿ç»­å˜åŒ–ã€‚</p>
</li>
<li>
<p><strong>åŠ£åŠ¿</strong>: æ¸²æŸ“ææ…¢ï¼ˆç”Ÿæˆä¸€å¼ å›¾è¦æŸ¥è¯¢å‡ ç™¾ä¸‡æ¬¡ç½‘ç»œï¼‰ã€‚</p>
</li>
</ul>
<hr>
<h2 id="4-3d-ç”Ÿæˆdreamfusion-ä¸-sds">4. 3D ç”Ÿæˆï¼šDreamFusion ä¸ SDS<a hidden class="anchor" aria-hidden="true" href="#4-3d-ç”Ÿæˆdreamfusion-ä¸-sds">#</a></h2>
<h3 id="3d-generation-dreamfusion--sds">3D Generation: DreamFusion &amp; SDS<a hidden class="anchor" aria-hidden="true" href="#3d-generation-dreamfusion--sds">#</a></h3>
<p>æˆ‘ä»¬æœ‰å¾ˆå¼ºçš„ 2D ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ Stable Diffusionï¼‰ï¼Œä½†æ²¡æœ‰å¾ˆå¤š 3D æ•°æ®æ¥è®­ç»ƒ 3D æ¨¡å‹ã€‚æ€ä¹ˆåŠï¼Ÿ
<strong>DreamFusion (2023)</strong> æå‡ºäº†ä¸€ç§â€œå€Ÿé¸¡ç”Ÿè›‹â€çš„æ–¹æ³•ã€‚</p>
<ul>
<li><strong>æ ¸å¿ƒé€»è¾‘</strong>: ç”¨ 2D æ¨¡å‹å½“â€œè€å¸ˆâ€ï¼ŒæŒ‡å¯¼ 3D æ¨¡å‹â€œç”»ç”»â€ã€‚</li>
<li><strong>æµç¨‹ (Process)</strong>:
<ol>
<li>åˆå§‹åŒ–ä¸€ä¸ªéšæœºçš„ NeRFï¼ˆçœ‹èµ·æ¥åƒä¸€å›¢é›¾ï¼‰ã€‚</li>
<li><strong>æ¸²æŸ“ (Render)</strong>: ä»æŸä¸ªè§’åº¦æ‹ä¸€å¼ è¿™å›¢é›¾çš„ç…§ç‰‡ã€‚</li>
<li><strong>æ‰“åˆ† (Critic)</strong>: æŠŠç…§ç‰‡ä¸¢ç»™ 2D Diffusion Model (æ¯”å¦‚ Stable Diffusion)ï¼Œå†åŠ ä¸Šæ–‡å­—æç¤ºï¼ˆå¦‚ &ldquo;ä¸€åªçº¢è‰²çš„é’è›™&rdquo;ï¼‰ã€‚</li>
<li><strong>SDS Loss (Score Distillation Sampling)</strong>: 2D æ¨¡å‹ä¼šå‘Šè¯‰ 3D æ¨¡å‹ï¼šâ€œè¿™çœ‹èµ·æ¥ä¸åƒé’è›™ï¼Œä½ åº”è¯¥æŠŠè¿™é‡Œçš„åƒç´ å˜çº¢ä¸€ç‚¹â€ã€‚ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰</li>
<li><strong>æ›´æ–° (Update)</strong>: æ ¹æ®å»ºè®®ä¿®æ”¹ NeRF çš„å‚æ•°ã€‚</li>
<li>é‡å¤å‡ åƒæ¬¡ï¼Œç›´åˆ° 3D æ¨¡å‹ä»å„ä¸ªè§’åº¦çœ‹éƒ½åƒä¸€åªé’è›™ã€‚</li>
</ol>
</li>
</ul>
<hr>
<h2 id="5-ä¸“æœ‰åè¯è¡¨-glossary-3">5. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#5-ä¸“æœ‰åè¯è¡¨-glossary-3">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>ä½“ç´ </strong></td>
          <td style="text-align: left">Voxel (Volumetric Pixel)</td>
          <td style="text-align: left">3D ç©ºé—´ä¸­çš„è§„åˆ™ç½‘æ ¼å•ä½ï¼Œç›¸å½“äº 2D å›¾ç‰‡ä¸­çš„åƒç´ ã€‚å¸¸ç”¨äºåŒ»ç–—å½±åƒæˆ–ç®€å•çš„ 3D æ¨¡å‹ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>ç‚¹äº‘</strong></td>
          <td style="text-align: left">Point Cloud</td>
          <td style="text-align: left">3D ç©ºé—´ä¸­ä¸€ç»„æ•°æ®ç‚¹ $(x,y,z)$ çš„é›†åˆã€‚ç”±æ¿€å…‰é›·è¾¾æ‰«æç”Ÿæˆï¼Œæ•°æ®ç¨€ç–ä¸”æ— åºã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>ç½‘æ ¼</strong></td>
          <td style="text-align: left">Mesh</td>
          <td style="text-align: left">ç”±å¤šè¾¹å½¢ï¼ˆé€šå¸¸æ˜¯ä¸‰è§’å½¢ï¼‰ç»„æˆçš„ 3D ç‰©ä½“è¡¨é¢è¡¨ç¤ºæ³•ã€‚ç”±é¡¶ç‚¹ã€è¾¹å’Œé¢æ„æˆã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>SDF</strong></td>
          <td style="text-align: left">Signed Distance Function</td>
          <td style="text-align: left">ç¬¦å·è·ç¦»å‡½æ•°ã€‚ä¸€ç§éšå¼è¡¨ç¤ºï¼Œè®°å½•ç©ºé—´ä¸­ä»»æ„ä¸€ç‚¹åˆ°æœ€è¿‘ç‰©ä½“è¡¨é¢çš„è·ç¦»ï¼ˆå†…éƒ¨ä¸ºè´Ÿï¼Œå¤–éƒ¨ä¸ºæ­£ï¼‰ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>NeRF</strong></td>
          <td style="text-align: left">NeRF (Neural Radiance Fields)</td>
          <td style="text-align: left">ç¥ç»è¾å°„åœºã€‚ç”¨ç¥ç»ç½‘ç»œæ¥è¡¨ç¤º 3D åœºæ™¯ï¼Œé€šè¿‡è¾“å…¥åæ ‡å’Œè§†è§’è¾“å‡ºé¢œè‰²å’Œå¯†åº¦ï¼Œèƒ½æ¸²æŸ“å‡ºæé«˜ä¿çœŸçš„æ–°è§†è§’å›¾åƒã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>ä½“æ¸²æŸ“</strong></td>
          <td style="text-align: left">Volumetric Rendering</td>
          <td style="text-align: left">ä¸€ç§åŸºäºå…‰çº¿ç©¿è¿‡ä»‹è´¨ï¼ˆå¦‚çƒŸé›¾ã€äº‘å±‚ï¼‰çš„ç‰©ç†ç‰¹æ€§çš„å›¾åƒç”ŸæˆæŠ€æœ¯ã€‚NeRF ä½¿ç”¨å®ƒå°† 3D å¯†åº¦åœºè½¬åŒ–ä¸º 2D å›¾åƒã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å…‰çº¿æŠ•å°„</strong></td>
          <td style="text-align: left">Ray Casting / Marching</td>
          <td style="text-align: left">å›¾å½¢å­¦æŠ€æœ¯ã€‚ä»ç›¸æœºå‘å±å¹•åƒç´ å‘å°„å…‰çº¿ï¼Œæ£€æµ‹å…‰çº¿ä¸ç‰©ä½“çš„äº¤äº’ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>PointNet</strong></td>
          <td style="text-align: left">PointNet</td>
          <td style="text-align: left">ä¸“é—¨å¤„ç†ç‚¹äº‘æ•°æ®çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œè§£å†³äº†ç‚¹äº‘æ— åºæ€§çš„é—®é¢˜ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>DreamFusion</strong></td>
          <td style="text-align: left">DreamFusion</td>
          <td style="text-align: left">Google æå‡ºçš„æ–‡æœ¬ç”Ÿæˆ 3D æ¨¡å‹ã€‚å®ƒä¸éœ€è¦ 3D è®­ç»ƒæ•°æ®ï¼Œè€Œæ˜¯åˆ©ç”¨é¢„è®­ç»ƒçš„ 2D æ‰©æ•£æ¨¡å‹æ¥ä¼˜åŒ– 3D ç»“æ„ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>SDS</strong></td>
          <td style="text-align: left">Score Distillation Sampling</td>
          <td style="text-align: left">åˆ†æ•°è’¸é¦é‡‡æ ·ã€‚DreamFusion çš„æ ¸å¿ƒç®—æ³•ï¼Œç”¨äºä» 2D æ‰©æ•£æ¨¡å‹ä¸­æå–æ¢¯åº¦ä¿¡æ¯æ¥æ›´æ–° 3D å‚æ•°ã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary-8">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary-8">#</a></h3>
<p>Week 9 çš„æ ¸å¿ƒåœ¨äº**â€œå‡ç»´â€**ã€‚</p>
<ul>
<li><strong>å­˜æ•°æ®</strong>: æ—¢ç„¶å­˜ä¸ä¸‹æ‰€æœ‰åƒç´ ï¼ˆä½“ç´ å¤ªè´µï¼‰ï¼Œæˆ‘ä»¬å°±å­˜ç‚¹ï¼ˆç‚¹äº‘ï¼‰ï¼Œæˆ–è€…å­˜çš®ï¼ˆç½‘æ ¼ï¼‰ï¼Œæˆ–è€…æœ€ç°ä»£çš„æ–¹æ³•â€”â€”å­˜<strong>å‡½æ•° (NeRF)</strong>ã€‚</li>
<li><strong>ç”Ÿæˆæ•°æ®</strong>: æ—¢ç„¶æ²¡æœ‰ 3D æ•°æ®é›†ï¼Œæˆ‘ä»¬å°±ç”¨ 2D çš„ AI å»â€œç›‘è€ƒâ€ 3D çš„ AIï¼Œè®©å®ƒå‡­ç©ºæå‡ºä¸€ä¸ª 3D æ¨¡å‹ (DreamFusion)ã€‚</li>
</ul>
<hr>
<h1 id="cs5494-week-10-large-foundation-models-vlm--vla">CS5494 Week 10: Large Foundation Models (VLM &amp; VLA)<a hidden class="anchor" aria-hidden="true" href="#cs5494-week-10-large-foundation-models-vlm--vla">#</a></h1>
<h1 id="ç¬¬åå‘¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸å…·èº«æ™ºèƒ½">ç¬¬åå‘¨ï¼šå¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸å…·èº«æ™ºèƒ½<a hidden class="anchor" aria-hidden="true" href="#ç¬¬åå‘¨å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ä¸å…·èº«æ™ºèƒ½">#</a></h1>
<h2 id="1-èŒƒå¼è½¬å˜ä»ä¸“ç”¨æ¨¡å‹åˆ°åŸºç¡€æ¨¡å‹">1. èŒƒå¼è½¬å˜ï¼šä»ä¸“ç”¨æ¨¡å‹åˆ°åŸºç¡€æ¨¡å‹<a hidden class="anchor" aria-hidden="true" href="#1-èŒƒå¼è½¬å˜ä»ä¸“ç”¨æ¨¡å‹åˆ°åŸºç¡€æ¨¡å‹">#</a></h2>
<h3 id="paradigm-shift-from-specialized-to-foundation-models">Paradigm Shift: From Specialized to Foundation Models<a hidden class="anchor" aria-hidden="true" href="#paradigm-shift-from-specialized-to-foundation-models">#</a></h3>
<p>è¯¾ä»¶å¼€ç¯‡é¦–å…ˆå¼ºè°ƒäº† AI å¼€å‘æ¨¡å¼çš„æ ¹æœ¬æ€§å˜åŒ–ã€‚</p>
<ul>
<li><strong>è¿‡å» (The Past)</strong>: ä¸ºæ¯ä¸ªä»»åŠ¡è®­ç»ƒä¸€ä¸ªä¸“ç”¨æ¨¡å‹ã€‚
<ul>
<li>åšåˆ†ç±»è®­ç»ƒä¸€ä¸ª ResNetï¼Œåšåˆ†å‰²è®­ç»ƒä¸€ä¸ª U-Netï¼Œåšç¿»è¯‘è®­ç»ƒä¸€ä¸ª LSTMã€‚</li>
<li><em>Analogy</em>: å°±åƒå®¶é‡Œä¹°äº†æ‰“è›‹å™¨ã€æ¦¨æ±æœºã€ç»è‚‰æœºï¼Œæ¯ä¸ªæœºå™¨åªèƒ½å¹²ä¸€ä»¶äº‹ã€‚</li>
</ul>
</li>
<li><strong>ç°åœ¨ (The Present)</strong>: åŸºç¡€æ¨¡å‹ (Foundation Models)ã€‚
<ul>
<li>åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šé¢„è®­ç»ƒä¸€ä¸ª<strong>é€šç”¨çš„æ¨¡å‹</strong>ï¼ˆå¦‚ GPT-4, Geminiï¼‰ï¼Œç„¶åé€šè¿‡å¾®è°ƒ (Fine-tuning) æˆ–æç¤º (Prompting) é€‚åº”å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚</li>
<li><em>Analogy</em>: å°±åƒä¸€ä¸ªå…¨èƒ½çš„å¤§å¨ï¼Œä½ ç»™ä»–ä»€ä¹ˆæŒ‡ä»¤ï¼Œä»–å°±èƒ½åšä»€ä¹ˆèœã€‚</li>
</ul>
</li>
</ul>
<hr>
<h2 id="2-è§†è§‰-è¯­è¨€æ¨¡å‹-vlm-clip">2. è§†è§‰-è¯­è¨€æ¨¡å‹ (VLM): CLIP<a hidden class="anchor" aria-hidden="true" href="#2-è§†è§‰-è¯­è¨€æ¨¡å‹-vlm-clip">#</a></h2>
<h3 id="vision-language-models-vlm-clip">Vision-Language Models (VLM): CLIP<a hidden class="anchor" aria-hidden="true" href="#vision-language-models-vlm-clip">#</a></h3>
<p>CLIP (Contrastive Language-Image Pre-training) æ˜¯è¿æ¥æ–‡å­—å’Œå›¾ç‰‡æœ€é‡è¦çš„æ¡¥æ¢ã€‚</p>
<ul>
<li><strong>æ ¸å¿ƒæ€æƒ³ (Core Idea)</strong>:
CLIP ä¸å†åƒä¼ ç»Ÿåˆ†ç±»æ¨¡å‹é‚£æ ·é¢„æµ‹å›ºå®šçš„ 1000 ä¸ªç±»åˆ«ï¼ˆImageNetï¼‰ï¼Œè€Œæ˜¯<strong>å­¦ä¹ å›¾ç‰‡å’Œæ–‡å­—æè¿°ä¹‹é—´çš„åŒ¹é…å…³ç³»</strong>ã€‚</li>
<li><strong>è®­ç»ƒæ–¹æ³•ï¼šå¯¹æ¯”å­¦ä¹  (Contrastive Learning)</strong>: [é‡ç‚¹]
<ol>
<li>è¾“å…¥ï¼šä¸€å¯¹å¯¹å›¾ç‰‡å’Œå¯¹åº”çš„æ–‡å­—è¯´æ˜ (Image-Text Pairs)ã€‚</li>
<li>æ¨¡å‹åŒ…å«ä¸€ä¸ª Image Encoder å’Œä¸€ä¸ª Text Encoderã€‚</li>
<li><strong>æ‹‰è¿‘ (Pull)</strong>: åªæœ‰æ­£ç¡®çš„å›¾æ–‡å¯¹ï¼Œå®ƒä»¬ç”Ÿæˆçš„å‘é‡åœ¨ç©ºé—´ä¸­åº”è¯¥é å¾—å¾ˆè¿‘ã€‚</li>
<li><strong>æ¨è¿œ (Push)</strong>: é”™è¯¯çš„å›¾æ–‡ç»„åˆï¼ˆæ¯”å¦‚â€œç‹—çš„å›¾â€é…â€œçŒ«çš„å­—â€ï¼‰ï¼Œå‘é‡åº”è¯¥äº’ç›¸æ’æ–¥ã€‚</li>
</ol>
<ul>
<li><em>Concept</em>: Maximizing the cosine similarity for $N$ real pairs, while minimizing it for $N^2 - N$ incorrect pairings.</li>
</ul>
</li>
<li><strong>èƒ½åŠ›</strong>: <strong>Zero-shot Classification (é›¶æ ·æœ¬åˆ†ç±»)</strong>ã€‚CLIP ä¸éœ€è¦é‡æ–°è®­ç»ƒå°±èƒ½è¯†åˆ«ä»æœªè§è¿‡çš„ç‰©ä½“ï¼Œåªè¦ä½ å‘Šè¯‰å®ƒé‚£ä¸ªç‰©ä½“çš„åå­—ï¼ˆæ–‡å­—ï¼‰ã€‚</li>
</ul>
<hr>
<h2 id="3-è®©-llm-çœ‹æ‡‚å›¾-llava">3. è®© LLM çœ‹æ‡‚å›¾: LLaVA<a hidden class="anchor" aria-hidden="true" href="#3-è®©-llm-çœ‹æ‡‚å›¾-llava">#</a></h2>
<h3 id="making-llm-see-llava-large-language-and-vision-assistant">Making LLM See: LLaVA (Large Language-and-Vision Assistant)<a hidden class="anchor" aria-hidden="true" href="#making-llm-see-llava-large-language-and-vision-assistant">#</a></h3>
<p>CLIP åªèƒ½åšåŒ¹é…ï¼Œä¸èƒ½åƒ ChatGPT é‚£æ ·èŠå¤©ã€‚LLaVA æ˜¯é€šè¿‡<strong>è§†è§‰æŒ‡ä»¤å¾®è°ƒ (Visual Instruction Tuning)</strong> æŠŠè§†è§‰èƒ½åŠ›å«æ¥åˆ° LLM ä¸Šã€‚</p>
<ul>
<li><strong>æ¶æ„ (Architecture)</strong>:
$$LLaVA = VisionEncoder + Projection + LLM$$
<ul>
<li><strong>Vision Encoder</strong>: ç”¨ CLIP æŠŠå›¾ç‰‡å˜æˆå‘é‡ç‰¹å¾ã€‚</li>
<li><strong>Projection Layer (æŠ•å½±å±‚)</strong>: [éš¾ç‚¹]
<ul>
<li>LLM åªèƒ½çœ‹æ‡‚æ–‡æœ¬å‘é‡ç©ºé—´ï¼Œçœ‹ä¸æ‡‚å›¾ç‰‡å‘é‡ç©ºé—´ã€‚</li>
<li>æŠ•å½±å±‚å°±æ˜¯ä¸€ä¸ªâ€œç¿»è¯‘å®˜â€ï¼ˆé€šå¸¸æ˜¯ä¸ªç®€å•çš„ MLPï¼‰ï¼ŒæŠŠå›¾ç‰‡ç‰¹å¾<strong>æ˜ å°„</strong>åˆ° LLM çš„æ–‡æœ¬ç‰¹å¾ç©ºé—´ä¸­ã€‚</li>
</ul>
</li>
<li><strong>LLM</strong>: ä¹Ÿå°±æ˜¯ Llama æˆ– Vicunaã€‚å¯¹å®ƒæ¥è¯´ï¼ŒæŠ•å½±åçš„å›¾ç‰‡ç‰¹å¾å°±åƒæ˜¯ä¸€å †â€œå¤–è¯­å•è¯â€ã€‚</li>
</ul>
</li>
<li><strong>è®­ç»ƒ</strong>:
é€šè¿‡é—®ç­”å¯¹ï¼ˆä¾‹å¦‚ï¼šâ€œè¿™å¼ å›¾é‡Œæœ‰ä»€ä¹ˆï¼Ÿâ€-â€œæœ‰ä¸€åªçŒ«â€ï¼‰æ¥è®­ç»ƒï¼Œè®© LLM å­¦ä¼šç†è§£è¿™äº›â€œè§†è§‰å•è¯â€çš„å«ä¹‰ã€‚</li>
</ul>
<hr>
<h2 id="4-è§†è§‰åˆ†å‰²åŸºåº§-sam">4. è§†è§‰åˆ†å‰²åŸºåº§: SAM<a hidden class="anchor" aria-hidden="true" href="#4-è§†è§‰åˆ†å‰²åŸºåº§-sam">#</a></h2>
<h3 id="vision-foundation-model-sam-segment-anything-model">Vision Foundation Model: SAM (Segment Anything Model)<a hidden class="anchor" aria-hidden="true" href="#vision-foundation-model-sam-segment-anything-model">#</a></h3>
<p>SAM æ˜¯ Meta å‘å¸ƒçš„å›¾åƒåˆ†å‰²é¢†åŸŸçš„â€œGPTâ€ã€‚</p>
<ul>
<li><strong>ç—›ç‚¹</strong>: ä»¥å‰åšåˆ†å‰²ï¼ˆæŠŠç‰©ä½“æŠ å‡ºæ¥ï¼‰éœ€è¦é’ˆå¯¹ç‰¹å®šç‰©ä½“è®­ç»ƒï¼Œä¸”æ ‡æ³¨æ•°æ®æéš¾ã€‚</li>
<li><strong>Promptable Segmentation (å¯æç¤ºåˆ†å‰²)</strong>:
SAM ä¸éœ€è¦ä½ å‘Šè¯‰å®ƒå…·ä½“çš„ç±»åˆ«ï¼Œä½ åªéœ€è¦ç»™å®ƒä¸€ä¸ª<strong>æç¤º (Prompt)</strong>ï¼š
<ul>
<li>ç‚¹ä¸€ä¸‹ (Point)</li>
<li>ç”»ä¸ªæ¡† (Box)</li>
<li>å†™è¡Œå­— (Text)
å®ƒå°±èƒ½æŠŠå¯¹åº”çš„ç‰©ä½“æŠ å‡ºæ¥ã€‚</li>
</ul>
</li>
<li><strong>æ„ä¹‰</strong>: è§£å†³äº†è§†è§‰ä»»åŠ¡ä¸­æœ€ç¹ççš„åˆ†å‰²é—®é¢˜ï¼Œæˆä¸ºå„ç§é«˜çº§è§†è§‰ä»»åŠ¡çš„é¢„å¤„ç†å·¥å…·ã€‚</li>
</ul>
<hr>
<h2 id="5-å…·èº«æ™ºèƒ½-vla-æ¨¡å‹-robotic-foundation-models">5. å…·èº«æ™ºèƒ½: VLA æ¨¡å‹ (Robotic Foundation Models)<a hidden class="anchor" aria-hidden="true" href="#5-å…·èº«æ™ºèƒ½-vla-æ¨¡å‹-robotic-foundation-models">#</a></h2>
<h3 id="embodied-ai-vision-language-action-models-vla">Embodied AI: Vision-Language-Action Models (VLA)<a hidden class="anchor" aria-hidden="true" href="#embodied-ai-vision-language-action-models-vla">#</a></h3>
<p>è¿™æ˜¯æœ¬èŠ‚è¯¾æœ€é«˜é˜¶çš„å†…å®¹ã€‚æˆ‘ä»¬ä¸ä»…è¦ AI çœ‹å›¾è¯´è¯ (VLM)ï¼Œè¿˜è¦å®ƒ<strong>çœ‹å›¾å¹²æ´»</strong>ã€‚</p>
<h4 id="a-rt-1-robotic-transformer-1">A. RT-1 (Robotic Transformer 1)<a hidden class="anchor" aria-hidden="true" href="#a-rt-1-robotic-transformer-1">#</a></h4>
<ul>
<li><strong>æ ¸å¿ƒåˆ›æ–°</strong>: <strong>åŠ¨ä½œ Token åŒ– (Tokenizing Actions)</strong>ã€‚
<ul>
<li>Transformer æœ¬è´¨ä¸Šæ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ (Token)ã€‚</li>
<li>RT-1 æŠŠæœºå™¨äººçš„åŠ¨ä½œï¼ˆæ‰‹è‡‚ç§»åŠ¨ x, y, zï¼Œå¤¹çˆªå¼€åˆï¼‰ç¦»æ•£åŒ–æˆä¸€ä¸ªä¸ªæ•°å­— Tokenã€‚</li>
<li>è¿™æ ·ï¼Œæ§åˆ¶æœºå™¨äººå°±å˜æˆäº†â€œæ–‡æœ¬ç”Ÿæˆâ€é—®é¢˜ï¼šè¾“å…¥æ˜¯å›¾ç‰‡ï¼Œè¾“å‡ºæ˜¯â€œåŠ¨ä½œå•è¯â€ã€‚</li>
<li><em>Concept</em>: Treating physical motor control as a sequence modeling problem, just like language.</li>
</ul>
</li>
</ul>
<h4 id="b-rt-2-äº’è”ç½‘çŸ¥è¯†è¿ç§»-internet-knowledge-transfer">B. RT-2: äº’è”ç½‘çŸ¥è¯†è¿ç§» (Internet Knowledge Transfer)<a hidden class="anchor" aria-hidden="true" href="#b-rt-2-äº’è”ç½‘çŸ¥è¯†è¿ç§»-internet-knowledge-transfer">#</a></h4>
<ul>
<li><strong>é—®é¢˜</strong>: æœºå™¨äººæ•°æ®å¾ˆå°‘ï¼ˆå¾ˆéš¾è®©æœºå™¨äººå®é™…æŠ“ 100 ä¸‡æ¬¡æ¯å­ï¼‰ï¼Œä½†äº’è”ç½‘å›¾ç‰‡/æ–‡å­—æ•°æ®å¾ˆå¤šã€‚</li>
<li><strong>Co-training (è”åˆè®­ç»ƒ)</strong>:
RT-2 æŠŠäº’è”ç½‘ä¸Šçš„ VQA æ•°æ®ï¼ˆçœ‹å›¾é—®ç­”ï¼‰å’Œæœºå™¨äººæ“ä½œæ•°æ®<strong>æ··åœ¨ä¸€èµ·è®­ç»ƒ</strong>ã€‚</li>
<li><strong>æ¶Œç°èƒ½åŠ› (Emergent Capabilities)</strong>: [éš¾ç‚¹]
<ul>
<li>å¦‚æœä½ æ•™æœºå™¨äººâ€œæŠ“è‹¹æœâ€ï¼Œå®ƒä¼šæŠ“è‹¹æœã€‚</li>
<li>ä½†å¦‚æœä½ è®©å®ƒâ€œ<strong>æŠ“é‚£ä¸ªç­ç»çš„åŠ¨ç‰©</strong>â€ï¼ˆæ¡Œä¸Šæœ‰æé¾™ç©å…·å’Œè‹¹æœï¼‰ï¼ŒRT-1 ä¼šå‚»æ‰ã€‚</li>
<li>RT-2 çŸ¥é“â€œæé¾™æ˜¯ç­ç»åŠ¨ç‰©â€ï¼ˆæ¥è‡ªäº’è”ç½‘çŸ¥è¯†ï¼‰ï¼Œä¹ŸçŸ¥é“â€œæ€ä¹ˆæŠ“ä¸œè¥¿â€ï¼ˆæ¥è‡ªæœºå™¨äººæ•°æ®ï¼‰ï¼Œæ‰€ä»¥å®ƒèƒ½æ¨æ–­å‡ºè¦å»æŠ“æé¾™ç©å…·ã€‚</li>
<li><em>Takeaway</em>: Semantic knowledge from the web is transferred to physical control.</li>
</ul>
</li>
</ul>
<h4 id="c-rt-x--openvla">C. RT-X / OpenVLA<a hidden class="anchor" aria-hidden="true" href="#c-rt-x--openvla">#</a></h4>
<ul>
<li><strong>è·¨å½¢æ€ (Cross-Embodiment)</strong>: ä¸åŒçš„æœºå™¨äººï¼ˆæœ‰çš„å•è‡‚ï¼Œæœ‰çš„åŒè‡‚ï¼Œæœ‰çš„é•¿æœ‰çš„çŸ­ï¼‰å¦‚ä½•å…±ç”¨ä¸€ä¸ªæ¨¡å‹ï¼Ÿ</li>
<li>é€šè¿‡æ ‡å‡†åŒ–åŠ¨ä½œç©ºé—´ï¼Œæ±‡é›†å…¨ä¸–ç•Œä¸åŒæœºå™¨äººçš„æ•°æ®è®­ç»ƒå‡ºä¸€ä¸ªé€šç”¨çš„ VLA æ¨¡å‹ã€‚</li>
</ul>
<hr>
<h2 id="6-ä¸“æœ‰åè¯è¡¨-glossary-4">6. ä¸“æœ‰åè¯è¡¨ (Glossary)<a hidden class="anchor" aria-hidden="true" href="#6-ä¸“æœ‰åè¯è¡¨-glossary-4">#</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">ä¸­æ–‡æœ¯è¯­</th>
          <th style="text-align: left">English Term</th>
          <th style="text-align: left">è¯¦ç»†è§£é‡Š / Detailed Explanation</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><strong>åŸºç¡€æ¨¡å‹</strong></td>
          <td style="text-align: left">Foundation Model</td>
          <td style="text-align: left">åœ¨æµ·é‡æ•°æ®ä¸Šé¢„è®­ç»ƒçš„å¤§è§„æ¨¡æ¨¡å‹ï¼Œå…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¯é€‚åº”å¤šç§ä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ GPT-4, Llamaï¼‰ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å¤šæ¨¡æ€</strong></td>
          <td style="text-align: left">Multimodal</td>
          <td style="text-align: left">æŒ‡æ¨¡å‹èƒ½åŒæ—¶å¤„ç†å¤šç§åª’ä½“æ•°æ®ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç­‰ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å¯¹æ¯”å­¦ä¹ </strong></td>
          <td style="text-align: left">Contrastive Learning</td>
          <td style="text-align: left">CLIP çš„æ ¸å¿ƒè®­ç»ƒæ–¹æ³•ã€‚é€šè¿‡æ‹‰è¿‘æ­£æ ·æœ¬å¯¹ï¼ˆåŒ¹é…çš„å›¾æ–‡ï¼‰ã€æ¨è¿œè´Ÿæ ·æœ¬å¯¹æ¥å­¦ä¹ ç‰¹å¾è¡¨ç¤ºã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>é›¶æ ·æœ¬åˆ†ç±»</strong></td>
          <td style="text-align: left">Zero-shot Classification</td>
          <td style="text-align: left">æ¨¡å‹æ— éœ€é’ˆå¯¹ç‰¹å®šç±»åˆ«è¿›è¡Œå¾®è°ƒï¼Œä»…å‡­ç±»åˆ«çš„è¯­ä¹‰æè¿°å°±èƒ½è¯†åˆ«æ ·æœ¬çš„èƒ½åŠ›ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è§†è§‰æŒ‡ä»¤å¾®è°ƒ</strong></td>
          <td style="text-align: left">Visual Instruction Tuning</td>
          <td style="text-align: left">LLaVA çš„è®­ç»ƒæ–¹æ³•ã€‚å°†å›¾åƒç‰¹å¾æ˜ å°„åˆ°è¯­è¨€ç©ºé—´ï¼Œå¹¶é€šè¿‡æŒ‡ä»¤-å›å¤å¯¹æ¥å¾®è°ƒ LLMï¼Œä½¿å…¶å…·å¤‡å¤šæ¨¡æ€å¯¹è¯èƒ½åŠ›ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>å…·èº«æ™ºèƒ½</strong></td>
          <td style="text-align: left">Embodied AI</td>
          <td style="text-align: left">æ‹¥æœ‰ç‰©ç†å®ä½“ï¼ˆå¦‚æœºå™¨äººï¼‰çš„ AIï¼Œä¸ä»…èƒ½è¿›è¡Œæ•°å­—è®¡ç®—ï¼Œè¿˜èƒ½ä¸ç‰©ç†ä¸–ç•Œè¿›è¡Œäº¤äº’ï¼ˆæ„ŸçŸ¥+è¡ŒåŠ¨ï¼‰ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>VLA æ¨¡å‹</strong></td>
          <td style="text-align: left">VLA (Vision-Language-Action)</td>
          <td style="text-align: left">è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ã€‚VLM çš„è¿›é˜¶ç‰ˆï¼Œè¾“å‡ºä¸ä»…æ˜¯æ–‡æœ¬ï¼Œè¿˜å¯ä»¥æ˜¯æœºå™¨äººçš„æ§åˆ¶æŒ‡ä»¤ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>åŠ¨ä½œ Token åŒ–</strong></td>
          <td style="text-align: left">Action Tokenization</td>
          <td style="text-align: left">å°†è¿ç»­çš„æœºå™¨äººåŠ¨ä½œï¼ˆå¦‚ç”µå‹ã€è§’åº¦ã€åæ ‡ï¼‰è½¬åŒ–ä¸ºç¦»æ•£çš„æ•°å­— Tokenï¼Œä»¥ä¾¿ Transformer å¤„ç†ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è”åˆè®­ç»ƒ</strong></td>
          <td style="text-align: left">Co-training</td>
          <td style="text-align: left">åœ¨ RT-2 ä¸­ä½¿ç”¨çš„æ–¹æ³•ï¼ŒåŒæ—¶ä½¿ç”¨äº’è”ç½‘å›¾æ–‡æ•°æ®å’Œæœºå™¨äººæ“ä½œæ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥å®ç°çŸ¥è¯†è¿ç§»ã€‚</td>
      </tr>
      <tr>
          <td style="text-align: left"><strong>è·¨å½¢æ€</strong></td>
          <td style="text-align: left">Cross-Embodiment</td>
          <td style="text-align: left">æŒ‡ä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿæ§åˆ¶èº«ä½“ç»“æ„å®Œå…¨ä¸åŒçš„å¤šç§æœºå™¨äººï¼ˆå¦‚ä¸åŒå“ç‰Œã€ä¸åŒè‡ªç”±åº¦çš„æœºæ¢°è‡‚ï¼‰ã€‚</td>
      </tr>
  </tbody>
</table>
<h3 id="æ€»ç»“-summary-9">æ€»ç»“ (Summary)<a hidden class="anchor" aria-hidden="true" href="#æ€»ç»“-summary-9">#</a></h3>
<p>Week 10 å‘Šè¯‰ä½  AI çš„ç»ˆæå½¢æ€ï¼š</p>
<ol>
<li><strong>CLIP</strong> è®©ç”µè„‘å­¦ä¼šäº†æŠŠâ€œå›¾â€å’Œâ€œå­—â€å¯¹ä¸Šå·ã€‚</li>
<li><strong>LLaVA</strong> è®© LLM é•¿äº†çœ¼ç›ï¼Œèƒ½çœ‹å›¾è¯´è¯ã€‚</li>
<li><strong>SAM</strong> æä¾›äº†åƒç´ çº§çš„ç²¾ç¡®è§†è§‰èƒ½åŠ›ã€‚</li>
<li><strong>RT-2 / VLA</strong> æŠŠè¿™ä¸€åˆ‡è£…è¿›äº†æœºå™¨äººçš„èº«ä½“é‡Œï¼Œåˆ©ç”¨å¤§æ¨¡å‹å­¦åˆ°çš„ä¸–ç•ŒçŸ¥è¯†ï¼ˆè¯­ä¹‰ï¼‰ï¼Œå»æŒ‡å¯¼ç‰©ç†ä¸–ç•Œçš„è¡ŒåŠ¨ï¼ˆæ“ä½œï¼‰ã€‚
<strong>ä»â€œè¯»ä¸‡å·ä¹¦â€ï¼ˆLLMï¼‰åˆ°â€œè¡Œä¸‡é‡Œè·¯â€ï¼ˆVLAï¼‰ï¼Œè¿™å°±æ˜¯ AI çš„æœªæ¥ã€‚</strong></li>
</ol>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://gopherding.github.io/posts/read/%E6%98%A5/">
    <span class="title">Â« ä¸Šä¸€é¡µ</span>
    <br>
    <span>æ˜¥</span>
  </a>
  <a class="next" href="https://gopherding.github.io/posts/life/red-beacon/">
    <span class="title">ä¸‹ä¸€é¡µ Â»</span>
    <br>
    <span>Red Beacon</span>
  </a>
</nav>

        </footer>
    </div>

<style>
    .comments_details summary::marker {
        font-size: 20px;
        content: 'ğŸ‘‰å±•å¼€è¯„è®º';
        color: var(--content);
    }
    .comments_details[open] summary::marker{
        font-size: 20px;
        content: 'ğŸ‘‡å…³é—­è¯„è®º';
        color: var(--content);
    }
</style>


<div>
    <details class="comments_details">
        <summary style="cursor: pointer; margin: 50px 0 20px 0;width: 130px;">
            <span style="font-size: 20px;color: var(--content);">...</span>
        </summary>
        <div id="tcomment"></div>
    </details>
    <script src="https://cdn.staticfile.org/twikoo//twikoo.all.min.js">
    </script>
    <script>
        twikoo.init({
            envId:  null ,
        el: "#tcomment",
            lang: 'zh-CN',
            region:  null ,
        path: window.TWIKOO_MAGIC_PATH||window.location.pathname,
        })
    </script>
</div>
</article>
</main>

<footer class="footer">
    <span>
        Copyright
        &copy;
        2021-2025
        <a href="https://gopherding.github.io/" style="color:#939393;">GopherDing&#39;s Blog</a>
        All Rights Reserved. Generated with 
        <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a>
        .Version 0.147.9.
    </span>
    
    
    <br>
    <span id="busuanzi_container">
        <span class="fa fa-user"></span> <span id="busuanzi_value_site_uv"></span>
        <span class="fa fa-eye"></span> <span id="busuanzi_value_site_pv"></span>
    </span>
</footer>




  <script src="/js/typewriter.js"></script>

<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 100) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"GopherDing's Blog"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"GopherDing's Blog"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;
        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'å¤åˆ¶';

        function copyingDone() {
            copybutton.innerText = 'å·²å¤åˆ¶ï¼';
            setTimeout(() => {
                copybutton.innerText = 'å¤åˆ¶';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                let text = codeblock.textContent +
                    '\r\nâ€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\r\n' +
                    'ç‰ˆæƒå£°æ˜ï¼šæœ¬æ–‡ä¸ºã€Œ'+"GopherDing's Blog"+'ã€çš„åŸåˆ›æ–‡ç« ï¼Œéµå¾ªCC 4.0 BY-SAç‰ˆæƒåè®®ï¼Œè½¬è½½è¯·é™„ä¸ŠåŸæ–‡å‡ºå¤„é“¾æ¥åŠæœ¬å£°æ˜ã€‚' +
                '\r\nåŸæ–‡é“¾æ¥ï¼š' + location.href;
                navigator.clipboard.writeText(text);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) {}
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
